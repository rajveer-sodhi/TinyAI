## SLURM PROLOG ###############################################################
##    Job ID : 14796441
##  Job Name : TinyAI
##  Nodelist : gpu2006
##      CPUs : 1
##  Mem/Node : 64000 MB
## Directory : /oscar/home/bpeckham/TinyAI
##   Job Started : Thu Dec 11 02:54:50 AM EST 2025
###############################################################################

==========================================
Job started at: Thu Dec 11 02:54:50 AM EST 2025
Job ID: 14796441
Node: gpu2006
==========================================

Python location: /users/bpeckham/.conda/envs/tinyai/bin/python
Python version: Python 3.10.19
Conda environment: tinyai
Installing/verifying required packages...
TensorFlow 2.15.0 and NumPy 1.26.4 imported successfully
GPU Information:
Thu Dec 11 02:55:00 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 6000                On  | 00000000:3D:00.0 Off |                  Off |
| 33%   26C    P8              11W / 260W |     22MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     30575      G   /usr/libexec/Xorg                            20MiB |
+---------------------------------------------------------------------------------------+

TensorFlow GPU Detection:
TF version: 2.15.0
Built with CUDA: True
GPUs detected: 0
GPU devices: []

==========================================
Starting main Python script at Thu Dec 11 02:55:04 AM EST 2025
==========================================

Configuration:
  d_model: 32
  num_layers: 1
  num_heads: 2
  ff_dim: 32
  deep_rec_cycles: 2
  num_l_steps: 3
  epochs: 30
  batch_size: 8
  learning_rate: 1e-4
  max_seq_length: 128

To monitor training with WandB:
  1. First, log in to WandB (one time only):
     python wandb_login.py
  2. Then view metrics at: https://wandb.ai
  (No port forwarding needed - metrics upload automatically!)


============================================================
TinyAI TRAINING SCRIPT
============================================================

Configuration:
  Data path: /oscar/home/bpeckham/TinyAI/preprocessing/data/final_train_data.txt
  Vocab path: /oscar/home/bpeckham/TinyAI/preprocessing/data/vocab.json
  Max sequence length: 128
  Model dimension: 32
  Layers: 1
  Heads: 2
  FF dim: 32
  Dropout: 0.1
  Deep recursion cycles: 2
  Inner iterations: 3
  Deep supervision steps: 2
  Batch size: 8
  Epochs: 30
  Learning rate: 0.0001
============================================================

Loading tokenizer...
Loaded vocabulary with 13678 tokens
  PAD=0, UNK=1, BOS=32, EOS=2
Loading data...
Loaded 7473 samples
Train: 5979, Val: 747, Test: 747

############################################################
# RECURSIVE TRANSFORMER (TRM-Inspired)
############################################################

Recursive Model Parameters: 899,855

============================================================
TRAINING RECURSIVE TRANSFORMER (TRM-Inspired)
============================================================
  Deep recursion cycles (T): 2
  Inner iterations (n): 3
  Deep supervision steps: 2
  ACT loss weight: 0.1
  Halt exploration prob: 0.1
  Max halt steps: 16
============================================================


Epoch 1/30
----------------------------------------
  Batch 50: Loss=9.4218, Acc=0.0235
  Batch 100: Loss=9.2131, Acc=0.0141
  Batch 150: Loss=8.8917, Acc=0.0444
  Batch 200: Loss=8.6575, Acc=0.0517
  Batch 250: Loss=8.4021, Acc=0.0397
  Batch 300: Loss=8.2111, Acc=0.0299
  Batch 350: Loss=7.9616, Acc=0.0357
  Batch 400: Loss=7.7542, Acc=0.0332
  Batch 450: Loss=7.5646, Acc=0.0413
  Batch 500: Loss=7.3225, Acc=0.0383
  Batch 550: Loss=7.2621, Acc=0.0403
  Batch 600: Loss=7.0703, Acc=0.0359
  Batch 650: Loss=6.6830, Acc=0.0414
  Batch 700: Loss=6.7349, Acc=0.0306

  Train Loss: 7.9551, Train Acc: 0.0355
  Val Loss: 6.6047, Val Acc: 0.0363
  ✓ Saved best model (val_loss=6.6047)

Epoch 2/30
----------------------------------------
  Batch 50: Loss=6.5361, Acc=0.0236
  Batch 100: Loss=6.2910, Acc=0.0355
  Batch 150: Loss=6.0915, Acc=0.0367
  Batch 200: Loss=6.2982, Acc=0.0423
  Batch 250: Loss=6.0761, Acc=0.0444
  Batch 300: Loss=6.4336, Acc=0.0356
  Batch 350: Loss=6.2806, Acc=0.0472
  Batch 400: Loss=5.9379, Acc=0.0553
  Batch 450: Loss=6.0702, Acc=0.0416
  Batch 500: Loss=5.9164, Acc=0.0238
  Batch 550: Loss=6.1294, Acc=0.0357
  Batch 600: Loss=6.2130, Acc=0.0458
  Batch 650: Loss=5.8207, Acc=0.0459
  Batch 700: Loss=5.5261, Acc=0.0486

  Train Loss: 6.1561, Train Acc: 0.0399
  Val Loss: 6.0884, Val Acc: 0.0100
  ✓ Saved best model (val_loss=6.0884)

Epoch 3/30
----------------------------------------
  Batch 50: Loss=5.8243, Acc=0.0000
  Batch 100: Loss=6.0713, Acc=0.0000
  Batch 150: Loss=5.7279, Acc=0.0011
  Batch 200: Loss=5.7921, Acc=0.0010
  Batch 250: Loss=6.0421, Acc=0.0092
  Batch 300: Loss=5.7057, Acc=0.0270
  Batch 350: Loss=5.7532, Acc=0.0516
  Batch 400: Loss=5.6487, Acc=0.0581
  Batch 450: Loss=5.5161, Acc=0.0805
  Batch 500: Loss=5.4433, Acc=0.0851
  Batch 550: Loss=5.8981, Acc=0.0785
  Batch 600: Loss=6.1764, Acc=0.0886
  Batch 650: Loss=5.6951, Acc=0.1034
  Batch 700: Loss=5.8149, Acc=0.1036

  Train Loss: 5.8143, Train Acc: 0.0499
  Val Loss: 5.7321, Val Acc: 0.1279
  ✓ Saved best model (val_loss=5.7321)

Epoch 4/30
----------------------------------------
  Batch 50: Loss=5.5297, Acc=0.1583
  Batch 100: Loss=5.4781, Acc=0.1688
  Batch 150: Loss=5.6548, Acc=0.1644
  Batch 200: Loss=5.7112, Acc=0.1803
  Batch 250: Loss=5.5650, Acc=0.1699
  Batch 300: Loss=5.3591, Acc=0.2138
  Batch 350: Loss=5.6705, Acc=0.1621
  Batch 400: Loss=5.6039, Acc=0.2063
  Batch 450: Loss=5.4499, Acc=0.1717
  Batch 500: Loss=5.1167, Acc=0.2190
  Batch 550: Loss=5.2558, Acc=0.2004
  Batch 600: Loss=5.0672, Acc=0.2059
  Batch 650: Loss=4.9609, Acc=0.2176
  Batch 700: Loss=5.2773, Acc=0.1810

  Train Loss: 5.3672, Train Acc: 0.1887
  Val Loss: 5.2288, Val Acc: 0.2053
  ✓ Saved best model (val_loss=5.2288)

Epoch 5/30
----------------------------------------
  Batch 50: Loss=5.0520, Acc=0.1851
  Batch 100: Loss=5.2464, Acc=0.2056
  Batch 150: Loss=5.0941, Acc=0.2054
  Batch 200: Loss=5.0146, Acc=0.2006
  Batch 250: Loss=5.1599, Acc=0.2047
