## SLURM PROLOG ###############################################################
##    Job ID : 14770557
##  Job Name : TinyAI
##  Nodelist : gpu2005
##      CPUs : 1
##  Mem/Node : 32000 MB
## Directory : /oscar/home/bpeckham/TinyAI
##   Job Started : Wed Dec 10 10:08:48 AM EST 2025
###############################################################################

==========================================
Job started at: Wed Dec 10 10:08:48 AM EST 2025
Job ID: 14770557
Node: gpu2005
==========================================

Python location: /users/bpeckham/.conda/envs/tinyai/bin/python
Python version: Python 3.10.19
Conda environment: tinyai
Installing/verifying required packages...
TensorFlow 2.15.0 and NumPy 1.26.4 imported successfully
GPU Information:
Wed Dec 10 10:08:58 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 6000                On  | 00000000:61:00.0 Off |                  Off |
| 33%   28C    P8              11W / 260W |     10MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     30530      G   /usr/libexec/Xorg                             8MiB |
+---------------------------------------------------------------------------------------+

TensorFlow GPU Detection:
TF version: 2.15.0
Built with CUDA: True
GPUs detected: 0
GPU devices: []

==========================================
Starting main Python script at Wed Dec 10 10:09:01 AM EST 2025
==========================================

Configuration:
  d_model: 256
  num_layers: 2
  num_heads: 4
  ff_dim: 512
  deep_rec_cycles: 3
  num_l_steps: 6
  epochs: 20
  batch_size: 32
  learning_rate: 1e-4
  max_seq_length: 256

To monitor training with WandB:
  1. First, log in to WandB (one time only):
     python wandb_login.py
  2. Then view metrics at: https://wandb.ai
  (No port forwarding needed - metrics upload automatically!)


============================================================
TinyAI TRAINING SCRIPT
============================================================

Configuration:
  Data path: /oscar/home/bpeckham/TinyAI/preprocessing/data/final_train_data.txt
  Vocab path: /oscar/home/bpeckham/TinyAI/preprocessing/data/vocab.json
  Max sequence length: 256
  Model dimension: 256
  Layers: 2
  Heads: 4
  FF dim: 512
  Dropout: 0.1
  Deep recursion cycles: 3
  Inner iterations: 6
  Deep supervision steps: 4
  Batch size: 32
  Epochs: 20
  Learning rate: 0.0001
============================================================

Loading tokenizer...
Loaded vocabulary with 9811 tokens
  PAD=0, UNK=1, BOS=20, EOS=2
Loading data...
Loaded 21794 samples
Train: 17436, Val: 2179, Test: 2179

############################################################
# RECURSIVE TRANSFORMER (TRM-Inspired)
############################################################

Recursive Model Parameters: 6,154,580

============================================================
TRAINING RECURSIVE TRANSFORMER (TRM-Inspired)
============================================================
  Deep recursion cycles (T): 3
  Inner iterations (n): 6
  Deep supervision steps: 4
  ACT loss weight: 0.1
  Halt exploration prob: 0.1
  Max halt steps: 16
============================================================


Epoch 1/20
----------------------------------------

==========================================
Python script finished at Wed Dec 10 10:09:26 AM EST 2025
Exit code: 137
==========================================
