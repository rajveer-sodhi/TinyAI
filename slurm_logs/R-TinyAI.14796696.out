## SLURM PROLOG ###############################################################
##    Job ID : 14796696
##  Job Name : TinyAI
##  Nodelist : gpu2006
##      CPUs : 1
##  Mem/Node : 64000 MB
## Directory : /oscar/home/bpeckham/TinyAI
##   Job Started : Thu Dec 11 03:54:19 AM EST 2025
###############################################################################

==========================================
Job started at: Thu Dec 11 03:54:19 AM EST 2025
Job ID: 14796696
Node: gpu2006
==========================================

Python location: /users/bpeckham/.conda/envs/tinyai/bin/python
Python version: Python 3.10.19
Conda environment: tinyai
Installing/verifying required packages...
TensorFlow 2.15.0 and NumPy 1.26.4 imported successfully
GPU Information:
Thu Dec 11 03:54:30 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 6000                On  | 00000000:3E:00.0 Off |                  Off |
| 33%   28C    P8               6W / 260W |     26MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     30575      G   /usr/libexec/Xorg                            24MiB |
+---------------------------------------------------------------------------------------+

TensorFlow GPU Detection:
TF version: 2.15.0
Built with CUDA: True
GPUs detected: 0
GPU devices: []

==========================================
Starting main Python script at Thu Dec 11 03:54:34 AM EST 2025
==========================================

Configuration:
  d_model: 16
  num_layers: 1
  num_heads: 2
  ff_dim: 16
  deep_rec_cycles: 2
  num_l_steps: 3
  epochs: 20
  batch_size: 8
  learning_rate: 1e-3
  max_seq_length: 128

To monitor training with WandB:
  1. First, log in to WandB (one time only):
     python wandb_login.py
  2. Then view metrics at: https://wandb.ai
  (No port forwarding needed - metrics upload automatically!)


============================================================
TinyAI TRAINING SCRIPT
============================================================

Configuration:
  Data path: /oscar/home/bpeckham/TinyAI/preprocessing/data/final_train_data.txt
  Vocab path: /oscar/home/bpeckham/TinyAI/preprocessing/data/vocab.json
  Max sequence length: 128
  Model dimension: 16
  Layers: 1
  Heads: 2
  FF dim: 16
  Dropout: 0.1
  Deep recursion cycles: 2
  Inner iterations: 3
  Deep supervision steps: 2
  Batch size: 8
  Epochs: 20
  Learning rate: 0.001
============================================================

Loading tokenizer...
Loaded vocabulary with 13678 tokens
  PAD=0, UNK=1, BOS=32, EOS=2
Loading data...
Loaded 7473 samples
Train: 5979, Val: 747, Test: 747

############################################################
# CONTROL TRANSFORMER
############################################################

Control Model Parameters: 455,182

============================================================
TRAINING CONTROL TRANSFORMER (Single-Pass Baseline)
============================================================


Epoch 1/20
----------------------------------------
  Batch 50: Loss=8.4561, Acc=0.0442
  Batch 100: Loss=7.0474, Acc=0.0297
  Batch 150: Loss=6.2257, Acc=0.0341
  Batch 200: Loss=6.0191, Acc=0.0452
  Batch 250: Loss=6.0565, Acc=0.0926
  Batch 300: Loss=5.6692, Acc=0.1631
  Batch 350: Loss=5.4996, Acc=0.1686
  Batch 400: Loss=5.6098, Acc=0.1744
  Batch 450: Loss=5.7013, Acc=0.1637
  Batch 500: Loss=5.3674, Acc=0.1971
  Batch 550: Loss=4.7889, Acc=0.2574
  Batch 600: Loss=5.0451, Acc=0.2479
  Batch 650: Loss=4.6231, Acc=0.2574
  Batch 700: Loss=4.9092, Acc=0.2608

  Train Loss: 5.9051, Train Acc: 0.1435
  Val Loss: 4.7276, Val Acc: 0.2464
  ✓ Saved best model (val_loss=4.7276)

Epoch 2/20
----------------------------------------
  Batch 50: Loss=5.1294, Acc=0.2214
  Batch 100: Loss=5.1678, Acc=0.2244
  Batch 150: Loss=4.5177, Acc=0.2455
  Batch 200: Loss=4.5295, Acc=0.2649
  Batch 250: Loss=4.5307, Acc=0.2613
  Batch 300: Loss=4.5726, Acc=0.2859
  Batch 350: Loss=4.5011, Acc=0.2443
  Batch 400: Loss=4.5437, Acc=0.2440
  Batch 450: Loss=4.7217, Acc=0.2628
  Batch 500: Loss=4.3878, Acc=0.2814
  Batch 550: Loss=4.3317, Acc=0.2640
  Batch 600: Loss=4.3569, Acc=0.2602
  Batch 650: Loss=4.4551, Acc=0.2608
  Batch 700: Loss=4.5110, Acc=0.2412

  Train Loss: 4.6031, Train Acc: 0.2506
  Val Loss: 4.3920, Val Acc: 0.2672
  ✓ Saved best model (val_loss=4.3920)

Epoch 3/20
----------------------------------------
  Batch 50: Loss=4.5361, Acc=0.2585
  Batch 100: Loss=4.5533, Acc=0.2695
  Batch 150: Loss=4.0870, Acc=0.2841
  Batch 200: Loss=4.5685, Acc=0.2421
  Batch 250: Loss=4.3887, Acc=0.2900
  Batch 300: Loss=4.1112, Acc=0.2734
  Batch 350: Loss=4.2108, Acc=0.2823
  Batch 400: Loss=4.7111, Acc=0.2278
  Batch 450: Loss=4.4211, Acc=0.2604
  Batch 500: Loss=4.4047, Acc=0.2515
  Batch 550: Loss=4.8639, Acc=0.2279
  Batch 600: Loss=4.3341, Acc=0.2581
  Batch 650: Loss=4.4222, Acc=0.2513
  Batch 700: Loss=4.3056, Acc=0.2546

  Train Loss: 4.3695, Train Acc: 0.2638
  Val Loss: 4.2791, Val Acc: 0.2775
  ✓ Saved best model (val_loss=4.2791)

Epoch 4/20
----------------------------------------
  Batch 50: Loss=4.3010, Acc=0.2500
  Batch 100: Loss=4.7254, Acc=0.2432
  Batch 150: Loss=4.3089, Acc=0.2902
  Batch 200: Loss=4.1799, Acc=0.2475
  Batch 250: Loss=4.4955, Acc=0.2505
  Batch 300: Loss=4.4351, Acc=0.2754
  Batch 350: Loss=3.9533, Acc=0.2712
  Batch 400: Loss=4.2894, Acc=0.2780
  Batch 450: Loss=4.4516, Acc=0.2725
  Batch 500: Loss=4.1931, Acc=0.2467
  Batch 550: Loss=4.1139, Acc=0.2953
  Batch 600: Loss=4.8590, Acc=0.2495
  Batch 650: Loss=4.4505, Acc=0.2621
  Batch 700: Loss=3.9960, Acc=0.2851

  Train Loss: 4.2589, Train Acc: 0.2699
  Val Loss: 4.2159, Val Acc: 0.2813
  ✓ Saved best model (val_loss=4.2159)

Epoch 5/20
----------------------------------------
  Batch 50: Loss=4.4812, Acc=0.2578
  Batch 100: Loss=4.2945, Acc=0.2594
  Batch 150: Loss=4.2353, Acc=0.3125
  Batch 200: Loss=4.1130, Acc=0.2872
  Batch 250: Loss=3.9105, Acc=0.3225
  Batch 300: Loss=4.3006, Acc=0.2745
  Batch 350: Loss=4.1295, Acc=0.2873
  Batch 400: Loss=4.0418, Acc=0.2599
  Batch 450: Loss=3.9587, Acc=0.2901
  Batch 500: Loss=3.9504, Acc=0.2770
  Batch 550: Loss=4.2967, Acc=0.2710
  Batch 600: Loss=4.5422, Acc=0.2923
  Batch 650: Loss=4.1464, Acc=0.2979
  Batch 700: Loss=4.4314, Acc=0.2411

  Train Loss: 4.1811, Train Acc: 0.2757
  Val Loss: 4.1604, Val Acc: 0.2889
  ✓ Saved best model (val_loss=4.1604)

Epoch 6/20
----------------------------------------
  Batch 50: Loss=4.4031, Acc=0.2520
  Batch 100: Loss=3.7172, Acc=0.3140
  Batch 150: Loss=4.2133, Acc=0.2659
  Batch 200: Loss=4.1379, Acc=0.2779
  Batch 250: Loss=3.8376, Acc=0.2923
  Batch 300: Loss=4.0436, Acc=0.3067
  Batch 350: Loss=4.4335, Acc=0.2654
  Batch 400: Loss=4.0760, Acc=0.2897
  Batch 450: Loss=4.3777, Acc=0.2730
  Batch 500: Loss=4.1292, Acc=0.2675
  Batch 550: Loss=4.2610, Acc=0.2726
  Batch 600: Loss=3.9631, Acc=0.2990
  Batch 650: Loss=3.6882, Acc=0.3379
  Batch 700: Loss=3.9770, Acc=0.2876

  Train Loss: 4.1207, Train Acc: 0.2795
  Val Loss: 4.1238, Val Acc: 0.2950
  ✓ Saved best model (val_loss=4.1238)

Epoch 7/20
----------------------------------------
  Batch 50: Loss=4.2510, Acc=0.2815
  Batch 100: Loss=4.1347, Acc=0.2477
  Batch 150: Loss=4.0659, Acc=0.2701
  Batch 200: Loss=4.2791, Acc=0.2569
  Batch 250: Loss=4.1145, Acc=0.2530
  Batch 300: Loss=3.9896, Acc=0.3068
  Batch 350: Loss=4.3196, Acc=0.2765
  Batch 400: Loss=4.1951, Acc=0.2492
  Batch 450: Loss=4.1027, Acc=0.3148
  Batch 500: Loss=3.8064, Acc=0.3038
  Batch 550: Loss=4.0826, Acc=0.2658
  Batch 600: Loss=3.8832, Acc=0.2882
  Batch 650: Loss=3.8154, Acc=0.3153
  Batch 700: Loss=4.3021, Acc=0.2606

  Train Loss: 4.0727, Train Acc: 0.2826
  Val Loss: 4.0855, Val Acc: 0.2989
  ✓ Saved best model (val_loss=4.0855)

Epoch 8/20
----------------------------------------
  Batch 50: Loss=4.1504, Acc=0.2768
  Batch 100: Loss=4.1041, Acc=0.2941
  Batch 150: Loss=4.3109, Acc=0.2748
  Batch 200: Loss=4.0822, Acc=0.2840
  Batch 250: Loss=4.1224, Acc=0.2525
  Batch 300: Loss=3.9255, Acc=0.2885
  Batch 350: Loss=4.0589, Acc=0.2825
  Batch 400: Loss=4.0604, Acc=0.2781
  Batch 450: Loss=3.9396, Acc=0.2842
  Batch 500: Loss=3.7756, Acc=0.3134
  Batch 550: Loss=4.2948, Acc=0.2982
  Batch 600: Loss=4.2250, Acc=0.2661
  Batch 650: Loss=3.8758, Acc=0.3126
  Batch 700: Loss=4.1149, Acc=0.3045

  Train Loss: 4.0296, Train Acc: 0.2851
  Val Loss: 4.0532, Val Acc: 0.3031
  ✓ Saved best model (val_loss=4.0532)

Epoch 9/20
----------------------------------------
  Batch 50: Loss=4.0403, Acc=0.2891
  Batch 100: Loss=3.8402, Acc=0.3109
  Batch 150: Loss=4.0319, Acc=0.2886
  Batch 200: Loss=3.9056, Acc=0.2936
  Batch 250: Loss=4.0415, Acc=0.2806
  Batch 300: Loss=4.0669, Acc=0.2963
  Batch 350: Loss=4.0888, Acc=0.2815
  Batch 400: Loss=3.6155, Acc=0.3281
  Batch 450: Loss=3.8402, Acc=0.2805
  Batch 500: Loss=4.1371, Acc=0.2764
  Batch 550: Loss=3.8954, Acc=0.2638
  Batch 600: Loss=3.8761, Acc=0.3040
  Batch 650: Loss=3.9980, Acc=0.2732
  Batch 700: Loss=3.8852, Acc=0.3019

  Train Loss: 3.9913, Train Acc: 0.2868
  Val Loss: 4.0199, Val Acc: 0.3062
  ✓ Saved best model (val_loss=4.0199)

Epoch 10/20
----------------------------------------
  Batch 50: Loss=3.5760, Acc=0.3251
  Batch 100: Loss=3.6603, Acc=0.3265
  Batch 150: Loss=4.0525, Acc=0.2699
  Batch 200: Loss=3.9448, Acc=0.2827
  Batch 250: Loss=4.0103, Acc=0.2795
  Batch 300: Loss=3.9659, Acc=0.2584
  Batch 350: Loss=4.1886, Acc=0.2997
  Batch 400: Loss=4.2753, Acc=0.2630
  Batch 450: Loss=4.0478, Acc=0.2657
  Batch 500: Loss=3.5518, Acc=0.3166
  Batch 550: Loss=3.9808, Acc=0.2705
  Batch 600: Loss=3.8246, Acc=0.2812
  Batch 650: Loss=4.4076, Acc=0.2471
  Batch 700: Loss=3.8421, Acc=0.2800

  Train Loss: 3.9581, Train Acc: 0.2875
  Val Loss: 3.9894, Val Acc: 0.3100
  ✓ Saved best model (val_loss=3.9894)

Epoch 11/20
----------------------------------------
  Batch 50: Loss=3.7638, Acc=0.2937
  Batch 100: Loss=3.7414, Acc=0.2756
  Batch 150: Loss=4.0803, Acc=0.2727
  Batch 200: Loss=3.8681, Acc=0.3021
  Batch 250: Loss=4.0276, Acc=0.2951
  Batch 300: Loss=3.8086, Acc=0.2895
  Batch 350: Loss=3.7662, Acc=0.3340
  Batch 400: Loss=3.8283, Acc=0.2926
  Batch 450: Loss=3.9931, Acc=0.3061
  Batch 500: Loss=3.8218, Acc=0.3231
  Batch 550: Loss=4.0764, Acc=0.2968
  Batch 600: Loss=3.9178, Acc=0.2977
  Batch 650: Loss=3.6901, Acc=0.3244
  Batch 700: Loss=3.9553, Acc=0.2834

  Train Loss: 3.9236, Train Acc: 0.2896
  Val Loss: 3.9599, Val Acc: 0.3147
  ✓ Saved best model (val_loss=3.9599)

Epoch 12/20
----------------------------------------
  Batch 50: Loss=3.7446, Acc=0.3114
  Batch 100: Loss=3.8019, Acc=0.3024
  Batch 150: Loss=4.1652, Acc=0.2638
  Batch 200: Loss=3.9057, Acc=0.2762
  Batch 250: Loss=4.0286, Acc=0.2941
  Batch 300: Loss=3.8838, Acc=0.3137
  Batch 350: Loss=3.7439, Acc=0.3038
  Batch 400: Loss=4.1507, Acc=0.3142
  Batch 450: Loss=3.8516, Acc=0.2755
  Batch 500: Loss=3.7103, Acc=0.2899
  Batch 550: Loss=3.9945, Acc=0.3036
  Batch 600: Loss=3.7774, Acc=0.2894
  Batch 650: Loss=4.0935, Acc=0.2618
  Batch 700: Loss=3.4666, Acc=0.3410

  Train Loss: 3.8945, Train Acc: 0.2910
  Val Loss: 3.9288, Val Acc: 0.3172
  ✓ Saved best model (val_loss=3.9288)

Epoch 13/20
----------------------------------------
  Batch 50: Loss=3.8236, Acc=0.2834
  Batch 100: Loss=3.4343, Acc=0.3340
  Batch 150: Loss=3.8543, Acc=0.3036
  Batch 200: Loss=3.6735, Acc=0.2874
  Batch 250: Loss=4.0351, Acc=0.2785
  Batch 300: Loss=3.9483, Acc=0.2743
  Batch 350: Loss=3.5601, Acc=0.3303
  Batch 400: Loss=3.8003, Acc=0.2878
  Batch 450: Loss=3.8989, Acc=0.2802
  Batch 500: Loss=3.9002, Acc=0.2975
  Batch 550: Loss=3.6335, Acc=0.3279
  Batch 600: Loss=4.1676, Acc=0.2844
  Batch 650: Loss=3.9985, Acc=0.2628
  Batch 700: Loss=4.0616, Acc=0.2847

  Train Loss: 3.8692, Train Acc: 0.2923
  Val Loss: 3.9067, Val Acc: 0.3197
  ✓ Saved best model (val_loss=3.9067)

Epoch 14/20
----------------------------------------
  Batch 50: Loss=3.7608, Acc=0.3043
  Batch 100: Loss=3.9881, Acc=0.2770
  Batch 150: Loss=3.8980, Acc=0.3167
  Batch 200: Loss=3.8742, Acc=0.3095
  Batch 250: Loss=3.7283, Acc=0.3169
  Batch 300: Loss=3.7626, Acc=0.3010
  Batch 350: Loss=3.8875, Acc=0.2940
  Batch 400: Loss=3.8448, Acc=0.2950
  Batch 450: Loss=3.3712, Acc=0.3431
  Batch 500: Loss=3.9806, Acc=0.2704
  Batch 550: Loss=4.0427, Acc=0.3036
  Batch 600: Loss=3.6479, Acc=0.3124
  Batch 650: Loss=4.1053, Acc=0.2782
  Batch 700: Loss=3.8843, Acc=0.2885

  Train Loss: 3.8461, Train Acc: 0.2935
  Val Loss: 3.8773, Val Acc: 0.3232
  ✓ Saved best model (val_loss=3.8773)

Epoch 15/20
----------------------------------------
  Batch 50: Loss=3.8279, Acc=0.3046
  Batch 100: Loss=4.1343, Acc=0.2747
  Batch 150: Loss=3.8676, Acc=0.3165
  Batch 200: Loss=3.6147, Acc=0.3180
  Batch 250: Loss=3.7294, Acc=0.2857
  Batch 300: Loss=3.9858, Acc=0.2808
  Batch 350: Loss=3.6598, Acc=0.3033
  Batch 400: Loss=3.4631, Acc=0.3500
  Batch 450: Loss=3.8375, Acc=0.3002
  Batch 500: Loss=3.9622, Acc=0.2797
  Batch 550: Loss=4.1224, Acc=0.2706
  Batch 600: Loss=4.2393, Acc=0.2615
  Batch 650: Loss=3.6903, Acc=0.3131
  Batch 700: Loss=3.9166, Acc=0.2925

  Train Loss: 3.8251, Train Acc: 0.2947
  Val Loss: 3.8608, Val Acc: 0.3258
  ✓ Saved best model (val_loss=3.8608)

Epoch 16/20
----------------------------------------
  Batch 50: Loss=3.7405, Acc=0.3014
  Batch 100: Loss=3.5387, Acc=0.3170
  Batch 150: Loss=3.8311, Acc=0.2919
  Batch 200: Loss=3.7824, Acc=0.2943
  Batch 250: Loss=3.8980, Acc=0.2756
  Batch 300: Loss=3.8767, Acc=0.2805
  Batch 350: Loss=3.8326, Acc=0.2831
  Batch 400: Loss=3.7041, Acc=0.2897
  Batch 450: Loss=3.8431, Acc=0.2790
  Batch 500: Loss=3.7245, Acc=0.2835
  Batch 550: Loss=4.2537, Acc=0.2735
  Batch 600: Loss=4.1298, Acc=0.2633
  Batch 650: Loss=4.0121, Acc=0.2703
  Batch 700: Loss=3.8350, Acc=0.2756

  Train Loss: 3.8029, Train Acc: 0.2962
  Val Loss: 3.8409, Val Acc: 0.3279
  ✓ Saved best model (val_loss=3.8409)

Epoch 17/20
----------------------------------------
  Batch 50: Loss=3.8486, Acc=0.2904
  Batch 100: Loss=3.9192, Acc=0.2821
  Batch 150: Loss=3.7882, Acc=0.3078
  Batch 200: Loss=3.9875, Acc=0.2857
  Batch 250: Loss=3.5822, Acc=0.3130
  Batch 300: Loss=3.7346, Acc=0.2669
  Batch 350: Loss=3.7493, Acc=0.3147
  Batch 400: Loss=4.0997, Acc=0.2546
  Batch 450: Loss=3.7747, Acc=0.3079
  Batch 500: Loss=3.7866, Acc=0.3253
  Batch 550: Loss=3.4981, Acc=0.3287
  Batch 600: Loss=3.6929, Acc=0.3416
  Batch 650: Loss=3.5947, Acc=0.3043
  Batch 700: Loss=3.8188, Acc=0.3043

  Train Loss: 3.7865, Train Acc: 0.2971
  Val Loss: 3.8161, Val Acc: 0.3335
  ✓ Saved best model (val_loss=3.8161)

Epoch 18/20
----------------------------------------
  Batch 50: Loss=3.6793, Acc=0.3059
  Batch 100: Loss=3.8285, Acc=0.3084
  Batch 150: Loss=3.8660, Acc=0.2837
  Batch 200: Loss=3.8325, Acc=0.2774
  Batch 250: Loss=3.6649, Acc=0.3090
  Batch 300: Loss=3.9094, Acc=0.2678
  Batch 350: Loss=3.5543, Acc=0.3199
  Batch 400: Loss=3.7689, Acc=0.3065
  Batch 450: Loss=3.8770, Acc=0.3007
  Batch 500: Loss=3.4717, Acc=0.3139
  Batch 550: Loss=4.0350, Acc=0.2668
  Batch 600: Loss=3.8983, Acc=0.3071
  Batch 650: Loss=3.7164, Acc=0.3132
  Batch 700: Loss=3.7725, Acc=0.3020

  Train Loss: 3.7689, Train Acc: 0.2979
  Val Loss: 3.7941, Val Acc: 0.3366
  ✓ Saved best model (val_loss=3.7941)

Epoch 19/20
----------------------------------------
  Batch 50: Loss=3.6702, Acc=0.3081
  Batch 100: Loss=4.0465, Acc=0.2798
  Batch 150: Loss=3.4461, Acc=0.3173
  Batch 200: Loss=3.6655, Acc=0.3040
  Batch 250: Loss=3.8199, Acc=0.3106
  Batch 300: Loss=3.9389, Acc=0.2928
  Batch 350: Loss=3.6379, Acc=0.3300
  Batch 400: Loss=3.7379, Acc=0.2876
  Batch 450: Loss=3.8226, Acc=0.2900
  Batch 500: Loss=3.9268, Acc=0.3012
  Batch 550: Loss=3.9845, Acc=0.2734
  Batch 600: Loss=3.5496, Acc=0.3278
  Batch 650: Loss=3.7537, Acc=0.2841
  Batch 700: Loss=3.8656, Acc=0.3030

  Train Loss: 3.7533, Train Acc: 0.2995
  Val Loss: 3.7736, Val Acc: 0.3382
  ✓ Saved best model (val_loss=3.7736)

Epoch 20/20
----------------------------------------
  Batch 50: Loss=3.6002, Acc=0.3058
  Batch 100: Loss=3.6824, Acc=0.3270
  Batch 150: Loss=3.3741, Acc=0.3421
  Batch 200: Loss=3.4984, Acc=0.3258
  Batch 250: Loss=3.4980, Acc=0.3201
  Batch 300: Loss=3.5921, Acc=0.3027
  Batch 350: Loss=3.7739, Acc=0.2862
  Batch 400: Loss=3.6234, Acc=0.2766
  Batch 450: Loss=3.5733, Acc=0.3072
  Batch 500: Loss=3.7730, Acc=0.3039
  Batch 550: Loss=3.9450, Acc=0.2781
  Batch 600: Loss=3.8609, Acc=0.2990
  Batch 650: Loss=3.8887, Acc=0.2889
  Batch 700: Loss=3.7450, Acc=0.2907

  Train Loss: 3.7423, Train Acc: 0.3004
  Val Loss: 3.7610, Val Acc: 0.3423
  ✓ Saved best model (val_loss=3.7610)

✓ Training complete. Best validation loss: 3.7610

Evaluating Control Transformer...
  Test Loss: 3.7708
  Test Accuracy: 0.3406

############################################################
# RECURSIVE TRANSFORMER (TRM-Inspired)
############################################################

Recursive Model Parameters: 455,231

============================================================
TRAINING RECURSIVE TRANSFORMER (TRM-Inspired)
============================================================
  Deep recursion cycles (T): 2
  Inner iterations (n): 3
  Deep supervision steps: 2
  ACT loss weight: 0.1
  Halt exploration prob: 0.1
  Max halt steps: 16
============================================================


Epoch 1/20
----------------------------------------
  Batch 50: Loss=8.3420, Acc=0.0394
  Batch 100: Loss=6.9663, Acc=0.0213
  Batch 150: Loss=6.0550, Acc=0.0334
  Batch 200: Loss=5.7368, Acc=0.0308
  Batch 250: Loss=6.2198, Acc=0.0000
  Batch 300: Loss=6.0063, Acc=0.0000
  Batch 350: Loss=5.5914, Acc=0.0043
  Batch 400: Loss=5.7161, Acc=0.0702
  Batch 450: Loss=5.6222, Acc=0.1242
  Batch 500: Loss=5.7930, Acc=0.1585
  Batch 550: Loss=5.4341, Acc=0.1632
  Batch 600: Loss=5.3247, Acc=0.2121
  Batch 650: Loss=5.1481, Acc=0.2029
  Batch 700: Loss=5.1484, Acc=0.2216

  Train Loss: 6.0171, Train Acc: 0.0983
  Val Loss: 4.9268, Val Acc: 0.2291
  ✓ Saved best model (val_loss=4.9268)

Epoch 2/20
----------------------------------------
  Batch 50: Loss=4.8348, Acc=0.2245
  Batch 100: Loss=4.6604, Acc=0.2256
  Batch 150: Loss=5.2785, Acc=0.2413
  Batch 200: Loss=4.0906, Acc=0.2804
  Batch 250: Loss=4.7589, Acc=0.2573
  Batch 300: Loss=4.7182, Acc=0.2371
  Batch 350: Loss=4.0157, Acc=0.2893
  Batch 400: Loss=4.6533, Acc=0.2495
  Batch 450: Loss=4.4165, Acc=0.2561
  Batch 500: Loss=4.5813, Acc=0.2687
  Batch 550: Loss=4.1836, Acc=0.2553
  Batch 600: Loss=4.5400, Acc=0.2372
  Batch 650: Loss=4.5043, Acc=0.2688
  Batch 700: Loss=4.7262, Acc=0.2474

  Train Loss: 4.6045, Train Acc: 0.2500
  Val Loss: 4.5028, Val Acc: 0.2578
  ✓ Saved best model (val_loss=4.5028)

Epoch 3/20
----------------------------------------
  Batch 50: Loss=4.1071, Acc=0.2721
  Batch 100: Loss=4.6281, Acc=0.2429
  Batch 150: Loss=4.3948, Acc=0.2676
  Batch 200: Loss=4.5368, Acc=0.2358
  Batch 250: Loss=4.5751, Acc=0.2607
  Batch 300: Loss=4.6223, Acc=0.2441
  Batch 350: Loss=4.2336, Acc=0.2992
  Batch 400: Loss=4.3553, Acc=0.2690
  Batch 450: Loss=4.3917, Acc=0.2897
  Batch 500: Loss=4.4562, Acc=0.2467
  Batch 550: Loss=4.4692, Acc=0.2765
  Batch 600: Loss=4.3526, Acc=0.2961
  Batch 650: Loss=4.6719, Acc=0.2679
  Batch 700: Loss=4.6112, Acc=0.2552

  Train Loss: 4.3529, Train Acc: 0.2654
  Val Loss: 4.3804, Val Acc: 0.2651
  ✓ Saved best model (val_loss=4.3804)

Epoch 4/20
----------------------------------------
  Batch 50: Loss=4.1106, Acc=0.2899
  Batch 100: Loss=4.3742, Acc=0.2650
  Batch 150: Loss=4.4083, Acc=0.2485
  Batch 200: Loss=4.2379, Acc=0.2667
  Batch 250: Loss=4.6355, Acc=0.2704
  Batch 300: Loss=4.1160, Acc=0.2511
  Batch 350: Loss=4.0361, Acc=0.3315
  Batch 400: Loss=4.5424, Acc=0.2687
  Batch 450: Loss=3.9205, Acc=0.2938
  Batch 500: Loss=4.2753, Acc=0.2933
  Batch 550: Loss=4.0641, Acc=0.2754
  Batch 600: Loss=4.0303, Acc=0.2883
  Batch 650: Loss=4.4036, Acc=0.2947
  Batch 700: Loss=4.3616, Acc=0.2984

  Train Loss: 4.2469, Train Acc: 0.2752
  Val Loss: 4.3223, Val Acc: 0.2712
  ✓ Saved best model (val_loss=4.3223)

Epoch 5/20
----------------------------------------
  Batch 50: Loss=3.4774, Acc=0.2860
  Batch 100: Loss=3.9879, Acc=0.3101
  Batch 150: Loss=4.6068, Acc=0.2278
  Batch 200: Loss=4.4768, Acc=0.2741
  Batch 250: Loss=3.6731, Acc=0.2808
  Batch 300: Loss=4.6194, Acc=0.2548
  Batch 350: Loss=4.7811, Acc=0.2313
  Batch 400: Loss=3.8707, Acc=0.3268
  Batch 450: Loss=4.5367, Acc=0.2815
  Batch 500: Loss=4.2170, Acc=0.3093
  Batch 550: Loss=4.0217, Acc=0.2815
  Batch 600: Loss=4.3667, Acc=0.2804
  Batch 650: Loss=4.4210, Acc=0.2672
  Batch 700: Loss=4.2235, Acc=0.2283

  Train Loss: 4.1775, Train Acc: 0.2808
  Val Loss: 4.2865, Val Acc: 0.2752
  ✓ Saved best model (val_loss=4.2865)

Epoch 6/20
----------------------------------------
  Batch 50: Loss=4.0612, Acc=0.3026
  Batch 100: Loss=3.9958, Acc=0.2949
  Batch 150: Loss=4.4508, Acc=0.2725
  Batch 200: Loss=3.7196, Acc=0.2890
  Batch 250: Loss=3.8182, Acc=0.3144
  Batch 300: Loss=3.9766, Acc=0.2774
  Batch 350: Loss=3.9031, Acc=0.3124
  Batch 400: Loss=4.1626, Acc=0.3082
  Batch 450: Loss=3.9526, Acc=0.2797
  Batch 500: Loss=3.8750, Acc=0.3140
  Batch 550: Loss=4.4467, Acc=0.2671
  Batch 600: Loss=3.6606, Acc=0.3220
  Batch 650: Loss=4.0565, Acc=0.2818
  Batch 700: Loss=4.0401, Acc=0.2994

  Train Loss: 4.1248, Train Acc: 0.2856
  Val Loss: 4.2561, Val Acc: 0.2780
  ✓ Saved best model (val_loss=4.2561)

Epoch 7/20
----------------------------------------
  Batch 50: Loss=3.8369, Acc=0.3089
  Batch 100: Loss=4.0279, Acc=0.2683
  Batch 150: Loss=4.2949, Acc=0.2761
  Batch 200: Loss=4.1515, Acc=0.2796
  Batch 250: Loss=4.0477, Acc=0.2867
  Batch 300: Loss=4.2178, Acc=0.2851
  Batch 350: Loss=4.1708, Acc=0.2942
  Batch 400: Loss=4.0913, Acc=0.2814
  Batch 450: Loss=4.1096, Acc=0.3071
  Batch 500: Loss=3.6595, Acc=0.3322
  Batch 550: Loss=3.9936, Acc=0.3005
  Batch 600: Loss=3.9165, Acc=0.3033
  Batch 650: Loss=3.9536, Acc=0.3152
  Batch 700: Loss=4.1049, Acc=0.2790

  Train Loss: 4.0799, Train Acc: 0.2895
  Val Loss: 4.2320, Val Acc: 0.2807
  ✓ Saved best model (val_loss=4.2320)

Epoch 8/20
----------------------------------------
  Batch 50: Loss=3.4296, Acc=0.3355
  Batch 100: Loss=3.8597, Acc=0.2958
