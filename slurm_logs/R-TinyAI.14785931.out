## SLURM PROLOG ###############################################################
##    Job ID : 14785931
##  Job Name : TinyAI
##  Nodelist : gpu2003
##      CPUs : 1
##  Mem/Node : 64000 MB
## Directory : /oscar/home/bpeckham/TinyAI
##   Job Started : Wed Dec 10 02:37:55 PM EST 2025
###############################################################################

==========================================
Job started at: Wed Dec 10 02:37:55 PM EST 2025
Job ID: 14785931
Node: gpu2003
==========================================

Python location: /users/bpeckham/.conda/envs/tinyai/bin/python
Python version: Python 3.10.19
Conda environment: tinyai
Installing/verifying required packages...
TensorFlow 2.15.0 and NumPy 1.26.4 imported successfully
GPU Information:
Wed Dec 10 14:38:07 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 6000                On  | 00000000:40:00.0 Off |                  Off |
| 33%   31C    P8               5W / 260W |     10MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     30829      G   /usr/libexec/Xorg                             8MiB |
+---------------------------------------------------------------------------------------+

TensorFlow GPU Detection:
TF version: 2.15.0
Built with CUDA: True
GPUs detected: 0
GPU devices: []

==========================================
Starting main Python script at Wed Dec 10 02:38:11 PM EST 2025
==========================================

Configuration:
  d_model: 64
  num_layers: 2
  num_heads: 3
  ff_dim: 64
  deep_rec_cycles: 1
  num_l_steps: 2
  epochs: 15
  batch_size: 8
  learning_rate: 1e-4
  max_seq_length: 128

To monitor training with WandB:
  1. First, log in to WandB (one time only):
     python wandb_login.py
  2. Then view metrics at: https://wandb.ai
  (No port forwarding needed - metrics upload automatically!)


============================================================
TinyAI TRAINING SCRIPT
============================================================

Configuration:
  Data path: /oscar/home/bpeckham/TinyAI/preprocessing/data/final_train_data.txt
  Vocab path: /oscar/home/bpeckham/TinyAI/preprocessing/data/vocab.json
  Max sequence length: 128
  Model dimension: 64
  Layers: 2
  Heads: 3
  FF dim: 64
  Dropout: 0.1
  Deep recursion cycles: 1
  Inner iterations: 2
  Deep supervision steps: 2
  Batch size: 8
  Epochs: 15
  Learning rate: 0.0001
============================================================

Loading tokenizer...
Loaded vocabulary with 7173 tokens
  PAD=0, UNK=1, BOS=18, EOS=2
Loading data...
Loaded 14180 samples
Train: 11344, Val: 1418, Test: 1418

############################################################
# CONTROL TRANSFORMER
############################################################

Control Model Parameters: 983,679

============================================================
TRAINING CONTROL TRANSFORMER (Single-Pass Baseline)
============================================================


Epoch 1/15
----------------------------------------
  Batch 50: Loss=8.6774, Acc=0.0443
  Batch 100: Loss=8.2119, Acc=0.0824
  Batch 150: Loss=7.7119, Acc=0.0643
  Batch 200: Loss=7.3530, Acc=0.0392
  Batch 250: Loss=7.0466, Acc=0.0375
  Batch 300: Loss=6.6871, Acc=0.0531
  Batch 350: Loss=6.4226, Acc=0.0432
  Batch 400: Loss=6.3685, Acc=0.0485
  Batch 450: Loss=5.9855, Acc=0.0349
  Batch 500: Loss=6.1092, Acc=0.0611
  Batch 550: Loss=5.9112, Acc=0.0587
  Batch 600: Loss=5.9703, Acc=0.0789
  Batch 650: Loss=5.8269, Acc=0.0756
  Batch 700: Loss=5.9803, Acc=0.1056
  Batch 750: Loss=5.9761, Acc=0.1450
  Batch 800: Loss=5.7452, Acc=0.1106
  Batch 850: Loss=5.9149, Acc=0.1140
  Batch 900: Loss=5.8861, Acc=0.1109
  Batch 950: Loss=5.8343, Acc=0.1301
  Batch 1000: Loss=5.8327, Acc=0.1332
  Batch 1050: Loss=5.7242, Acc=0.1622
  Batch 1100: Loss=5.4936, Acc=0.1675
  Batch 1150: Loss=5.7240, Acc=0.1581
  Batch 1200: Loss=5.5719, Acc=0.1652
  Batch 1250: Loss=5.2379, Acc=0.2141
  Batch 1300: Loss=5.3735, Acc=0.2003
  Batch 1350: Loss=5.4337, Acc=0.1702
  Batch 1400: Loss=5.2414, Acc=0.1803

  Train Loss: 6.1994, Train Acc: 0.1096
  Val Loss: 5.1067, Val Acc: 0.2176
  ✓ Saved best model (val_loss=5.1067)

Epoch 2/15
----------------------------------------
  Batch 50: Loss=5.0881, Acc=0.2064
  Batch 100: Loss=5.0126, Acc=0.2138
  Batch 150: Loss=4.8995, Acc=0.2201
  Batch 200: Loss=5.0397, Acc=0.2299
  Batch 250: Loss=4.9003, Acc=0.2190
  Batch 300: Loss=4.9089, Acc=0.2445
  Batch 350: Loss=4.7168, Acc=0.2535
  Batch 400: Loss=4.5732, Acc=0.2488
  Batch 450: Loss=4.8201, Acc=0.2661
  Batch 500: Loss=4.9942, Acc=0.1998
  Batch 550: Loss=5.0001, Acc=0.2286
  Batch 600: Loss=4.8153, Acc=0.2400
  Batch 650: Loss=4.4628, Acc=0.2353
  Batch 700: Loss=4.8564, Acc=0.2417
  Batch 750: Loss=4.7378, Acc=0.2548
  Batch 800: Loss=4.4290, Acc=0.3017
  Batch 850: Loss=4.4095, Acc=0.2381
  Batch 900: Loss=4.9039, Acc=0.2334
  Batch 950: Loss=4.3674, Acc=0.2880
  Batch 1000: Loss=4.3813, Acc=0.2553
  Batch 1050: Loss=4.4584, Acc=0.2622
  Batch 1100: Loss=4.5571, Acc=0.2761
  Batch 1150: Loss=4.3244, Acc=0.3057
  Batch 1200: Loss=4.4851, Acc=0.2480
  Batch 1250: Loss=4.5758, Acc=0.2724
  Batch 1300: Loss=4.3035, Acc=0.2649
  Batch 1350: Loss=4.1115, Acc=0.2894
  Batch 1400: Loss=4.5480, Acc=0.2830

  Train Loss: 4.6561, Train Acc: 0.2529
  Val Loss: 4.2505, Val Acc: 0.2802
  ✓ Saved best model (val_loss=4.2505)

Epoch 3/15
----------------------------------------
  Batch 50: Loss=4.2214, Acc=0.3224
  Batch 100: Loss=4.1279, Acc=0.2891
  Batch 150: Loss=4.5462, Acc=0.2512
  Batch 200: Loss=4.1152, Acc=0.2710
  Batch 250: Loss=4.1867, Acc=0.2897
  Batch 300: Loss=4.0829, Acc=0.2938
  Batch 350: Loss=4.3188, Acc=0.2844
  Batch 400: Loss=4.0760, Acc=0.2812
  Batch 450: Loss=4.2158, Acc=0.2560
  Batch 500: Loss=4.3163, Acc=0.2614
  Batch 550: Loss=4.2531, Acc=0.2507
  Batch 600: Loss=4.2557, Acc=0.2671
  Batch 650: Loss=3.9165, Acc=0.3164
  Batch 700: Loss=4.1445, Acc=0.2794
  Batch 750: Loss=4.0694, Acc=0.3171
  Batch 800: Loss=3.9265, Acc=0.2797
  Batch 850: Loss=4.1876, Acc=0.2922
  Batch 900: Loss=4.0256, Acc=0.2891
  Batch 950: Loss=3.7986, Acc=0.3261
  Batch 1000: Loss=4.3490, Acc=0.2763
  Batch 1050: Loss=3.9586, Acc=0.3047
  Batch 1100: Loss=4.2925, Acc=0.2563
  Batch 1150: Loss=3.8108, Acc=0.3097
  Batch 1200: Loss=3.9473, Acc=0.3045
  Batch 1250: Loss=4.2139, Acc=0.2574
  Batch 1300: Loss=3.6699, Acc=0.3221
  Batch 1350: Loss=3.8168, Acc=0.2848
  Batch 1400: Loss=3.9772, Acc=0.3161

  Train Loss: 4.1146, Train Acc: 0.2889
  Val Loss: 3.9145, Val Acc: 0.3074
  ✓ Saved best model (val_loss=3.9145)

Epoch 4/15
----------------------------------------
  Batch 50: Loss=3.7632, Acc=0.3252
