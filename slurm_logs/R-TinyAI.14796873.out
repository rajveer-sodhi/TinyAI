## SLURM PROLOG ###############################################################
##    Job ID : 14796873
##  Job Name : TinyAI
##  Nodelist : gpu1402
##      CPUs : 1
##  Mem/Node : 64000 MB
## Directory : /oscar/home/bpeckham/TinyAI
##   Job Started : Thu Dec 11 04:58:23 AM EST 2025
###############################################################################

==========================================
Job started at: Thu Dec 11 04:58:23 AM EST 2025
Job ID: 14796873
Node: gpu1402
==========================================

Python location: /users/bpeckham/.conda/envs/tinyai/bin/python
Python version: Python 3.10.19
Conda environment: tinyai
Installing/verifying required packages...
TensorFlow 2.15.0 and NumPy 1.26.4 imported successfully
GPU Information:
Thu Dec 11 04:58:39 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA TITAN RTX               On  | 00000000:3D:00.0 Off |                  N/A |
| 41%   38C    P8              29W / 280W |     30MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     29134      G   /usr/libexec/Xorg                            27MiB |
+---------------------------------------------------------------------------------------+

TensorFlow GPU Detection:
TF version: 2.15.0
Built with CUDA: True
GPUs detected: 0
GPU devices: []

==========================================
Starting main Python script at Thu Dec 11 04:58:43 AM EST 2025
==========================================

Configuration:
  d_model: 16
  num_layers: 1
  num_heads: 2
  ff_dim: 16
  deep_rec_cycles: 2
  num_l_steps: 5
  epochs: 20
  batch_size: 8
  learning_rate: 1e-3
  max_seq_length: 128

To monitor training with WandB:
  1. First, log in to WandB (one time only):
     python wandb_login.py
  2. Then view metrics at: https://wandb.ai
  (No port forwarding needed - metrics upload automatically!)


============================================================
TinyAI TRAINING SCRIPT
============================================================

Configuration:
  Data path: /oscar/home/bpeckham/TinyAI/preprocessing/data/final_train_data.txt
  Vocab path: /oscar/home/bpeckham/TinyAI/preprocessing/data/vocab.json
  Max sequence length: 128
  Model dimension: 16
  Layers: 1
  Heads: 2
  FF dim: 16
  Dropout: 0.1
  Deep recursion cycles: 2
  Inner iterations: 5
  Deep supervision steps: 2
  Batch size: 8
  Epochs: 20
  Learning rate: 0.001
============================================================

Loading tokenizer...
Loaded vocabulary with 13678 tokens
  PAD=0, UNK=1, BOS=32, EOS=2
Loading data...
Loaded 7473 samples
Train: 5979, Val: 747, Test: 747

############################################################
# RECURSIVE TRANSFORMER (TRM-Inspired)
############################################################

Recursive Model Parameters: 455,231

============================================================
TRAINING RECURSIVE TRANSFORMER (TRM-Inspired)
============================================================
  Deep recursion cycles (T): 2
  Inner iterations (n): 5
  Deep supervision steps: 2
  ACT loss weight: 0.1
  Halt exploration prob: 0.1
  Max halt steps: 16
============================================================


Epoch 1/20
----------------------------------------
  Batch 50: Loss=8.4317, Acc=0.0449
  Batch 100: Loss=7.1884, Acc=0.0367
  Batch 150: Loss=6.4269, Acc=0.0338
  Batch 200: Loss=6.1647, Acc=0.0354
  Batch 250: Loss=5.7344, Acc=0.0041
  Batch 300: Loss=6.0183, Acc=0.0177
  Batch 350: Loss=6.0799, Acc=0.0281
  Batch 400: Loss=5.5823, Acc=0.0827
  Batch 450: Loss=5.3159, Acc=0.1700
  Batch 500: Loss=5.3368, Acc=0.1857
  Batch 550: Loss=5.2631, Acc=0.1848
  Batch 600: Loss=5.3857, Acc=0.2149
  Batch 650: Loss=4.9324, Acc=0.1948
  Batch 700: Loss=4.4286, Acc=0.2409

  Train Loss: 5.9505, Train Acc: 0.1085
  Val Loss: 4.8769, Val Acc: 0.2328
  ✓ Saved best model (val_loss=4.8769)

Epoch 2/20
----------------------------------------
  Batch 50: Loss=4.7307, Acc=0.2087
  Batch 100: Loss=4.5555, Acc=0.2559
  Batch 150: Loss=4.0405, Acc=0.2636
  Batch 200: Loss=5.2258, Acc=0.2244
  Batch 250: Loss=4.9284, Acc=0.2438
  Batch 300: Loss=4.5891, Acc=0.2461
  Batch 350: Loss=4.9811, Acc=0.2371
  Batch 400: Loss=4.3121, Acc=0.2467
  Batch 450: Loss=4.9420, Acc=0.2373
  Batch 500: Loss=4.9216, Acc=0.2359
  Batch 550: Loss=4.4648, Acc=0.2674
  Batch 600: Loss=4.4671, Acc=0.2645
  Batch 650: Loss=4.4423, Acc=0.2891
  Batch 700: Loss=4.5422, Acc=0.2451

  Train Loss: 4.5877, Train Acc: 0.2507
  Val Loss: 4.5185, Val Acc: 0.2587
  ✓ Saved best model (val_loss=4.5185)

Epoch 3/20
----------------------------------------
  Batch 50: Loss=5.2146, Acc=0.2087
  Batch 100: Loss=4.3314, Acc=0.2780
  Batch 150: Loss=4.0636, Acc=0.2703
  Batch 200: Loss=4.3791, Acc=0.2535
  Batch 250: Loss=4.7334, Acc=0.2490
  Batch 300: Loss=4.5765, Acc=0.2673
  Batch 350: Loss=4.2209, Acc=0.2693
