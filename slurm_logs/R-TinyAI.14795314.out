## SLURM PROLOG ###############################################################
##    Job ID : 14795314
##  Job Name : TinyAI
##  Nodelist : gpu2004
##      CPUs : 1
##  Mem/Node : 64000 MB
## Directory : /oscar/home/bpeckham/TinyAI
##   Job Started : Thu Dec 11 12:17:07 AM EST 2025
###############################################################################

==========================================
Job started at: Thu Dec 11 12:17:07 AM EST 2025
Job ID: 14795314
Node: gpu2004
==========================================

Python location: /users/bpeckham/.conda/envs/tinyai/bin/python
Python version: Python 3.10.19
Conda environment: tinyai
Installing/verifying required packages...
TensorFlow 2.15.0 and NumPy 1.26.4 imported successfully
GPU Information:
Thu Dec 11 00:17:18 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 6000                On  | 00000000:40:00.0 Off |                  Off |
| 33%   27C    P8              15W / 260W |     24MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     30537      G   /usr/libexec/Xorg                            22MiB |
+---------------------------------------------------------------------------------------+

TensorFlow GPU Detection:
TF version: 2.15.0
Built with CUDA: True
GPUs detected: 0
GPU devices: []

==========================================
Starting main Python script at Thu Dec 11 12:17:21 AM EST 2025
==========================================

Configuration:
  d_model: 32
  num_layers: 1
  num_heads: 2
  ff_dim: 32
  deep_rec_cycles: 2
  num_l_steps: 3
  epochs: 30
  batch_size: 8
  learning_rate: 1e-4
  max_seq_length: 128

To monitor training with WandB:
  1. First, log in to WandB (one time only):
     python wandb_login.py
  2. Then view metrics at: https://wandb.ai
  (No port forwarding needed - metrics upload automatically!)


============================================================
TinyAI TRAINING SCRIPT
============================================================

Configuration:
  Data path: /oscar/home/bpeckham/TinyAI/preprocessing/data/final_train_data.txt
  Vocab path: /oscar/home/bpeckham/TinyAI/preprocessing/data/vocab.json
  Max sequence length: 128
  Model dimension: 32
  Layers: 1
  Heads: 2
  FF dim: 32
  Dropout: 0.1
  Deep recursion cycles: 2
  Inner iterations: 3
  Deep supervision steps: 2
  Batch size: 8
  Epochs: 30
  Learning rate: 0.0001
============================================================

Loading tokenizer...
Loaded vocabulary with 13678 tokens
  PAD=0, UNK=1, BOS=32, EOS=2
Loading data...
Loaded 7473 samples
Train: 5979, Val: 747, Test: 747

############################################################
# CONTROL TRANSFORMER
############################################################

Control Model Parameters: 899,758

============================================================
TRAINING CONTROL TRANSFORMER (Single-Pass Baseline)
============================================================


Epoch 1/30
----------------------------------------
  Batch 50: Loss=9.4222, Acc=0.0182
  Batch 100: Loss=9.2381, Acc=0.0453
  Batch 150: Loss=8.9302, Acc=0.0509
  Batch 200: Loss=8.6042, Acc=0.0309
  Batch 250: Loss=8.3535, Acc=0.0350
  Batch 300: Loss=8.0857, Acc=0.0367
  Batch 350: Loss=7.8320, Acc=0.0322
  Batch 400: Loss=7.7542, Acc=0.0300
  Batch 450: Loss=7.4036, Acc=0.0420
  Batch 500: Loss=7.2607, Acc=0.0286
  Batch 550: Loss=7.1384, Acc=0.0320
  Batch 600: Loss=6.9265, Acc=0.0322
  Batch 650: Loss=6.6595, Acc=0.0348
  Batch 700: Loss=6.5847, Acc=0.0417

  Train Loss: 7.9074, Train Acc: 0.0335
  Val Loss: 6.5272, Val Acc: 0.0433
  ✓ Saved best model (val_loss=6.5272)

Epoch 2/30
----------------------------------------
  Batch 50: Loss=6.5596, Acc=0.0329
  Batch 100: Loss=6.4909, Acc=0.0412
  Batch 150: Loss=6.3268, Acc=0.0465
  Batch 200: Loss=6.1756, Acc=0.0446
  Batch 250: Loss=6.3063, Acc=0.0392
  Batch 300: Loss=6.1152, Acc=0.0336
  Batch 350: Loss=6.1323, Acc=0.0386
  Batch 400: Loss=6.0023, Acc=0.0383
  Batch 450: Loss=6.1517, Acc=0.0469
  Batch 500: Loss=5.8477, Acc=0.0487
  Batch 550: Loss=5.9372, Acc=0.0432
  Batch 600: Loss=5.8645, Acc=0.0633
  Batch 650: Loss=6.0003, Acc=0.0512
  Batch 700: Loss=6.1263, Acc=0.0574

  Train Loss: 6.1665, Train Acc: 0.0415
  Val Loss: 6.0152, Val Acc: 0.0638
  ✓ Saved best model (val_loss=6.0152)

Epoch 3/30
----------------------------------------
  Batch 50: Loss=5.8349, Acc=0.0979
  Batch 100: Loss=5.8475, Acc=0.0773
  Batch 150: Loss=5.8427, Acc=0.1043
  Batch 200: Loss=5.9485, Acc=0.1212
  Batch 250: Loss=5.7218, Acc=0.1511
  Batch 300: Loss=5.9994, Acc=0.1221
  Batch 350: Loss=5.6301, Acc=0.1421
  Batch 400: Loss=5.7037, Acc=0.1639
  Batch 450: Loss=5.7605, Acc=0.1628
  Batch 500: Loss=5.6657, Acc=0.1591
  Batch 550: Loss=5.6924, Acc=0.1814
  Batch 600: Loss=5.4096, Acc=0.2058
  Batch 650: Loss=5.6981, Acc=0.1731
  Batch 700: Loss=5.6386, Acc=0.2002

  Train Loss: 5.8289, Train Acc: 0.1436
  Val Loss: 5.6243, Val Acc: 0.2007
  ✓ Saved best model (val_loss=5.6243)

Epoch 4/30
----------------------------------------
  Batch 50: Loss=5.0715, Acc=0.2377
  Batch 100: Loss=5.3344, Acc=0.2110
  Batch 150: Loss=5.3952, Acc=0.2294
  Batch 200: Loss=5.3577, Acc=0.2141
  Batch 250: Loss=5.3825, Acc=0.1994
  Batch 300: Loss=5.2685, Acc=0.2212
  Batch 350: Loss=5.1387, Acc=0.2338
  Batch 400: Loss=5.2038, Acc=0.2071
  Batch 450: Loss=4.9695, Acc=0.2239
  Batch 500: Loss=5.5017, Acc=0.2098
  Batch 550: Loss=5.2759, Acc=0.2202
  Batch 600: Loss=5.1396, Acc=0.2165
  Batch 650: Loss=5.4962, Acc=0.1956
  Batch 700: Loss=5.1004, Acc=0.2170

  Train Loss: 5.3634, Train Acc: 0.2103
  Val Loss: 5.1128, Val Acc: 0.2255
  ✓ Saved best model (val_loss=5.1128)

Epoch 5/30
----------------------------------------
  Batch 50: Loss=5.1460, Acc=0.2043
  Batch 100: Loss=5.1635, Acc=0.2246
  Batch 150: Loss=4.8310, Acc=0.2364
  Batch 200: Loss=5.0459, Acc=0.2296
  Batch 250: Loss=5.0810, Acc=0.2417
  Batch 300: Loss=4.8424, Acc=0.2535
  Batch 350: Loss=4.9422, Acc=0.2418
  Batch 400: Loss=4.8045, Acc=0.2547
  Batch 450: Loss=4.5906, Acc=0.2708
  Batch 500: Loss=4.9370, Acc=0.2401
  Batch 550: Loss=4.7299, Acc=0.2378
  Batch 600: Loss=4.9480, Acc=0.2320
  Batch 650: Loss=4.8178, Acc=0.2400
  Batch 700: Loss=4.7296, Acc=0.2401

  Train Loss: 4.9542, Train Acc: 0.2339
  Val Loss: 4.8172, Val Acc: 0.2451
  ✓ Saved best model (val_loss=4.8172)

Epoch 6/30
----------------------------------------
  Batch 50: Loss=4.5195, Acc=0.2519
  Batch 100: Loss=4.6390, Acc=0.2762
  Batch 150: Loss=4.7011, Acc=0.2581
  Batch 200: Loss=4.7848, Acc=0.2428
  Batch 250: Loss=4.7303, Acc=0.2478
  Batch 300: Loss=4.8360, Acc=0.2457
  Batch 350: Loss=4.7509, Acc=0.2507
  Batch 400: Loss=4.7533, Acc=0.2356
  Batch 450: Loss=4.6778, Acc=0.2602
  Batch 500: Loss=4.4634, Acc=0.2647
  Batch 550: Loss=4.7986, Acc=0.2576
  Batch 600: Loss=4.4608, Acc=0.2641
  Batch 650: Loss=4.3313, Acc=0.2771
  Batch 700: Loss=4.4664, Acc=0.2862

  Train Loss: 4.7169, Train Acc: 0.2495
  Val Loss: 4.6464, Val Acc: 0.2551
  ✓ Saved best model (val_loss=4.6464)

Epoch 7/30
----------------------------------------
  Batch 50: Loss=4.7752, Acc=0.2536
  Batch 100: Loss=4.8491, Acc=0.2401
  Batch 150: Loss=4.5379, Acc=0.2571
  Batch 200: Loss=4.5297, Acc=0.2723
  Batch 250: Loss=4.6118, Acc=0.2766
  Batch 300: Loss=5.2510, Acc=0.2175
  Batch 350: Loss=4.4688, Acc=0.2845
  Batch 400: Loss=4.3060, Acc=0.2937
  Batch 450: Loss=4.3264, Acc=0.2802
  Batch 500: Loss=4.2353, Acc=0.2730
  Batch 550: Loss=4.2837, Acc=0.2710
  Batch 600: Loss=4.3074, Acc=0.2576
  Batch 650: Loss=4.6475, Acc=0.2698
  Batch 700: Loss=4.6320, Acc=0.2356

  Train Loss: 4.5620, Train Acc: 0.2585
  Val Loss: 4.5337, Val Acc: 0.2625
  ✓ Saved best model (val_loss=4.5337)

Epoch 8/30
----------------------------------------
  Batch 50: Loss=4.2900, Acc=0.2620
  Batch 100: Loss=4.2757, Acc=0.2791
  Batch 150: Loss=4.5993, Acc=0.2400
  Batch 200: Loss=4.5413, Acc=0.2581
  Batch 250: Loss=4.2669, Acc=0.2587
  Batch 300: Loss=4.3313, Acc=0.2894
  Batch 350: Loss=4.5425, Acc=0.2251
  Batch 400: Loss=4.2969, Acc=0.2832
  Batch 450: Loss=4.4012, Acc=0.2581
  Batch 500: Loss=4.6051, Acc=0.2643
  Batch 550: Loss=4.4707, Acc=0.2564
  Batch 600: Loss=4.4015, Acc=0.2566
  Batch 650: Loss=4.3122, Acc=0.2703
  Batch 700: Loss=4.6321, Acc=0.2710

  Train Loss: 4.4520, Train Acc: 0.2645
  Val Loss: 4.4561, Val Acc: 0.2666
  ✓ Saved best model (val_loss=4.4561)

Epoch 9/30
----------------------------------------
  Batch 50: Loss=4.2680, Acc=0.2687
  Batch 100: Loss=4.3356, Acc=0.2691
  Batch 150: Loss=4.5532, Acc=0.2564
  Batch 200: Loss=4.5537, Acc=0.2340
  Batch 250: Loss=4.5722, Acc=0.2474
  Batch 300: Loss=4.3706, Acc=0.2727
  Batch 350: Loss=4.2826, Acc=0.2827
  Batch 400: Loss=4.6892, Acc=0.2436
  Batch 450: Loss=4.1850, Acc=0.3018
  Batch 500: Loss=4.5944, Acc=0.2375
  Batch 550: Loss=4.5674, Acc=0.2361
  Batch 600: Loss=4.1078, Acc=0.3028
  Batch 650: Loss=4.3564, Acc=0.2594
  Batch 700: Loss=4.2218, Acc=0.2657

  Train Loss: 4.3696, Train Acc: 0.2684
  Val Loss: 4.3979, Val Acc: 0.2699
  ✓ Saved best model (val_loss=4.3979)

Epoch 10/30
----------------------------------------
  Batch 50: Loss=4.3014, Acc=0.2495
  Batch 100: Loss=4.2401, Acc=0.2673
  Batch 150: Loss=4.2826, Acc=0.2762
  Batch 200: Loss=4.1575, Acc=0.2755
  Batch 250: Loss=4.3784, Acc=0.2856
  Batch 300: Loss=4.5155, Acc=0.2492
  Batch 350: Loss=4.3862, Acc=0.2375
  Batch 400: Loss=4.0592, Acc=0.2827
  Batch 450: Loss=4.1562, Acc=0.2651
  Batch 500: Loss=4.1792, Acc=0.2653
  Batch 550: Loss=4.0600, Acc=0.2882
  Batch 600: Loss=4.6833, Acc=0.2345
  Batch 650: Loss=4.2008, Acc=0.2783
  Batch 700: Loss=4.2989, Acc=0.2837

  Train Loss: 4.3055, Train Acc: 0.2712
  Val Loss: 4.3535, Val Acc: 0.2726
  ✓ Saved best model (val_loss=4.3535)

Epoch 11/30
----------------------------------------
  Batch 50: Loss=4.1888, Acc=0.2600
  Batch 100: Loss=4.3158, Acc=0.2778
  Batch 150: Loss=4.4860, Acc=0.2598
  Batch 200: Loss=3.9334, Acc=0.3001
  Batch 250: Loss=3.9623, Acc=0.3159
  Batch 300: Loss=4.1878, Acc=0.2698
  Batch 350: Loss=4.3216, Acc=0.2619
  Batch 400: Loss=4.0615, Acc=0.3184
  Batch 450: Loss=3.9859, Acc=0.2925
  Batch 500: Loss=4.1297, Acc=0.3019
  Batch 550: Loss=4.5994, Acc=0.2585
  Batch 600: Loss=4.1027, Acc=0.3043
  Batch 650: Loss=4.0320, Acc=0.2898
  Batch 700: Loss=4.1692, Acc=0.3076

  Train Loss: 4.2526, Train Acc: 0.2744
  Val Loss: 4.3185, Val Acc: 0.2751
  ✓ Saved best model (val_loss=4.3185)

Epoch 12/30
----------------------------------------
  Batch 50: Loss=4.4252, Acc=0.2557
  Batch 100: Loss=3.8419, Acc=0.2744
  Batch 150: Loss=4.2655, Acc=0.2847
  Batch 200: Loss=4.1293, Acc=0.2951
  Batch 250: Loss=4.4280, Acc=0.2785
  Batch 300: Loss=3.9995, Acc=0.2774
  Batch 350: Loss=4.0342, Acc=0.2631
  Batch 400: Loss=4.0068, Acc=0.2816
  Batch 450: Loss=4.3047, Acc=0.2724
  Batch 500: Loss=4.4346, Acc=0.2834
  Batch 550: Loss=4.0447, Acc=0.2791
  Batch 600: Loss=4.2120, Acc=0.2872
  Batch 650: Loss=4.5118, Acc=0.2497
  Batch 700: Loss=4.3842, Acc=0.2791

  Train Loss: 4.2063, Train Acc: 0.2767
  Val Loss: 4.2891, Val Acc: 0.2777
  ✓ Saved best model (val_loss=4.2891)

Epoch 13/30
----------------------------------------
  Batch 50: Loss=4.2517, Acc=0.2492
  Batch 100: Loss=4.1845, Acc=0.2835
  Batch 150: Loss=3.9578, Acc=0.2868
  Batch 200: Loss=4.0872, Acc=0.3110
  Batch 250: Loss=4.5646, Acc=0.2503
  Batch 300: Loss=4.1338, Acc=0.2727
  Batch 350: Loss=4.1873, Acc=0.2940
  Batch 400: Loss=4.2898, Acc=0.2537
  Batch 450: Loss=4.2125, Acc=0.2841
  Batch 500: Loss=4.1483, Acc=0.2751
  Batch 550: Loss=4.2773, Acc=0.2655
  Batch 600: Loss=4.1499, Acc=0.2646
  Batch 650: Loss=3.8342, Acc=0.2624
  Batch 700: Loss=4.3876, Acc=0.2708

  Train Loss: 4.1618, Train Acc: 0.2797
  Val Loss: 4.2582, Val Acc: 0.2809
  ✓ Saved best model (val_loss=4.2582)

Epoch 14/30
----------------------------------------
  Batch 50: Loss=4.1414, Acc=0.2929
  Batch 100: Loss=4.4835, Acc=0.2559
  Batch 150: Loss=3.9055, Acc=0.2678
  Batch 200: Loss=4.2017, Acc=0.2697
  Batch 250: Loss=4.1553, Acc=0.2657
  Batch 300: Loss=3.7902, Acc=0.3224
  Batch 350: Loss=4.0803, Acc=0.2956
  Batch 400: Loss=3.9887, Acc=0.2553
  Batch 450: Loss=3.8635, Acc=0.3111
  Batch 500: Loss=4.2572, Acc=0.2862
  Batch 550: Loss=3.9988, Acc=0.2878
  Batch 600: Loss=4.1338, Acc=0.2933
  Batch 650: Loss=4.0494, Acc=0.3027
  Batch 700: Loss=4.2766, Acc=0.2678

  Train Loss: 4.1226, Train Acc: 0.2824
  Val Loss: 4.2309, Val Acc: 0.2835
  ✓ Saved best model (val_loss=4.2309)

Epoch 15/30
----------------------------------------
  Batch 50: Loss=4.2658, Acc=0.2994
  Batch 100: Loss=4.1501, Acc=0.2705
  Batch 150: Loss=3.9260, Acc=0.2875
  Batch 200: Loss=4.1081, Acc=0.3098
  Batch 250: Loss=4.0637, Acc=0.2916
  Batch 300: Loss=3.8888, Acc=0.2835
  Batch 350: Loss=3.9851, Acc=0.2692
  Batch 400: Loss=3.8082, Acc=0.3297
  Batch 450: Loss=3.9957, Acc=0.2975
  Batch 500: Loss=3.8698, Acc=0.3068
  Batch 550: Loss=3.7685, Acc=0.2980
  Batch 600: Loss=4.1467, Acc=0.2719
  Batch 650: Loss=4.3568, Acc=0.2802
  Batch 700: Loss=4.1241, Acc=0.2441

  Train Loss: 4.0833, Train Acc: 0.2854
  Val Loss: 4.1996, Val Acc: 0.2859
  ✓ Saved best model (val_loss=4.1996)

Epoch 16/30
----------------------------------------
  Batch 50: Loss=3.8746, Acc=0.3021
  Batch 100: Loss=3.8551, Acc=0.2907
  Batch 150: Loss=3.8192, Acc=0.2953
  Batch 200: Loss=4.1030, Acc=0.3061
  Batch 250: Loss=3.7664, Acc=0.2933
  Batch 300: Loss=4.0329, Acc=0.2853
  Batch 350: Loss=4.0768, Acc=0.2874
  Batch 400: Loss=4.1664, Acc=0.2784
  Batch 450: Loss=3.9912, Acc=0.3027
  Batch 500: Loss=4.0702, Acc=0.2848
  Batch 550: Loss=4.4048, Acc=0.2349
  Batch 600: Loss=4.0309, Acc=0.2734
  Batch 650: Loss=3.8852, Acc=0.2830
  Batch 700: Loss=3.9496, Acc=0.2699

  Train Loss: 4.0466, Train Acc: 0.2880
  Val Loss: 4.1690, Val Acc: 0.2898
  ✓ Saved best model (val_loss=4.1690)

Epoch 17/30
----------------------------------------
  Batch 50: Loss=3.5576, Acc=0.3273
  Batch 100: Loss=4.1048, Acc=0.2646
  Batch 150: Loss=4.0828, Acc=0.2927
  Batch 200: Loss=3.9164, Acc=0.3032
  Batch 250: Loss=4.1144, Acc=0.2942
  Batch 300: Loss=3.8239, Acc=0.2994
  Batch 350: Loss=4.2267, Acc=0.2804
  Batch 400: Loss=4.0347, Acc=0.2944
  Batch 450: Loss=3.9693, Acc=0.2879
  Batch 500: Loss=3.5302, Acc=0.3441
  Batch 550: Loss=4.0830, Acc=0.2915
  Batch 600: Loss=3.9943, Acc=0.2864
  Batch 650: Loss=4.1297, Acc=0.2830
  Batch 700: Loss=3.9540, Acc=0.2595

  Train Loss: 4.0085, Train Acc: 0.2923
  Val Loss: 4.1372, Val Acc: 0.2950
  ✓ Saved best model (val_loss=4.1372)

Epoch 18/30
----------------------------------------
  Batch 50: Loss=3.9741, Acc=0.2694
  Batch 100: Loss=3.9739, Acc=0.3108
  Batch 150: Loss=4.0049, Acc=0.2962
  Batch 200: Loss=4.0628, Acc=0.3220
  Batch 250: Loss=4.0606, Acc=0.2899
  Batch 300: Loss=3.7528, Acc=0.3007
  Batch 350: Loss=3.8276, Acc=0.2973
  Batch 400: Loss=3.7251, Acc=0.3056
  Batch 450: Loss=3.9060, Acc=0.2848
  Batch 500: Loss=4.0950, Acc=0.2924
  Batch 550: Loss=4.0531, Acc=0.2977
  Batch 600: Loss=4.1587, Acc=0.2895
  Batch 650: Loss=3.7161, Acc=0.3080
  Batch 700: Loss=4.0340, Acc=0.3202

  Train Loss: 3.9701, Train Acc: 0.2965
  Val Loss: 4.0996, Val Acc: 0.3028
  ✓ Saved best model (val_loss=4.0996)

Epoch 19/30
----------------------------------------
  Batch 50: Loss=3.9623, Acc=0.2894
  Batch 100: Loss=4.4810, Acc=0.2510
  Batch 150: Loss=4.0500, Acc=0.3058
  Batch 200: Loss=3.9158, Acc=0.2628
  Batch 250: Loss=3.8311, Acc=0.2765
  Batch 300: Loss=3.8385, Acc=0.2994
  Batch 350: Loss=3.9132, Acc=0.2803
  Batch 400: Loss=4.2649, Acc=0.2748
  Batch 450: Loss=3.7322, Acc=0.3175
  Batch 500: Loss=3.9681, Acc=0.2890
  Batch 550: Loss=3.8958, Acc=0.2874
  Batch 600: Loss=3.6100, Acc=0.3361
  Batch 650: Loss=4.0155, Acc=0.3230
  Batch 700: Loss=3.8232, Acc=0.3026

  Train Loss: 3.9321, Train Acc: 0.3015
  Val Loss: 4.0608, Val Acc: 0.3094
  ✓ Saved best model (val_loss=4.0608)

Epoch 20/30
----------------------------------------
  Batch 50: Loss=3.9316, Acc=0.3116
  Batch 100: Loss=3.8700, Acc=0.3384
  Batch 150: Loss=3.8527, Acc=0.2976
  Batch 200: Loss=3.8213, Acc=0.3082
  Batch 250: Loss=3.7716, Acc=0.3012
  Batch 300: Loss=4.1370, Acc=0.2894
  Batch 350: Loss=4.1162, Acc=0.2900
  Batch 400: Loss=3.6450, Acc=0.3414
  Batch 450: Loss=3.4623, Acc=0.3523
  Batch 500: Loss=3.9897, Acc=0.3241
  Batch 550: Loss=3.8536, Acc=0.3198
  Batch 600: Loss=4.0301, Acc=0.3131
  Batch 650: Loss=4.1820, Acc=0.2880
  Batch 700: Loss=3.7763, Acc=0.3401

  Train Loss: 3.8900, Train Acc: 0.3077
  Val Loss: 4.0147, Val Acc: 0.3190
  ✓ Saved best model (val_loss=4.0147)

Epoch 21/30
----------------------------------------
  Batch 50: Loss=3.9963, Acc=0.2960
  Batch 100: Loss=3.7831, Acc=0.3131
  Batch 150: Loss=3.6629, Acc=0.3242
  Batch 200: Loss=3.8008, Acc=0.3316
  Batch 250: Loss=3.8710, Acc=0.3270
  Batch 300: Loss=3.7656, Acc=0.3125
  Batch 350: Loss=3.8604, Acc=0.3272
  Batch 400: Loss=3.6159, Acc=0.3294
  Batch 450: Loss=3.5700, Acc=0.3247
  Batch 500: Loss=3.9398, Acc=0.2790
  Batch 550: Loss=3.8208, Acc=0.3364
  Batch 600: Loss=3.6913, Acc=0.3362
  Batch 650: Loss=3.9607, Acc=0.3082
  Batch 700: Loss=3.7193, Acc=0.3113

  Train Loss: 3.8494, Train Acc: 0.3143
  Val Loss: 3.9676, Val Acc: 0.3277
  ✓ Saved best model (val_loss=3.9676)

Epoch 22/30
----------------------------------------
  Batch 50: Loss=3.6826, Acc=0.3532
  Batch 100: Loss=3.7300, Acc=0.3135
  Batch 150: Loss=3.9486, Acc=0.3199
  Batch 200: Loss=3.5863, Acc=0.3385
  Batch 250: Loss=3.5506, Acc=0.3580
  Batch 300: Loss=3.4171, Acc=0.3636
  Batch 350: Loss=3.7939, Acc=0.3244
  Batch 400: Loss=3.8678, Acc=0.3358
  Batch 450: Loss=4.0015, Acc=0.2840
  Batch 500: Loss=3.9294, Acc=0.3164
  Batch 550: Loss=3.6255, Acc=0.3312
  Batch 600: Loss=3.7223, Acc=0.3401
  Batch 650: Loss=3.9404, Acc=0.2927
  Batch 700: Loss=3.8292, Acc=0.3285

  Train Loss: 3.8079, Train Acc: 0.3208
  Val Loss: 3.9239, Val Acc: 0.3359
  ✓ Saved best model (val_loss=3.9239)

Epoch 23/30
----------------------------------------
  Batch 50: Loss=3.5232, Acc=0.3377
  Batch 100: Loss=4.0476, Acc=0.3173
  Batch 150: Loss=3.7565, Acc=0.3124
  Batch 200: Loss=3.6294, Acc=0.3547
  Batch 250: Loss=3.5602, Acc=0.3672
  Batch 300: Loss=3.7331, Acc=0.3348
  Batch 350: Loss=3.8419, Acc=0.2949
  Batch 400: Loss=3.6568, Acc=0.3421
  Batch 450: Loss=4.1741, Acc=0.3028
  Batch 500: Loss=3.6812, Acc=0.3568
  Batch 550: Loss=3.7559, Acc=0.3313
  Batch 600: Loss=3.9756, Acc=0.3215
  Batch 650: Loss=3.6491, Acc=0.3464
  Batch 700: Loss=3.9143, Acc=0.2914

  Train Loss: 3.7694, Train Acc: 0.3261
  Val Loss: 3.8736, Val Acc: 0.3442
  ✓ Saved best model (val_loss=3.8736)

Epoch 24/30
----------------------------------------
  Batch 50: Loss=3.8960, Acc=0.3128
  Batch 100: Loss=3.4980, Acc=0.3600
  Batch 150: Loss=3.5976, Acc=0.3529
  Batch 200: Loss=3.7845, Acc=0.3313
  Batch 250: Loss=3.8136, Acc=0.3219
  Batch 300: Loss=3.7017, Acc=0.3178
  Batch 350: Loss=3.8941, Acc=0.2992
  Batch 400: Loss=3.6767, Acc=0.3525
  Batch 450: Loss=3.4754, Acc=0.3351
  Batch 500: Loss=3.8112, Acc=0.3005
  Batch 550: Loss=3.5972, Acc=0.3488
  Batch 600: Loss=3.7471, Acc=0.3203
  Batch 650: Loss=3.7302, Acc=0.3175
  Batch 700: Loss=3.9918, Acc=0.3022

  Train Loss: 3.7294, Train Acc: 0.3312
  Val Loss: 3.8241, Val Acc: 0.3526
  ✓ Saved best model (val_loss=3.8241)

Epoch 25/30
----------------------------------------
  Batch 50: Loss=3.5620, Acc=0.3565
  Batch 100: Loss=3.6472, Acc=0.3278
  Batch 150: Loss=4.0933, Acc=0.3031
  Batch 200: Loss=3.5153, Acc=0.3510
  Batch 250: Loss=3.5233, Acc=0.3591
  Batch 300: Loss=3.5607, Acc=0.3539
  Batch 350: Loss=3.7562, Acc=0.3272
  Batch 400: Loss=3.5619, Acc=0.3464
  Batch 450: Loss=3.5995, Acc=0.3463
  Batch 500: Loss=3.4779, Acc=0.3704
  Batch 550: Loss=3.6544, Acc=0.3320
  Batch 600: Loss=3.5481, Acc=0.3738
  Batch 650: Loss=3.8355, Acc=0.3443
  Batch 700: Loss=3.5932, Acc=0.3384

  Train Loss: 3.6874, Train Acc: 0.3376
  Val Loss: 3.7695, Val Acc: 0.3614
  ✓ Saved best model (val_loss=3.7695)

Epoch 26/30
----------------------------------------
  Batch 50: Loss=3.6179, Acc=0.3566
  Batch 100: Loss=3.7024, Acc=0.3330
  Batch 150: Loss=3.9256, Acc=0.3309
  Batch 200: Loss=3.6868, Acc=0.3525
  Batch 250: Loss=3.6533, Acc=0.3365
  Batch 300: Loss=3.9427, Acc=0.3179
  Batch 350: Loss=3.9835, Acc=0.2929
  Batch 400: Loss=3.6055, Acc=0.3746
  Batch 450: Loss=3.7217, Acc=0.3376
  Batch 500: Loss=3.7105, Acc=0.3364
  Batch 550: Loss=3.6016, Acc=0.3533
  Batch 600: Loss=3.6668, Acc=0.3320
  Batch 650: Loss=3.6055, Acc=0.3475
  Batch 700: Loss=3.6763, Acc=0.3571

  Train Loss: 3.6464, Train Acc: 0.3438
  Val Loss: 3.7145, Val Acc: 0.3716
  ✓ Saved best model (val_loss=3.7145)

Epoch 27/30
----------------------------------------
  Batch 50: Loss=3.5959, Acc=0.3511
  Batch 100: Loss=3.6342, Acc=0.3219
  Batch 150: Loss=3.5732, Acc=0.3871
  Batch 200: Loss=3.7002, Acc=0.3453
  Batch 250: Loss=3.5254, Acc=0.3598
  Batch 300: Loss=3.4578, Acc=0.3582
  Batch 350: Loss=3.3150, Acc=0.3874
  Batch 400: Loss=3.4237, Acc=0.3599
  Batch 450: Loss=3.4482, Acc=0.3571
  Batch 500: Loss=3.4754, Acc=0.3615
  Batch 550: Loss=3.5760, Acc=0.3630
  Batch 600: Loss=3.5142, Acc=0.3590
  Batch 650: Loss=3.8504, Acc=0.3346
  Batch 700: Loss=3.2869, Acc=0.4002

  Train Loss: 3.6052, Train Acc: 0.3498
  Val Loss: 3.6597, Val Acc: 0.3823
  ✓ Saved best model (val_loss=3.6597)

Epoch 28/30
----------------------------------------
  Batch 50: Loss=3.4123, Acc=0.3423
  Batch 100: Loss=3.7222, Acc=0.3515
  Batch 150: Loss=3.7386, Acc=0.3246
  Batch 200: Loss=3.9014, Acc=0.3361
  Batch 250: Loss=3.7363, Acc=0.3489
  Batch 300: Loss=3.5826, Acc=0.3789
  Batch 350: Loss=3.6465, Acc=0.3594
  Batch 400: Loss=3.6616, Acc=0.3587
  Batch 450: Loss=3.2886, Acc=0.4045
  Batch 500: Loss=3.5424, Acc=0.3759
  Batch 550: Loss=3.3016, Acc=0.3695
  Batch 600: Loss=3.5740, Acc=0.3562
  Batch 650: Loss=3.6358, Acc=0.3484
  Batch 700: Loss=3.7489, Acc=0.3259

  Train Loss: 3.5640, Train Acc: 0.3573
  Val Loss: 3.5975, Val Acc: 0.3939
  ✓ Saved best model (val_loss=3.5975)

Epoch 29/30
----------------------------------------
  Batch 50: Loss=3.7744, Acc=0.3598
  Batch 100: Loss=3.3237, Acc=0.3679
  Batch 150: Loss=3.2813, Acc=0.3903
  Batch 200: Loss=3.6591, Acc=0.3456
  Batch 250: Loss=3.5848, Acc=0.3268
  Batch 300: Loss=3.5964, Acc=0.3494
  Batch 350: Loss=3.3437, Acc=0.3637
  Batch 400: Loss=3.2070, Acc=0.4260
  Batch 450: Loss=3.6570, Acc=0.3376
  Batch 500: Loss=3.3086, Acc=0.3758
  Batch 550: Loss=3.4573, Acc=0.3749
  Batch 600: Loss=3.7080, Acc=0.3402
  Batch 650: Loss=3.3970, Acc=0.3868
  Batch 700: Loss=3.5024, Acc=0.3523

  Train Loss: 3.5162, Train Acc: 0.3647
  Val Loss: 3.5260, Val Acc: 0.4078
  ✓ Saved best model (val_loss=3.5260)

Epoch 30/30
----------------------------------------
  Batch 50: Loss=3.4559, Acc=0.3522
  Batch 100: Loss=3.4561, Acc=0.3869
  Batch 150: Loss=3.7435, Acc=0.3354
  Batch 200: Loss=3.5888, Acc=0.3756
  Batch 250: Loss=3.4267, Acc=0.3720
  Batch 300: Loss=3.4539, Acc=0.3586
  Batch 350: Loss=3.3493, Acc=0.4026
  Batch 400: Loss=3.7588, Acc=0.3400
  Batch 450: Loss=3.7736, Acc=0.3400
  Batch 500: Loss=3.4071, Acc=0.3825
  Batch 550: Loss=3.3688, Acc=0.3855
  Batch 600: Loss=3.6533, Acc=0.3649
  Batch 650: Loss=3.3371, Acc=0.3882
  Batch 700: Loss=3.4777, Acc=0.3994

  Train Loss: 3.4593, Train Acc: 0.3763
  Val Loss: 3.4151, Val Acc: 0.4330
  ✓ Saved best model (val_loss=3.4151)

✓ Training complete. Best validation loss: 3.4151

Evaluating Control Transformer...
  Test Loss: 3.4273
  Test Accuracy: 0.4320

############################################################
# RECURSIVE TRANSFORMER (TRM-Inspired)
############################################################

Recursive Model Parameters: 899,855

============================================================
TRAINING RECURSIVE TRANSFORMER (TRM-Inspired)
============================================================
  Deep recursion cycles (T): 2
  Inner iterations (n): 3
  Deep supervision steps: 2
  ACT loss weight: 0.1
  Halt exploration prob: 0.1
  Max halt steps: 16
============================================================


Epoch 1/30
----------------------------------------
  Batch 50: Loss=9.4168, Acc=0.0145
  Batch 100: Loss=9.2016, Acc=0.0293
  Batch 150: Loss=8.9233, Acc=0.0354
  Batch 200: Loss=8.6625, Acc=0.0651
  Batch 250: Loss=8.4036, Acc=0.0638
  Batch 300: Loss=8.1873, Acc=0.0381
  Batch 350: Loss=7.9708, Acc=0.0237
  Batch 400: Loss=7.6547, Acc=0.0462
  Batch 450: Loss=7.5128, Acc=0.0326
  Batch 500: Loss=7.3887, Acc=0.0364
  Batch 550: Loss=7.1722, Acc=0.0402
  Batch 600: Loss=7.0066, Acc=0.0296
  Batch 650: Loss=6.8649, Acc=0.0309
  Batch 700: Loss=6.6726, Acc=0.0411

  Train Loss: 7.9222, Train Acc: 0.0371
  Val Loss: 6.5769, Val Acc: 0.0359
  ✓ Saved best model (val_loss=6.5769)

Epoch 2/30
----------------------------------------
  Batch 50: Loss=6.4170, Acc=0.0367
  Batch 100: Loss=6.0976, Acc=0.0442
  Batch 150: Loss=6.3286, Acc=0.0317
  Batch 200: Loss=6.3235, Acc=0.0394
  Batch 250: Loss=6.1546, Acc=0.0392
  Batch 300: Loss=6.2429, Acc=0.0336
  Batch 350: Loss=6.2604, Acc=0.0305
  Batch 400: Loss=5.9700, Acc=0.0359
  Batch 450: Loss=6.1280, Acc=0.0354
  Batch 500: Loss=6.0741, Acc=0.0335
  Batch 550: Loss=5.7537, Acc=0.0385
  Batch 600: Loss=5.8782, Acc=0.0395
  Batch 650: Loss=6.0743, Acc=0.0473
  Batch 700: Loss=5.9354, Acc=0.0513

  Train Loss: 6.1347, Train Acc: 0.0374
  Val Loss: 6.1013, Val Acc: 0.0363
  ✓ Saved best model (val_loss=6.1013)

Epoch 3/30
----------------------------------------
  Batch 50: Loss=6.0434, Acc=0.0286
  Batch 100: Loss=5.7622, Acc=0.0271
  Batch 150: Loss=6.1418, Acc=0.0000
  Batch 200: Loss=6.0398, Acc=0.0000
  Batch 250: Loss=5.7508, Acc=0.0000
  Batch 300: Loss=5.5720, Acc=0.0000
  Batch 350: Loss=5.9213, Acc=0.0000
  Batch 400: Loss=5.4094, Acc=0.0000
  Batch 450: Loss=5.7312, Acc=0.0000
  Batch 500: Loss=5.9512, Acc=0.0011
  Batch 550: Loss=5.9511, Acc=0.0040
  Batch 600: Loss=5.7969, Acc=0.0127
  Batch 650: Loss=5.7576, Acc=0.0510
  Batch 700: Loss=5.4379, Acc=0.0707

  Train Loss: 5.8431, Train Acc: 0.0187
  Val Loss: 5.8630, Val Acc: 0.1033
  ✓ Saved best model (val_loss=5.8630)

Epoch 4/30
----------------------------------------
  Batch 50: Loss=5.6617, Acc=0.1491
  Batch 100: Loss=5.6540, Acc=0.1713
  Batch 150: Loss=5.6307, Acc=0.1589
  Batch 200: Loss=5.6686, Acc=0.1680
  Batch 250: Loss=5.8635, Acc=0.1594
  Batch 300: Loss=5.3957, Acc=0.1915
  Batch 350: Loss=5.5235, Acc=0.1901
  Batch 400: Loss=5.4410, Acc=0.1739
  Batch 450: Loss=5.6176, Acc=0.1716
  Batch 500: Loss=5.5048, Acc=0.1944
  Batch 550: Loss=5.0252, Acc=0.2158
  Batch 600: Loss=5.1238, Acc=0.2026
  Batch 650: Loss=5.1599, Acc=0.1779
  Batch 700: Loss=5.2858, Acc=0.2221

  Train Loss: 5.4100, Train Acc: 0.1812
  Val Loss: 5.2648, Val Acc: 0.2060
  ✓ Saved best model (val_loss=5.2648)

Epoch 5/30
----------------------------------------
  Batch 50: Loss=5.2255, Acc=0.2010
  Batch 100: Loss=4.9533, Acc=0.2181
  Batch 150: Loss=4.9324, Acc=0.2221
  Batch 200: Loss=4.3818, Acc=0.2393
  Batch 250: Loss=4.9212, Acc=0.2248
  Batch 300: Loss=4.8891, Acc=0.1887
  Batch 350: Loss=4.6547, Acc=0.2362
  Batch 400: Loss=4.9176, Acc=0.2309
  Batch 450: Loss=5.3549, Acc=0.2080
  Batch 500: Loss=4.8612, Acc=0.2186
  Batch 550: Loss=4.7787, Acc=0.2331
  Batch 600: Loss=4.6329, Acc=0.2248
  Batch 650: Loss=4.1998, Acc=0.2454
  Batch 700: Loss=5.1133, Acc=0.2270

  Train Loss: 4.9464, Train Acc: 0.2196
  Val Loss: 4.9314, Val Acc: 0.2332
  ✓ Saved best model (val_loss=4.9314)

Epoch 6/30
----------------------------------------
  Batch 50: Loss=5.3558, Acc=0.2093
  Batch 100: Loss=4.9394, Acc=0.2069
  Batch 150: Loss=4.8691, Acc=0.2266
  Batch 200: Loss=5.0712, Acc=0.2196
  Batch 250: Loss=4.8891, Acc=0.2468
  Batch 300: Loss=4.5342, Acc=0.2397
  Batch 350: Loss=4.5937, Acc=0.2495
  Batch 400: Loss=4.7864, Acc=0.2274
  Batch 450: Loss=4.7443, Acc=0.2364
  Batch 500: Loss=4.5059, Acc=0.2547
  Batch 550: Loss=4.7904, Acc=0.2490
  Batch 600: Loss=4.6574, Acc=0.2192
  Batch 650: Loss=4.4362, Acc=0.2833
  Batch 700: Loss=5.2053, Acc=0.2149

  Train Loss: 4.6756, Train Acc: 0.2444
  Val Loss: 4.7438, Val Acc: 0.2488
  ✓ Saved best model (val_loss=4.7438)

Epoch 7/30
----------------------------------------
  Batch 50: Loss=4.1738, Acc=0.2691
  Batch 100: Loss=4.5946, Acc=0.2526
  Batch 150: Loss=4.6536, Acc=0.2260
  Batch 200: Loss=4.3936, Acc=0.2264
  Batch 250: Loss=4.6893, Acc=0.2721
  Batch 300: Loss=4.3466, Acc=0.2637
  Batch 350: Loss=4.6666, Acc=0.2307
  Batch 400: Loss=4.5927, Acc=0.2363
  Batch 450: Loss=4.7448, Acc=0.2253
  Batch 500: Loss=4.7280, Acc=0.2523
  Batch 550: Loss=3.9788, Acc=0.2757
  Batch 600: Loss=4.7979, Acc=0.2438
  Batch 650: Loss=4.0877, Acc=0.2846
  Batch 700: Loss=4.3912, Acc=0.2551

  Train Loss: 4.5138, Train Acc: 0.2554
  Val Loss: 4.6294, Val Acc: 0.2545
  ✓ Saved best model (val_loss=4.6294)

Epoch 8/30
----------------------------------------
  Batch 50: Loss=4.2884, Acc=0.2862
  Batch 100: Loss=4.3686, Acc=0.2660
  Batch 150: Loss=4.5518, Acc=0.2565
  Batch 200: Loss=3.8314, Acc=0.2852
  Batch 250: Loss=4.5434, Acc=0.2544
  Batch 300: Loss=4.3743, Acc=0.2728
  Batch 350: Loss=4.4683, Acc=0.2373
  Batch 400: Loss=4.0456, Acc=0.2821
  Batch 450: Loss=4.6170, Acc=0.2495
  Batch 500: Loss=4.5062, Acc=0.2607
  Batch 550: Loss=4.7552, Acc=0.2318
  Batch 600: Loss=5.0173, Acc=0.2126
  Batch 650: Loss=4.3746, Acc=0.2626
  Batch 700: Loss=4.1441, Acc=0.2727

  Train Loss: 4.4079, Train Acc: 0.2608
  Val Loss: 4.5515, Val Acc: 0.2574
  ✓ Saved best model (val_loss=4.5515)

Epoch 9/30
----------------------------------------
  Batch 50: Loss=4.7997, Acc=0.2343
  Batch 100: Loss=4.8668, Acc=0.2287
  Batch 150: Loss=4.1528, Acc=0.2553
  Batch 200: Loss=4.5129, Acc=0.2376
  Batch 250: Loss=4.6165, Acc=0.2539
  Batch 300: Loss=4.5570, Acc=0.2280
  Batch 350: Loss=4.8891, Acc=0.2281
  Batch 400: Loss=4.2829, Acc=0.2663
  Batch 450: Loss=4.2209, Acc=0.2957
  Batch 500: Loss=4.4931, Acc=0.2458
  Batch 550: Loss=3.9558, Acc=0.2753
  Batch 600: Loss=4.2564, Acc=0.2856
  Batch 650: Loss=4.6108, Acc=0.2452
  Batch 700: Loss=4.1164, Acc=0.2678

  Train Loss: 4.3284, Train Acc: 0.2659
  Val Loss: 4.4909, Val Acc: 0.2621
  ✓ Saved best model (val_loss=4.4909)

Epoch 10/30
----------------------------------------
  Batch 50: Loss=4.2238, Acc=0.2607
  Batch 100: Loss=4.5986, Acc=0.2727
  Batch 150: Loss=4.6871, Acc=0.2447
  Batch 200: Loss=4.3559, Acc=0.2533
  Batch 250: Loss=4.0471, Acc=0.2857
  Batch 300: Loss=4.5241, Acc=0.2704
  Batch 350: Loss=4.1587, Acc=0.2955
  Batch 400: Loss=3.5997, Acc=0.2914
  Batch 450: Loss=4.0376, Acc=0.2974
  Batch 500: Loss=4.2224, Acc=0.2738
  Batch 550: Loss=4.4853, Acc=0.2889
  Batch 600: Loss=4.4336, Acc=0.2513
  Batch 650: Loss=4.6421, Acc=0.2513
  Batch 700: Loss=4.6386, Acc=0.2394

  Train Loss: 4.2641, Train Acc: 0.2718
  Val Loss: 4.4426, Val Acc: 0.2666
  ✓ Saved best model (val_loss=4.4426)

Epoch 11/30
----------------------------------------
  Batch 50: Loss=4.2196, Acc=0.2750
  Batch 100: Loss=4.0382, Acc=0.2966
  Batch 150: Loss=3.8022, Acc=0.3200
  Batch 200: Loss=4.2383, Acc=0.2634
  Batch 250: Loss=4.3574, Acc=0.2781
  Batch 300: Loss=4.2789, Acc=0.2873
  Batch 350: Loss=4.0819, Acc=0.2826
  Batch 400: Loss=4.3076, Acc=0.2757
  Batch 450: Loss=3.7419, Acc=0.2834
  Batch 500: Loss=4.1922, Acc=0.2818
  Batch 550: Loss=3.8955, Acc=0.3063
  Batch 600: Loss=4.8559, Acc=0.2475
  Batch 650: Loss=4.1090, Acc=0.2764
  Batch 700: Loss=4.4495, Acc=0.2607

  Train Loss: 4.2122, Train Acc: 0.2769
  Val Loss: 4.4033, Val Acc: 0.2702
  ✓ Saved best model (val_loss=4.4033)

Epoch 12/30
----------------------------------------
  Batch 50: Loss=4.0811, Acc=0.2756
