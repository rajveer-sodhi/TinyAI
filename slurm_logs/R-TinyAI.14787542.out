## SLURM PROLOG ###############################################################
##    Job ID : 14787542
##  Job Name : TinyAI
##  Nodelist : gpu1401
##      CPUs : 1
##  Mem/Node : 64000 MB
## Directory : /oscar/home/bpeckham/TinyAI
##   Job Started : Wed Dec 10 04:10:50 PM EST 2025
###############################################################################

==========================================
Job started at: Wed Dec 10 04:10:50 PM EST 2025
Job ID: 14787542
Node: gpu1401
==========================================

Python location: /users/bpeckham/.conda/envs/tinyai/bin/python
Python version: Python 3.10.19
Conda environment: tinyai
Installing/verifying required packages...
TensorFlow 2.15.0 and NumPy 1.26.4 imported successfully
GPU Information:
Wed Dec 10 16:11:19 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA TITAN RTX               On  | 00000000:61:00.0 Off |                  N/A |
| 41%   28C    P8              17W / 280W |      7MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     30276      G   /usr/libexec/Xorg                             5MiB |
+---------------------------------------------------------------------------------------+

TensorFlow GPU Detection:
TF version: 2.15.0
Built with CUDA: True
GPUs detected: 0
GPU devices: []

==========================================
Starting main Python script at Wed Dec 10 04:11:24 PM EST 2025
==========================================

Configuration:
  d_model: 64
  num_layers: 2
  num_heads: 3
  ff_dim: 64
  deep_rec_cycles: 1
  num_l_steps: 2
  epochs: 15
  batch_size: 8
  learning_rate: 1e-4
  max_seq_length: 128

To monitor training with WandB:
  1. First, log in to WandB (one time only):
     python wandb_login.py
  2. Then view metrics at: https://wandb.ai
  (No port forwarding needed - metrics upload automatically!)


============================================================
TinyAI TRAINING SCRIPT
============================================================

Configuration:
  Data path: /oscar/home/bpeckham/TinyAI/preprocessing/data/final_train_data.txt
  Vocab path: /oscar/home/bpeckham/TinyAI/preprocessing/data/vocab.json
  Max sequence length: 128
  Model dimension: 64
  Layers: 2
  Heads: 3
  FF dim: 64
  Dropout: 0.1
  Deep recursion cycles: 1
  Inner iterations: 2
  Deep supervision steps: 2
  Batch size: 8
  Epochs: 15
  Learning rate: 0.0001
============================================================

Loading tokenizer...
Loaded vocabulary with 7173 tokens
  PAD=0, UNK=1, BOS=18, EOS=2
Loading data...
Loaded 14180 samples
Train: 11344, Val: 1418, Test: 1418

############################################################
# CONTROL TRANSFORMER
############################################################

Control Model Parameters: 983,679

============================================================
TRAINING CONTROL TRANSFORMER (Single-Pass Baseline)
============================================================


Epoch 1/15
----------------------------------------
  Batch 50: Loss=8.6419, TokenAcc=0.0245, ExactMatch=0.0000
  Batch 100: Loss=8.1509, TokenAcc=0.0416, ExactMatch=0.0000
  Batch 150: Loss=7.6254, TokenAcc=0.0219, ExactMatch=0.0000
  Batch 200: Loss=7.2500, TokenAcc=0.0325, ExactMatch=0.0000
  Batch 250: Loss=6.9288, TokenAcc=0.0434, ExactMatch=0.0000
  Batch 300: Loss=6.6631, TokenAcc=0.0470, ExactMatch=0.0000
  Batch 350: Loss=6.3985, TokenAcc=0.0412, ExactMatch=0.0000
  Batch 400: Loss=6.2962, TokenAcc=0.0431, ExactMatch=0.0000
  Batch 450: Loss=6.0663, TokenAcc=0.0441, ExactMatch=0.0000
  Batch 500: Loss=6.1289, TokenAcc=0.0367, ExactMatch=0.0000
  Batch 550: Loss=6.1274, TokenAcc=0.0395, ExactMatch=0.0000
  Batch 600: Loss=6.0099, TokenAcc=0.0649, ExactMatch=0.0000
  Batch 650: Loss=5.9604, TokenAcc=0.0637, ExactMatch=0.0000
  Batch 700: Loss=5.9371, TokenAcc=0.0855, ExactMatch=0.0000
  Batch 750: Loss=5.8968, TokenAcc=0.1132, ExactMatch=0.0000
  Batch 800: Loss=5.7773, TokenAcc=0.1199, ExactMatch=0.0000
  Batch 850: Loss=5.6251, TokenAcc=0.1258, ExactMatch=0.0000
  Batch 900: Loss=5.9837, TokenAcc=0.1322, ExactMatch=0.0000
  Batch 950: Loss=5.6703, TokenAcc=0.1365, ExactMatch=0.0000
  Batch 1000: Loss=5.5339, TokenAcc=0.1790, ExactMatch=0.0000
  Batch 1050: Loss=5.5509, TokenAcc=0.1889, ExactMatch=0.0000
  Batch 1100: Loss=5.5784, TokenAcc=0.1719, ExactMatch=0.0000
  Batch 1150: Loss=5.4786, TokenAcc=0.1865, ExactMatch=0.0000
  Batch 1200: Loss=5.3314, TokenAcc=0.2002, ExactMatch=0.0000
  Batch 1250: Loss=5.4514, TokenAcc=0.1858, ExactMatch=0.0000
  Batch 1300: Loss=4.9946, TokenAcc=0.2170, ExactMatch=0.0000
  Batch 1350: Loss=5.2212, TokenAcc=0.2253, ExactMatch=0.0000
