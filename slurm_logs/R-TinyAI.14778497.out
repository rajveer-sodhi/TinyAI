## SLURM PROLOG ###############################################################
##    Job ID : 14778497
##  Job Name : TinyAI
##  Nodelist : gpu1402
##      CPUs : 1
##  Mem/Node : 64000 MB
## Directory : /oscar/home/bpeckham/TinyAI
##   Job Started : Wed Dec 10 12:10:56 PM EST 2025
###############################################################################

==========================================
Job started at: Wed Dec 10 12:10:56 PM EST 2025
Job ID: 14778497
Node: gpu1402
==========================================

Python location: /users/bpeckham/.conda/envs/tinyai/bin/python
Python version: Python 3.10.19
Conda environment: tinyai
Installing/verifying required packages...
TensorFlow 2.15.0 and NumPy 1.26.4 imported successfully
GPU Information:
Wed Dec 10 12:11:07 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA TITAN RTX               On  | 00000000:41:00.0 Off |                  N/A |
| 41%   36C    P8              34W / 280W |     20MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     29134      G   /usr/libexec/Xorg                            17MiB |
+---------------------------------------------------------------------------------------+

TensorFlow GPU Detection:
TF version: 2.15.0
Built with CUDA: True
GPUs detected: 0
GPU devices: []

==========================================
Starting main Python script at Wed Dec 10 12:11:11 PM EST 2025
==========================================

Configuration:
  d_model: 64
  num_layers: 2
  num_heads: 4
  ff_dim: 128
  deep_rec_cycles: 2
  num_l_steps: 3
  epochs: 15
  batch_size: 8
  learning_rate: 1e-4
  max_seq_length: 128

To monitor training with WandB:
  1. First, log in to WandB (one time only):
     python wandb_login.py
  2. Then view metrics at: https://wandb.ai
  (No port forwarding needed - metrics upload automatically!)


============================================================
TinyAI TRAINING SCRIPT
============================================================

Configuration:
  Data path: /oscar/home/bpeckham/TinyAI/preprocessing/data/final_train_data.txt
  Vocab path: /oscar/home/bpeckham/TinyAI/preprocessing/data/vocab.json
  Max sequence length: 128
  Model dimension: 64
  Layers: 2
  Heads: 4
  FF dim: 128
  Dropout: 0.1
  Deep recursion cycles: 2
  Inner iterations: 3
  Deep supervision steps: 2
  Batch size: 8
  Epochs: 15
  Learning rate: 0.0001
============================================================

Loading tokenizer...
Loaded vocabulary with 7173 tokens
  PAD=0, UNK=1, BOS=18, EOS=2
Loading data...
Loaded 14180 samples
Train: 11344, Val: 1418, Test: 1418

############################################################
# CONTROL TRANSFORMER
############################################################

Control Model Parameters: 1,000,709

============================================================
TRAINING CONTROL TRANSFORMER (Single-Pass Baseline)
============================================================


Epoch 1/15
----------------------------------------
  Batch 50: Loss=8.6492, Acc=0.0197
  Batch 100: Loss=8.0949, Acc=0.0489
  Batch 150: Loss=7.6231, Acc=0.0389
  Batch 200: Loss=7.2532, Acc=0.0370
  Batch 250: Loss=7.0457, Acc=0.0429
  Batch 300: Loss=6.6664, Acc=0.0383
  Batch 350: Loss=6.3084, Acc=0.0359
  Batch 400: Loss=6.2391, Acc=0.0392
  Batch 450: Loss=6.1669, Acc=0.0324
  Batch 500: Loss=6.2129, Acc=0.0320
  Batch 550: Loss=6.1968, Acc=0.0429
  Batch 600: Loss=5.9769, Acc=0.0488
  Batch 650: Loss=6.0164, Acc=0.0750
  Batch 700: Loss=6.0396, Acc=0.0840
  Batch 750: Loss=5.7940, Acc=0.1056
  Batch 800: Loss=5.9808, Acc=0.1100
  Batch 850: Loss=5.8535, Acc=0.1147
  Batch 900: Loss=5.9102, Acc=0.1270
  Batch 950: Loss=5.5968, Acc=0.1513
  Batch 1000: Loss=6.0498, Acc=0.1368
  Batch 1050: Loss=5.5035, Acc=0.1907
  Batch 1100: Loss=5.5743, Acc=0.1940
  Batch 1150: Loss=5.4018, Acc=0.2519
  Batch 1200: Loss=5.3635, Acc=0.2186
  Batch 1250: Loss=5.3863, Acc=0.1956
  Batch 1300: Loss=5.1260, Acc=0.2134
  Batch 1350: Loss=5.3127, Acc=0.2181
  Batch 1400: Loss=4.9198, Acc=0.2367

  Train Loss: 6.2033, Train Acc: 0.1056
  Val Loss: 5.1131, Val Acc: 0.2189
  âœ“ Saved best model (val_loss=5.1131)

Epoch 2/15
----------------------------------------
  Batch 50: Loss=5.0789, Acc=0.2465
  Batch 100: Loss=5.1762, Acc=0.2092
  Batch 150: Loss=5.0069, Acc=0.2347
  Batch 200: Loss=5.0762, Acc=0.1838
  Batch 250: Loss=5.0591, Acc=0.2086
  Batch 300: Loss=4.9406, Acc=0.2493
  Batch 350: Loss=4.8555, Acc=0.2377
  Batch 400: Loss=4.7562, Acc=0.2472
  Batch 450: Loss=4.8123, Acc=0.2435
  Batch 500: Loss=4.7750, Acc=0.2377
  Batch 550: Loss=4.6039, Acc=0.3061
  Batch 600: Loss=4.5319, Acc=0.2436
  Batch 650: Loss=4.9594, Acc=0.2172
  Batch 700: Loss=4.5347, Acc=0.2608
  Batch 750: Loss=4.3780, Acc=0.2850
  Batch 800: Loss=4.7269, Acc=0.2413
  Batch 850: Loss=4.5408, Acc=0.2648
  Batch 900: Loss=4.3236, Acc=0.2963
  Batch 950: Loss=4.2731, Acc=0.2692
  Batch 1000: Loss=4.5534, Acc=0.2644
  Batch 1050: Loss=4.1585, Acc=0.2879
  Batch 1100: Loss=4.4584, Acc=0.2799
  Batch 1150: Loss=4.4393, Acc=0.2827
  Batch 1200: Loss=4.3323, Acc=0.2567
  Batch 1250: Loss=4.0266, Acc=0.3234
  Batch 1300: Loss=4.3160, Acc=0.2745
  Batch 1350: Loss=3.9849, Acc=0.3007
  Batch 1400: Loss=4.4346, Acc=0.2476

  Train Loss: 4.6487, Train Acc: 0.2550
  Val Loss: 4.2379, Val Acc: 0.2802
  âœ“ Saved best model (val_loss=4.2379)

Epoch 3/15
----------------------------------------
  Batch 50: Loss=4.3168, Acc=0.2400
  Batch 100: Loss=4.1472, Acc=0.2690
  Batch 150: Loss=4.3824, Acc=0.2577
  Batch 200: Loss=3.9538, Acc=0.3279
  Batch 250: Loss=4.2181, Acc=0.2621
  Batch 300: Loss=4.2661, Acc=0.2593
  Batch 350: Loss=4.3140, Acc=0.2841
  Batch 400: Loss=4.3153, Acc=0.2672
  Batch 450: Loss=4.2801, Acc=0.2559
  Batch 500: Loss=4.0681, Acc=0.2652
  Batch 550: Loss=4.4385, Acc=0.2842
  Batch 600: Loss=4.0858, Acc=0.2815
  Batch 650: Loss=3.8512, Acc=0.3109
  Batch 700: Loss=4.0851, Acc=0.2752
  Batch 750: Loss=4.1735, Acc=0.2853
  Batch 800: Loss=4.0516, Acc=0.2760
  Batch 850: Loss=3.7620, Acc=0.3416
  Batch 900: Loss=4.2338, Acc=0.2843
  Batch 950: Loss=4.2271, Acc=0.2928
  Batch 1000: Loss=4.1868, Acc=0.2770
  Batch 1050: Loss=3.9740, Acc=0.3107
  Batch 1100: Loss=4.0348, Acc=0.2798
  Batch 1150: Loss=3.9134, Acc=0.3066
  Batch 1200: Loss=3.8596, Acc=0.3235
  Batch 1250: Loss=4.2963, Acc=0.2687
  Batch 1300: Loss=4.1773, Acc=0.2558
  Batch 1350: Loss=4.2547, Acc=0.2503
  Batch 1400: Loss=3.7357, Acc=0.3244

  Train Loss: 4.1062, Train Acc: 0.2883
  Val Loss: 3.9091, Val Acc: 0.3084
  âœ“ Saved best model (val_loss=3.9091)

Epoch 4/15
----------------------------------------
  Batch 50: Loss=3.9410, Acc=0.2940
  Batch 100: Loss=3.7168, Acc=0.3079
  Batch 150: Loss=3.9998, Acc=0.3080
  Batch 200: Loss=4.1922, Acc=0.2606
  Batch 250: Loss=3.9439, Acc=0.3199
  Batch 300: Loss=3.9072, Acc=0.2950
  Batch 350: Loss=3.7367, Acc=0.3357
  Batch 400: Loss=3.5759, Acc=0.3386
  Batch 450: Loss=3.7631, Acc=0.3161
  Batch 500: Loss=3.9672, Acc=0.3142
  Batch 550: Loss=3.9223, Acc=0.3059
  Batch 600: Loss=3.5709, Acc=0.3187
  Batch 650: Loss=3.8771, Acc=0.3071
  Batch 700: Loss=3.8581, Acc=0.3007
  Batch 750: Loss=3.8125, Acc=0.2966
  Batch 800: Loss=3.6867, Acc=0.3134
  Batch 850: Loss=3.7914, Acc=0.2941
  Batch 900: Loss=3.6225, Acc=0.2936
  Batch 950: Loss=4.1528, Acc=0.2712
  Batch 1000: Loss=3.7007, Acc=0.3187
  Batch 1050: Loss=3.6313, Acc=0.3411
  Batch 1100: Loss=3.5539, Acc=0.3223
  Batch 1150: Loss=3.7041, Acc=0.3056
  Batch 1200: Loss=4.1525, Acc=0.2704
  Batch 1250: Loss=3.6232, Acc=0.3246
  Batch 1300: Loss=3.4148, Acc=0.3194
  Batch 1350: Loss=3.6671, Acc=0.2879
  Batch 1400: Loss=3.6592, Acc=0.3451

  Train Loss: 3.8435, Train Acc: 0.3098
  Val Loss: 3.7008, Val Acc: 0.3295
  âœ“ Saved best model (val_loss=3.7008)

Epoch 5/15
----------------------------------------
  Batch 50: Loss=3.6258, Acc=0.3236
  Batch 100: Loss=3.5732, Acc=0.3432
  Batch 150: Loss=3.7899, Acc=0.3221
  Batch 200: Loss=4.0004, Acc=0.3159
  Batch 250: Loss=3.4019, Acc=0.3619
  Batch 300: Loss=3.5103, Acc=0.3514
  Batch 350: Loss=3.8235, Acc=0.2883
  Batch 400: Loss=3.6769, Acc=0.3493
  Batch 450: Loss=3.7458, Acc=0.3181
  Batch 500: Loss=3.6205, Acc=0.3509
  Batch 550: Loss=3.5812, Acc=0.3475
  Batch 600: Loss=3.8106, Acc=0.3382
  Batch 650: Loss=3.4399, Acc=0.3688
  Batch 700: Loss=3.7747, Acc=0.3057
  Batch 750: Loss=3.7267, Acc=0.3119
  Batch 800: Loss=4.2011, Acc=0.3036
  Batch 850: Loss=3.5683, Acc=0.3181
  Batch 900: Loss=3.8123, Acc=0.3109
  Batch 950: Loss=3.7304, Acc=0.3438
  Batch 1000: Loss=3.5471, Acc=0.3316
  Batch 1050: Loss=3.4347, Acc=0.3596
  Batch 1100: Loss=3.4324, Acc=0.3591
  Batch 1150: Loss=3.5075, Acc=0.3132
  Batch 1200: Loss=3.6072, Acc=0.3317
  Batch 1250: Loss=3.5077, Acc=0.3313
  Batch 1300: Loss=3.3727, Acc=0.3419
  Batch 1350: Loss=3.3568, Acc=0.3797
  Batch 1400: Loss=3.7620, Acc=0.3294

  Train Loss: 3.6461, Train Acc: 0.3287
  Val Loss: 3.5153, Val Acc: 0.3497
  âœ“ Saved best model (val_loss=3.5153)

Epoch 6/15
----------------------------------------
  Batch 50: Loss=3.7639, Acc=0.3496
  Batch 100: Loss=3.7465, Acc=0.3127
  Batch 150: Loss=3.8426, Acc=0.3020
  Batch 200: Loss=3.4321, Acc=0.3380
  Batch 250: Loss=3.2748, Acc=0.3861
  Batch 300: Loss=3.4468, Acc=0.3432
  Batch 350: Loss=3.6846, Acc=0.3275
  Batch 400: Loss=3.8242, Acc=0.3078
  Batch 450: Loss=3.4403, Acc=0.3555
  Batch 500: Loss=3.7242, Acc=0.3429
  Batch 550: Loss=3.8428, Acc=0.2852
  Batch 600: Loss=3.7197, Acc=0.2963
  Batch 650: Loss=3.4309, Acc=0.3479
  Batch 700: Loss=3.1336, Acc=0.3853
  Batch 750: Loss=3.4816, Acc=0.3234
  Batch 800: Loss=3.4227, Acc=0.3922
  Batch 850: Loss=3.3395, Acc=0.3732
  Batch 900: Loss=3.5045, Acc=0.3666
  Batch 950: Loss=3.3759, Acc=0.3555
  Batch 1000: Loss=3.4286, Acc=0.3457
  Batch 1050: Loss=3.5945, Acc=0.3492
  Batch 1100: Loss=3.3254, Acc=0.3738
  Batch 1150: Loss=2.9912, Acc=0.3741
  Batch 1200: Loss=3.0848, Acc=0.3897
  Batch 1250: Loss=3.4441, Acc=0.3377
  Batch 1300: Loss=3.3424, Acc=0.3612
  Batch 1350: Loss=3.5349, Acc=0.3566
  Batch 1400: Loss=3.1455, Acc=0.4169

  Train Loss: 3.4706, Train Acc: 0.3481
  Val Loss: 3.3413, Val Acc: 0.3747
  âœ“ Saved best model (val_loss=3.3413)

Epoch 7/15
----------------------------------------
  Batch 50: Loss=3.3214, Acc=0.3765
  Batch 100: Loss=3.2775, Acc=0.3711
  Batch 150: Loss=3.6615, Acc=0.3256
  Batch 200: Loss=3.2460, Acc=0.3824
  Batch 250: Loss=3.6985, Acc=0.3218
  Batch 300: Loss=3.5997, Acc=0.3572
  Batch 350: Loss=3.6957, Acc=0.3237
  Batch 400: Loss=3.5644, Acc=0.3226
  Batch 450: Loss=2.9840, Acc=0.4281
  Batch 500: Loss=3.3460, Acc=0.3659
  Batch 550: Loss=3.1855, Acc=0.3812
  Batch 600: Loss=3.1674, Acc=0.3918
  Batch 650: Loss=3.0122, Acc=0.4025
  Batch 700: Loss=2.9418, Acc=0.4049
  Batch 750: Loss=3.0984, Acc=0.4014
  Batch 800: Loss=3.1556, Acc=0.3400
  Batch 850: Loss=3.2883, Acc=0.3572
  Batch 900: Loss=3.1136, Acc=0.4030
  Batch 950: Loss=3.3073, Acc=0.3840
  Batch 1000: Loss=3.5311, Acc=0.3765
  Batch 1050: Loss=3.3364, Acc=0.3724
  Batch 1100: Loss=3.2583, Acc=0.3936
  Batch 1150: Loss=3.0163, Acc=0.3925
  Batch 1200: Loss=3.2818, Acc=0.3701
  Batch 1250: Loss=3.6038, Acc=0.3320
  Batch 1300: Loss=2.9412, Acc=0.4010
  Batch 1350: Loss=3.0150, Acc=0.4024
  Batch 1400: Loss=3.1961, Acc=0.3848

  Train Loss: 3.3010, Train Acc: 0.3707
  Val Loss: 3.1629, Val Acc: 0.4029
  âœ“ Saved best model (val_loss=3.1629)

Epoch 8/15
----------------------------------------
  Batch 50: Loss=3.2615, Acc=0.3770
  Batch 100: Loss=2.9479, Acc=0.4232
  Batch 150: Loss=3.3513, Acc=0.3577
  Batch 200: Loss=3.1144, Acc=0.3892
  Batch 250: Loss=2.8939, Acc=0.4170
  Batch 300: Loss=3.2260, Acc=0.4097
  Batch 350: Loss=3.1669, Acc=0.3730
  Batch 400: Loss=2.9347, Acc=0.4178
  Batch 450: Loss=3.1370, Acc=0.3858
  Batch 500: Loss=3.2163, Acc=0.3769
  Batch 550: Loss=3.3474, Acc=0.3845
  Batch 600: Loss=2.7915, Acc=0.4104
  Batch 650: Loss=3.4927, Acc=0.3407
  Batch 700: Loss=3.5207, Acc=0.3577
  Batch 750: Loss=3.3990, Acc=0.3742
  Batch 800: Loss=3.3902, Acc=0.3634
  Batch 850: Loss=3.1023, Acc=0.4030
  Batch 900: Loss=3.2149, Acc=0.3961
  Batch 950: Loss=2.9689, Acc=0.4324
  Batch 1000: Loss=3.4581, Acc=0.3947
  Batch 1050: Loss=2.5322, Acc=0.4533
  Batch 1100: Loss=3.0209, Acc=0.4216
  Batch 1150: Loss=3.1529, Acc=0.4034
  Batch 1200: Loss=3.0153, Acc=0.4175
  Batch 1250: Loss=3.0986, Acc=0.4027
  Batch 1300: Loss=2.7704, Acc=0.4706
  Batch 1350: Loss=2.9589, Acc=0.4183
  Batch 1400: Loss=3.0743, Acc=0.4215

  Train Loss: 3.1177, Train Acc: 0.3998
  Val Loss: 2.9212, Val Acc: 0.4474
  âœ“ Saved best model (val_loss=2.9212)

Epoch 9/15
----------------------------------------
  Batch 50: Loss=2.8230, Acc=0.4167
  Batch 100: Loss=3.3969, Acc=0.3832
  Batch 150: Loss=2.7637, Acc=0.4652
  Batch 200: Loss=2.9114, Acc=0.4616
  Batch 250: Loss=3.1024, Acc=0.4307
  Batch 300: Loss=3.3911, Acc=0.3881
  Batch 350: Loss=3.1669, Acc=0.4081
  Batch 400: Loss=3.0816, Acc=0.4109
  Batch 450: Loss=3.2214, Acc=0.4070
  Batch 500: Loss=2.7901, Acc=0.4501
  Batch 550: Loss=2.9019, Acc=0.4310
  Batch 600: Loss=2.7505, Acc=0.4633
  Batch 650: Loss=2.7949, Acc=0.4161
  Batch 700: Loss=2.9971, Acc=0.4245
  Batch 750: Loss=3.0014, Acc=0.4291
  Batch 800: Loss=2.8123, Acc=0.4497
  Batch 850: Loss=3.0882, Acc=0.4399
  Batch 900: Loss=2.9217, Acc=0.4731
  Batch 950: Loss=2.8157, Acc=0.4451
  Batch 1000: Loss=2.6256, Acc=0.4854
  Batch 1050: Loss=2.6856, Acc=0.4687
  Batch 1100: Loss=2.6817, Acc=0.5094
  Batch 1150: Loss=2.8682, Acc=0.4741
  Batch 1200: Loss=2.4732, Acc=0.4994
  Batch 1250: Loss=2.8993, Acc=0.4717
  Batch 1300: Loss=2.4855, Acc=0.5000
  Batch 1350: Loss=2.8086, Acc=0.4719
  Batch 1400: Loss=2.5497, Acc=0.4925

  Train Loss: 2.8493, Train Acc: 0.4555
  Val Loss: 2.4377, Val Acc: 0.5608
  âœ“ Saved best model (val_loss=2.4377)

Epoch 10/15
----------------------------------------
  Batch 50: Loss=2.8216, Acc=0.4715
  Batch 100: Loss=2.7686, Acc=0.4888
  Batch 150: Loss=2.5269, Acc=0.5372
  Batch 200: Loss=2.4314, Acc=0.5375
  Batch 250: Loss=2.5321, Acc=0.5237
  Batch 300: Loss=2.6300, Acc=0.5180
  Batch 350: Loss=2.5326, Acc=0.5358
  Batch 400: Loss=2.3265, Acc=0.5633
  Batch 450: Loss=2.7727, Acc=0.4897
  Batch 500: Loss=2.2988, Acc=0.5602
  Batch 550: Loss=2.1681, Acc=0.5908
  Batch 600: Loss=2.2187, Acc=0.5823
  Batch 650: Loss=2.3787, Acc=0.5622
  Batch 700: Loss=2.0971, Acc=0.5977
  Batch 750: Loss=2.4684, Acc=0.5353
  Batch 800: Loss=2.4630, Acc=0.5415
  Batch 850: Loss=2.3762, Acc=0.5536
  Batch 900: Loss=2.6978, Acc=0.5118
  Batch 950: Loss=2.0211, Acc=0.6088
  Batch 1000: Loss=2.2316, Acc=0.5773
  Batch 1050: Loss=2.2738, Acc=0.5755
  Batch 1100: Loss=2.2434, Acc=0.6111
  Batch 1150: Loss=2.1716, Acc=0.6085
  Batch 1200: Loss=2.5162, Acc=0.5544
  Batch 1250: Loss=2.7008, Acc=0.5712
  Batch 1300: Loss=2.1193, Acc=0.6408
  Batch 1350: Loss=2.2892, Acc=0.5878
  Batch 1400: Loss=2.5832, Acc=0.5919

  Train Loss: 2.4093, Train Acc: 0.5559
  Val Loss: 1.8117, Val Acc: 0.6967
  âœ“ Saved best model (val_loss=1.8117)

Epoch 11/15
----------------------------------------
  Batch 50: Loss=2.2654, Acc=0.6201
  Batch 100: Loss=1.8106, Acc=0.6585
  Batch 150: Loss=2.0934, Acc=0.6061
  Batch 200: Loss=2.0944, Acc=0.6161
  Batch 250: Loss=2.0673, Acc=0.6258
  Batch 300: Loss=1.9771, Acc=0.6599
  Batch 350: Loss=1.8145, Acc=0.6541
  Batch 400: Loss=2.2250, Acc=0.6086
  Batch 450: Loss=2.2377, Acc=0.6296
  Batch 500: Loss=1.9459, Acc=0.6756
  Batch 550: Loss=2.0784, Acc=0.6076
  Batch 600: Loss=1.7869, Acc=0.6715
  Batch 650: Loss=1.9839, Acc=0.6341
  Batch 700: Loss=2.2044, Acc=0.6194
  Batch 750: Loss=2.2662, Acc=0.6037
  Batch 800: Loss=2.0453, Acc=0.6564
  Batch 850: Loss=1.6275, Acc=0.6957
  Batch 900: Loss=1.9042, Acc=0.6675
  Batch 950: Loss=1.8759, Acc=0.6577
  Batch 1000: Loss=1.8116, Acc=0.6608
  Batch 1050: Loss=1.7572, Acc=0.6809
  Batch 1100: Loss=2.0963, Acc=0.6496
  Batch 1150: Loss=1.3725, Acc=0.7354
  Batch 1200: Loss=2.1608, Acc=0.6314
  Batch 1250: Loss=2.0540, Acc=0.6527
  Batch 1300: Loss=1.8896, Acc=0.6932
  Batch 1350: Loss=1.4968, Acc=0.7312
  Batch 1400: Loss=1.6379, Acc=0.7102

  Train Loss: 1.9565, Train Acc: 0.6520
  Val Loss: 1.2945, Val Acc: 0.7989
  âœ“ Saved best model (val_loss=1.2945)

Epoch 12/15
----------------------------------------
  Batch 50: Loss=1.7103, Acc=0.6964
  Batch 100: Loss=2.0481, Acc=0.6538
  Batch 150: Loss=1.6324, Acc=0.7113
  Batch 200: Loss=1.4980, Acc=0.7230
  Batch 250: Loss=1.4649, Acc=0.7559
  Batch 300: Loss=1.6299, Acc=0.7158
  Batch 350: Loss=1.5397, Acc=0.7443
  Batch 400: Loss=1.5464, Acc=0.7361
  Batch 450: Loss=1.3021, Acc=0.7625
  Batch 500: Loss=1.2581, Acc=0.7819
  Batch 550: Loss=1.1471, Acc=0.7933
  Batch 600: Loss=1.3983, Acc=0.7356
  Batch 650: Loss=1.6466, Acc=0.7061
  Batch 700: Loss=1.4922, Acc=0.7451
  Batch 750: Loss=1.3454, Acc=0.7445
  Batch 800: Loss=1.1668, Acc=0.8177
  Batch 850: Loss=1.5094, Acc=0.7580
  Batch 900: Loss=1.3414, Acc=0.7784
  Batch 950: Loss=1.0697, Acc=0.8058
  Batch 1000: Loss=1.4351, Acc=0.7622
  Batch 1050: Loss=1.4135, Acc=0.7728
  Batch 1100: Loss=0.7752, Acc=0.8586
  Batch 1150: Loss=1.1254, Acc=0.8113
  Batch 1200: Loss=1.2459, Acc=0.7981
  Batch 1250: Loss=1.4077, Acc=0.7669
  Batch 1300: Loss=0.9630, Acc=0.8453
  Batch 1350: Loss=1.0881, Acc=0.8290
  Batch 1400: Loss=1.2098, Acc=0.7905

  Train Loss: 1.4160, Train Acc: 0.7612
  Val Loss: 0.7604, Val Acc: 0.8961
  âœ“ Saved best model (val_loss=0.7604)

Epoch 13/15
----------------------------------------
  Batch 50: Loss=1.1262, Acc=0.8310
  Batch 100: Loss=1.1213, Acc=0.8193
  Batch 150: Loss=0.8381, Acc=0.8527
  Batch 200: Loss=0.7731, Acc=0.8835
  Batch 250: Loss=0.8917, Acc=0.8474
  Batch 300: Loss=1.2845, Acc=0.8151
  Batch 350: Loss=0.8078, Acc=0.8804
  Batch 400: Loss=0.8802, Acc=0.8595
  Batch 450: Loss=0.6742, Acc=0.8951
  Batch 500: Loss=0.7938, Acc=0.8918
  Batch 550: Loss=0.7215, Acc=0.8767
  Batch 600: Loss=0.9234, Acc=0.8470
  Batch 650: Loss=1.1929, Acc=0.8268
  Batch 700: Loss=0.7799, Acc=0.8847
  Batch 750: Loss=0.7105, Acc=0.8912
  Batch 800: Loss=0.9014, Acc=0.8687
  Batch 850: Loss=0.7513, Acc=0.8801
  Batch 900: Loss=0.7720, Acc=0.8893
  Batch 950: Loss=0.8568, Acc=0.8714
  Batch 1000: Loss=0.6942, Acc=0.8959
  Batch 1050: Loss=0.6109, Acc=0.9013
  Batch 1100: Loss=0.7379, Acc=0.8763
  Batch 1150: Loss=0.5791, Acc=0.9075
  Batch 1200: Loss=0.6014, Acc=0.9123
  Batch 1250: Loss=0.8004, Acc=0.8575
  Batch 1300: Loss=0.6356, Acc=0.8974
  Batch 1350: Loss=0.6763, Acc=0.8933
  Batch 1400: Loss=0.7166, Acc=0.8833

  Train Loss: 0.8369, Train Acc: 0.8714
  Val Loss: 0.4874, Val Acc: 0.9365
  âœ“ Saved best model (val_loss=0.4874)

Epoch 14/15
----------------------------------------
  Batch 50: Loss=0.8877, Acc=0.8813
  Batch 100: Loss=0.6838, Acc=0.8966
  Batch 150: Loss=0.6095, Acc=0.9138
  Batch 200: Loss=0.8035, Acc=0.8660
  Batch 250: Loss=0.4235, Acc=0.9376
  Batch 300: Loss=0.4280, Acc=0.9415
  Batch 350: Loss=0.6728, Acc=0.9068
  Batch 400: Loss=0.5465, Acc=0.9258
  Batch 450: Loss=0.6248, Acc=0.9155
  Batch 500: Loss=0.5004, Acc=0.9330
  Batch 550: Loss=0.5263, Acc=0.9205
  Batch 600: Loss=0.3896, Acc=0.9450
  Batch 650: Loss=0.4703, Acc=0.9236
  Batch 700: Loss=0.2859, Acc=0.9612
  Batch 750: Loss=0.8092, Acc=0.8929
  Batch 800: Loss=0.5364, Acc=0.9401
  Batch 850: Loss=0.5594, Acc=0.9059
  Batch 900: Loss=0.6518, Acc=0.9013
  Batch 950: Loss=0.4786, Acc=0.9241
  Batch 1000: Loss=0.5532, Acc=0.9203
  Batch 1050: Loss=0.5039, Acc=0.9220
  Batch 1100: Loss=0.5108, Acc=0.9224
  Batch 1150: Loss=0.5752, Acc=0.9093
  Batch 1200: Loss=0.7254, Acc=0.9100
  Batch 1250: Loss=0.3789, Acc=0.9444
  Batch 1300: Loss=0.5829, Acc=0.9117
  Batch 1350: Loss=0.4877, Acc=0.9307
  Batch 1400: Loss=0.5371, Acc=0.9280

  Train Loss: 0.5546, Train Acc: 0.9188
  Val Loss: 0.3564, Val Acc: 0.9567
  âœ“ Saved best model (val_loss=0.3564)

Epoch 15/15
----------------------------------------
  Batch 50: Loss=0.3556, Acc=0.9531
  Batch 100: Loss=0.4614, Acc=0.9306
  Batch 150: Loss=0.5666, Acc=0.9193
  Batch 200: Loss=0.7003, Acc=0.8983
  Batch 250: Loss=0.3557, Acc=0.9520
  Batch 300: Loss=0.3705, Acc=0.9476
  Batch 350: Loss=0.3084, Acc=0.9558
  Batch 400: Loss=0.3619, Acc=0.9489
  Batch 450: Loss=0.4014, Acc=0.9464
  Batch 500: Loss=0.2629, Acc=0.9600
  Batch 550: Loss=0.3877, Acc=0.9538
  Batch 600: Loss=0.4621, Acc=0.9289
  Batch 650: Loss=0.3906, Acc=0.9458
  Batch 700: Loss=0.5688, Acc=0.9237
  Batch 750: Loss=0.3573, Acc=0.9474
  Batch 800: Loss=0.3425, Acc=0.9519
  Batch 850: Loss=0.2948, Acc=0.9582
  Batch 900: Loss=0.4881, Acc=0.9296
  Batch 950: Loss=0.4744, Acc=0.9383
  Batch 1000: Loss=0.2874, Acc=0.9584
  Batch 1050: Loss=0.4852, Acc=0.9267
  Batch 1100: Loss=0.2839, Acc=0.9631
  Batch 1150: Loss=0.2433, Acc=0.9668
  Batch 1200: Loss=0.3326, Acc=0.9588
  Batch 1250: Loss=0.2430, Acc=0.9643
  Batch 1300: Loss=0.2706, Acc=0.9667
  Batch 1350: Loss=0.3271, Acc=0.9572
  Batch 1400: Loss=0.1498, Acc=0.9838

  Train Loss: 0.4019, Train Acc: 0.9432
  Val Loss: 0.2793, Val Acc: 0.9690
  âœ“ Saved best model (val_loss=0.2793)

âœ“ Training complete. Best validation loss: 0.2793

Evaluating Control Transformer...
  Test Loss: 0.2969
  Test Accuracy: 0.9665

############################################################
# RECURSIVE TRANSFORMER (TRM-Inspired)
############################################################

Recursive Model Parameters: 1,000,902

============================================================
TRAINING RECURSIVE TRANSFORMER (TRM-Inspired)
============================================================
  Deep recursion cycles (T): 2
  Inner iterations (n): 3
  Deep supervision steps: 2
  ACT loss weight: 0.1
  Halt exploration prob: 0.1
  Max halt steps: 16
============================================================


Epoch 1/15
----------------------------------------
  Batch 50: Loss=8.5398, Acc=0.0000
  Batch 100: Loss=7.9320, Acc=0.0000
  Batch 150: Loss=7.5619, Acc=0.0000
  Batch 200: Loss=7.0474, Acc=0.0000
  Batch 250: Loss=6.6151, Acc=0.0000
  Batch 300: Loss=6.5130, Acc=0.0000
  Batch 350: Loss=6.1593, Acc=0.0000
  Batch 400: Loss=5.9343, Acc=0.0000
  Batch 450: Loss=5.5720, Acc=0.0000
  Batch 500: Loss=5.3779, Acc=0.0000
  Batch 550: Loss=5.6970, Acc=0.0000
  Batch 600: Loss=5.3460, Acc=0.0000
  Batch 650: Loss=5.1903, Acc=0.0000
  Batch 700: Loss=5.3862, Acc=0.0000
  Batch 750: Loss=5.1300, Acc=0.0000
  Batch 800: Loss=5.4364, Acc=0.0000
  Batch 850: Loss=5.6332, Acc=0.0000
  Batch 900: Loss=5.0852, Acc=0.0000
  Batch 950: Loss=4.5748, Acc=0.0000
  Batch 1000: Loss=5.1794, Acc=0.0000
  Batch 1050: Loss=5.1602, Acc=0.0000
  Batch 1100: Loss=5.0712, Acc=0.0000
  Batch 1150: Loss=5.1069, Acc=0.0000
  Batch 1200: Loss=4.8734, Acc=0.0000
  Batch 1250: Loss=4.7535, Acc=0.0000
  Batch 1300: Loss=4.7798, Acc=0.0075
  Batch 1350: Loss=5.0329, Acc=0.0120
  Batch 1400: Loss=4.9269, Acc=0.0264
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mrecursive-transformer[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../oscar/home/bpeckham/TinyAI/wandb/run-20251210_123815-ds6grqh1/logs[0m

==========================================
Python script finished at Wed Dec 10 01:43:20 PM EST 2025
Exit code: 1
==========================================
