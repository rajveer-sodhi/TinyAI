## SLURM PROLOG ###############################################################
##    Job ID : 14796683
##  Job Name : TinyAI
##  Nodelist : gpu2004
##      CPUs : 1
##  Mem/Node : 64000 MB
## Directory : /oscar/home/bpeckham/TinyAI
##   Job Started : Thu Dec 11 03:42:34 AM EST 2025
###############################################################################

==========================================
Job started at: Thu Dec 11 03:42:34 AM EST 2025
Job ID: 14796683
Node: gpu2004
==========================================

Python location: /users/bpeckham/.conda/envs/tinyai/bin/python
Python version: Python 3.10.19
Conda environment: tinyai
Installing/verifying required packages...
TensorFlow 2.15.0 and NumPy 1.26.4 imported successfully
GPU Information:
Thu Dec 11 03:42:46 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 6000                On  | 00000000:40:00.0 Off |                  Off |
| 33%   26C    P8              15W / 260W |     24MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     30537      G   /usr/libexec/Xorg                            22MiB |
+---------------------------------------------------------------------------------------+

TensorFlow GPU Detection:
TF version: 2.15.0
Built with CUDA: True
GPUs detected: 0
GPU devices: []

==========================================
Starting main Python script at Thu Dec 11 03:42:50 AM EST 2025
==========================================

Configuration:
  d_model: 32
  num_layers: 1
  num_heads: 2
  ff_dim: 32
  deep_rec_cycles: 2
  num_l_steps: 3
  epochs: 30
  batch_size: 8
  learning_rate: 1e-3
  max_seq_length: 128

To monitor training with WandB:
  1. First, log in to WandB (one time only):
     python wandb_login.py
  2. Then view metrics at: https://wandb.ai
  (No port forwarding needed - metrics upload automatically!)


============================================================
TinyAI TRAINING SCRIPT
============================================================

Configuration:
  Data path: /oscar/home/bpeckham/TinyAI/preprocessing/data/final_train_data.txt
  Vocab path: /oscar/home/bpeckham/TinyAI/preprocessing/data/vocab.json
  Max sequence length: 128
  Model dimension: 32
  Layers: 1
  Heads: 2
  FF dim: 32
  Dropout: 0.1
  Deep recursion cycles: 2
  Inner iterations: 3
  Deep supervision steps: 2
  Batch size: 8
  Epochs: 30
  Learning rate: 0.001
============================================================

Loading tokenizer...
Loaded vocabulary with 13678 tokens
  PAD=0, UNK=1, BOS=32, EOS=2
Loading data...
Loaded 7473 samples
Train: 5979, Val: 747, Test: 747

############################################################
# RECURSIVE TRANSFORMER (TRM-Inspired)
############################################################

Recursive Model Parameters: 899,855

============================================================
TRAINING RECURSIVE TRANSFORMER (TRM-Inspired)
============================================================
  Deep recursion cycles (T): 2
  Inner iterations (n): 3
  Deep supervision steps: 2
  ACT loss weight: 0.1
  Halt exploration prob: 0.1
  Max halt steps: 16
============================================================


Epoch 1/30
----------------------------------------
  Batch 50: Loss=7.4463, Acc=0.0227
  Batch 100: Loss=6.0487, Acc=0.0367
  Batch 150: Loss=6.1394, Acc=0.0000
  Batch 200: Loss=6.0110, Acc=0.0892
  Batch 250: Loss=5.2095, Acc=0.1851
  Batch 300: Loss=4.9862, Acc=0.2486
  Batch 350: Loss=4.7606, Acc=0.2214
  Batch 400: Loss=4.8895, Acc=0.2297
  Batch 450: Loss=4.8326, Acc=0.2464
  Batch 500: Loss=5.1411, Acc=0.2022
  Batch 550: Loss=3.8065, Acc=0.3221
  Batch 600: Loss=4.9077, Acc=0.2425
  Batch 650: Loss=4.6305, Acc=0.2760
  Batch 700: Loss=4.1536, Acc=0.2708

  Train Loss: 5.3645, Train Acc: 0.1744
  Val Loss: 4.4902, Val Acc: 0.2607
  ✓ Saved best model (val_loss=4.4902)

Epoch 2/30
----------------------------------------
  Batch 50: Loss=4.1722, Acc=0.3012
  Batch 100: Loss=4.3124, Acc=0.2721
  Batch 150: Loss=4.3347, Acc=0.2492
  Batch 200: Loss=4.1872, Acc=0.2656
  Batch 250: Loss=4.0883, Acc=0.2638
  Batch 300: Loss=3.9671, Acc=0.3067
  Batch 350: Loss=3.8561, Acc=0.2906
  Batch 400: Loss=4.3437, Acc=0.2589
  Batch 450: Loss=4.2503, Acc=0.2797
  Batch 500: Loss=4.3236, Acc=0.2737
  Batch 550: Loss=4.5315, Acc=0.2647
  Batch 600: Loss=3.9911, Acc=0.3040
  Batch 650: Loss=4.0496, Acc=0.2970
  Batch 700: Loss=4.2617, Acc=0.2812

  Train Loss: 4.2186, Train Acc: 0.2767
  Val Loss: 4.2046, Val Acc: 0.2840
  ✓ Saved best model (val_loss=4.2046)

Epoch 3/30
----------------------------------------
  Batch 50: Loss=3.8815, Acc=0.2974
  Batch 100: Loss=3.9164, Acc=0.3032
  Batch 150: Loss=3.6408, Acc=0.3190
  Batch 200: Loss=4.0869, Acc=0.3088
  Batch 250: Loss=4.1830, Acc=0.2913
  Batch 300: Loss=4.2404, Acc=0.3098
  Batch 350: Loss=4.5494, Acc=0.2663
  Batch 400: Loss=3.7934, Acc=0.3213
  Batch 450: Loss=3.5739, Acc=0.3246
  Batch 500: Loss=4.0153, Acc=0.2896
  Batch 550: Loss=4.2065, Acc=0.2897
  Batch 600: Loss=3.7107, Acc=0.3192
  Batch 650: Loss=4.1015, Acc=0.3031
  Batch 700: Loss=3.8126, Acc=0.3298

  Train Loss: 3.9883, Train Acc: 0.3016
  Val Loss: 4.0389, Val Acc: 0.3047
  ✓ Saved best model (val_loss=4.0389)

Epoch 4/30
----------------------------------------
  Batch 50: Loss=3.7575, Acc=0.3309
  Batch 100: Loss=3.6310, Acc=0.3201
  Batch 150: Loss=4.2187, Acc=0.3015
  Batch 200: Loss=3.7349, Acc=0.3435
  Batch 250: Loss=4.1201, Acc=0.3148
  Batch 300: Loss=4.1392, Acc=0.2904
  Batch 350: Loss=3.7107, Acc=0.3275
  Batch 400: Loss=3.9666, Acc=0.3194
  Batch 450: Loss=3.8661, Acc=0.3431
  Batch 500: Loss=3.8807, Acc=0.3412
  Batch 550: Loss=3.8188, Acc=0.3186
  Batch 600: Loss=3.6027, Acc=0.3448
  Batch 650: Loss=3.8149, Acc=0.3625
  Batch 700: Loss=3.9276, Acc=0.3051

  Train Loss: 3.8221, Train Acc: 0.3294
  Val Loss: 3.8625, Val Acc: 0.3300
  ✓ Saved best model (val_loss=3.8625)

Epoch 5/30
----------------------------------------
  Batch 50: Loss=3.6697, Acc=0.3369
  Batch 100: Loss=3.9450, Acc=0.3210
  Batch 150: Loss=3.8885, Acc=0.3425
  Batch 200: Loss=4.1021, Acc=0.3431
  Batch 250: Loss=3.5209, Acc=0.3638
  Batch 300: Loss=3.4284, Acc=0.3971
  Batch 350: Loss=3.4246, Acc=0.3878
  Batch 400: Loss=4.0017, Acc=0.3034
  Batch 450: Loss=3.5931, Acc=0.3536
  Batch 500: Loss=3.4972, Acc=0.3519
  Batch 550: Loss=3.7296, Acc=0.3519
  Batch 600: Loss=3.4053, Acc=0.4041
  Batch 650: Loss=3.4567, Acc=0.3681
  Batch 700: Loss=3.5887, Acc=0.3714

  Train Loss: 3.6704, Train Acc: 0.3578
  Val Loss: 3.6802, Val Acc: 0.3591
  ✓ Saved best model (val_loss=3.6802)

Epoch 6/30
----------------------------------------
  Batch 50: Loss=3.5994, Acc=0.3526
  Batch 100: Loss=3.6897, Acc=0.3702
  Batch 150: Loss=3.3974, Acc=0.4103
  Batch 200: Loss=3.8227, Acc=0.3691
  Batch 250: Loss=3.7273, Acc=0.3906
  Batch 300: Loss=3.5709, Acc=0.3981
  Batch 350: Loss=3.3211, Acc=0.4213
  Batch 400: Loss=3.2506, Acc=0.4217
  Batch 450: Loss=3.3880, Acc=0.4293
  Batch 500: Loss=3.3245, Acc=0.4268
  Batch 550: Loss=3.3804, Acc=0.4447
  Batch 600: Loss=3.5825, Acc=0.4449
  Batch 650: Loss=3.6195, Acc=0.4795
  Batch 700: Loss=4.0002, Acc=0.4084
