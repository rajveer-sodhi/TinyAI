## SLURM PROLOG ###############################################################
##    Job ID : 14823519
##  Job Name : TinyAI
##  Nodelist : gpu2101
##      CPUs : 1
##  Mem/Node : 64000 MB
## Directory : /oscar/home/bpeckham/TinyAI
##   Job Started : Fri Dec 12 04:34:07 PM EST 2025
###############################################################################

==========================================
Job started at: Fri Dec 12 04:34:07 PM EST 2025
Job ID: 14823519
Node: gpu2101
==========================================

Python location: /users/bpeckham/.conda/envs/tinyai/bin/python
Python version: Python 3.10.19
Conda environment: tinyai
Installing/verifying required packages...
TensorFlow 2.15.0 and NumPy 1.26.4 imported successfully
GPU Information:
Fri Dec 12 16:34:33 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3090        On  | 00000000:61:00.0 Off |                  N/A |
| 30%   25C    P8              24W / 350W |     13MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     51732      G   /usr/libexec/Xorg                             5MiB |
+---------------------------------------------------------------------------------------+

TensorFlow GPU Detection:
TF version: 2.15.0
Built with CUDA: True
GPUs detected: 0
GPU devices: []

==========================================
Starting main Python script at Fri Dec 12 04:34:37 PM EST 2025
==========================================

Configuration:
  d_model: 128
  num_layers: 2
  num_heads: 4
  ff_dim: 256
  deep_rec_cycles: 4
  num_l_steps: 6
  epochs: 15
  batch_size: 8
  learning_rate: 1e-4
  max_seq_length: 128

To monitor training with WandB:
  1. First, log in to WandB (one time only):
     python wandb_login.py
  2. Then view metrics at: https://wandb.ai
  (No port forwarding needed - metrics upload automatically!)


============================================================
TinyAI TRAINING SCRIPT
============================================================

Configuration:
  Data path: /oscar/home/bpeckham/TinyAI/preprocessing/data/final_train_data.txt
  Vocab path: /oscar/home/bpeckham/TinyAI/preprocessing/data/vocab.json
  Max sequence length: 128
  Model dimension: 128
  Layers: 2
  Heads: 4
  FF dim: 256
  Dropout: 0.1
  Deep recursion cycles: 4
  Inner iterations: 6
  Deep supervision steps: 4
  Batch size: 8
  Epochs: 15
  Learning rate: 0.0001
============================================================

Loading tokenizer...
Loaded vocabulary with 12443 tokens
  PAD=0, UNK=1, BOS=12, EOS=2
Using token ID 5 for 'a:'/'A:' detection
Loading data...
Loaded 7473 samples
Train: 5979, Val: 747, Test: 747

############################################################
# CONTROL TRANSFORMER
############################################################

Control Model Parameters: 3,479,707

============================================================
TRAINING CONTROL TRANSFORMER (Single-Pass Baseline)
============================================================


Epoch 1/15
----------------------------------------
  Batch 50: Loss=8.8335, Acc=0.1111, AnsAcc=0.0795
  Batch 100: Loss=8.0300, Acc=0.0542, AnsAcc=0.0340
  Batch 150: Loss=7.2439, Acc=0.0473, AnsAcc=0.0179
  Batch 200: Loss=6.7372, Acc=0.0515, AnsAcc=0.0373
  Batch 250: Loss=6.4591, Acc=0.0774, AnsAcc=0.0531
  Batch 300: Loss=6.3231, Acc=0.1231, AnsAcc=0.1000
  Batch 350: Loss=6.2761, Acc=0.1330, AnsAcc=0.0969
  Batch 400: Loss=6.0262, Acc=0.1670, AnsAcc=0.1528
  Batch 450: Loss=5.9069, Acc=0.1745, AnsAcc=0.1532
  Batch 500: Loss=5.6759, Acc=0.2070, AnsAcc=0.1700
  Batch 550: Loss=5.5349, Acc=0.1828, AnsAcc=0.1402
  Batch 600: Loss=5.6555, Acc=0.1763, AnsAcc=0.1543
  Batch 650: Loss=5.7120, Acc=0.2125, AnsAcc=0.1667
  Batch 700: Loss=5.4366, Acc=0.1992, AnsAcc=0.1837

  Train Loss: 6.4811, Train Acc: 0.1343, Train AnsAcc: 0.1203
  Val Loss: 5.4070, Val Acc: 0.2152, Val AnsAcc: 0.2035
  ✓ Saved best model (val_loss=5.4070)

Epoch 2/15
----------------------------------------
  Batch 50: Loss=5.4653, Acc=0.2225, AnsAcc=0.2170
  Batch 100: Loss=5.3375, Acc=0.2126, AnsAcc=0.2113
  Batch 150: Loss=5.3068, Acc=0.2207, AnsAcc=0.1921
  Batch 200: Loss=5.0820, Acc=0.2450, AnsAcc=0.2589
  Batch 250: Loss=5.1529, Acc=0.2259, AnsAcc=0.1924
  Batch 300: Loss=4.8815, Acc=0.2402, AnsAcc=0.1961
  Batch 350: Loss=5.0955, Acc=0.2358, AnsAcc=0.2090
  Batch 400: Loss=4.7829, Acc=0.2765, AnsAcc=0.3310
  Batch 450: Loss=4.8805, Acc=0.2763, AnsAcc=0.3288
  Batch 500: Loss=4.8581, Acc=0.2391, AnsAcc=0.2345
  Batch 550: Loss=4.8161, Acc=0.2760, AnsAcc=0.2606
  Batch 600: Loss=5.0552, Acc=0.2378, AnsAcc=0.2222
  Batch 650: Loss=4.8967, Acc=0.2619, AnsAcc=0.2658
  Batch 700: Loss=4.6789, Acc=0.2860, AnsAcc=0.2837

  Train Loss: 4.9964, Train Acc: 0.2488, Train AnsAcc: 0.2464
  Val Loss: 4.7500, Val Acc: 0.2807, Val AnsAcc: 0.2798
  ✓ Saved best model (val_loss=4.7500)

Epoch 3/15
----------------------------------------
  Batch 50: Loss=4.6147, Acc=0.2597, AnsAcc=0.2557
  Batch 100: Loss=4.6515, Acc=0.2721, AnsAcc=0.2585
  Batch 150: Loss=4.5632, Acc=0.2931, AnsAcc=0.3333
  Batch 200: Loss=4.5789, Acc=0.2657, AnsAcc=0.2301
  Batch 250: Loss=4.6829, Acc=0.2734, AnsAcc=0.2611
  Batch 300: Loss=4.4473, Acc=0.3013, AnsAcc=0.3081
  Batch 350: Loss=4.5524, Acc=0.2811, AnsAcc=0.2995
  Batch 400: Loss=4.2887, Acc=0.3204, AnsAcc=0.3736
  Batch 450: Loss=4.1132, Acc=0.3458, AnsAcc=0.3645
  Batch 500: Loss=4.5551, Acc=0.2899, AnsAcc=0.3407
  Batch 550: Loss=4.2862, Acc=0.3325, AnsAcc=0.3696
  Batch 600: Loss=4.4774, Acc=0.3028, AnsAcc=0.2735
  Batch 650: Loss=4.2377, Acc=0.3298, AnsAcc=0.3711
  Batch 700: Loss=3.9990, Acc=0.3821, AnsAcc=0.3632

  Train Loss: 4.5104, Train Acc: 0.2963, Train AnsAcc: 0.3011
  Val Loss: 4.4720, Val Acc: 0.3098, Val AnsAcc: 0.3136
  ✓ Saved best model (val_loss=4.4720)

Epoch 4/15
----------------------------------------
  Batch 50: Loss=4.0597, Acc=0.3244, AnsAcc=0.3222
  Batch 100: Loss=4.3213, Acc=0.3319, AnsAcc=0.3762
  Batch 150: Loss=4.4864, Acc=0.3018, AnsAcc=0.3151
  Batch 200: Loss=4.1781, Acc=0.3253, AnsAcc=0.3321
  Batch 250: Loss=4.5144, Acc=0.2866, AnsAcc=0.3146
  Batch 300: Loss=4.0499, Acc=0.3753, AnsAcc=0.3824
  Batch 350: Loss=4.2762, Acc=0.3056, AnsAcc=0.3564
  Batch 400: Loss=4.2213, Acc=0.3245, AnsAcc=0.3541
  Batch 450: Loss=4.4113, Acc=0.3286, AnsAcc=0.3136
  Batch 500: Loss=4.2848, Acc=0.3270, AnsAcc=0.3136
  Batch 550: Loss=4.3745, Acc=0.3028, AnsAcc=0.3757
  Batch 600: Loss=4.4147, Acc=0.3061, AnsAcc=0.2725
  Batch 650: Loss=4.1841, Acc=0.3075, AnsAcc=0.2961
  Batch 700: Loss=4.5534, Acc=0.3043, AnsAcc=0.3507

  Train Loss: 4.2628, Train Acc: 0.3182, Train AnsAcc: 0.3278
  Val Loss: 4.3250, Val Acc: 0.3236, Val AnsAcc: 0.3265
  ✓ Saved best model (val_loss=4.3250)

Epoch 5/15
----------------------------------------
  Batch 50: Loss=3.9112, Acc=0.3669, AnsAcc=0.3397
  Batch 100: Loss=4.1668, Acc=0.3275, AnsAcc=0.3053
  Batch 150: Loss=3.9489, Acc=0.3389, AnsAcc=0.3439
  Batch 200: Loss=4.0225, Acc=0.3258, AnsAcc=0.3500
  Batch 250: Loss=4.3335, Acc=0.3205, AnsAcc=0.3832
  Batch 300: Loss=4.0818, Acc=0.3499, AnsAcc=0.4423
  Batch 350: Loss=4.0107, Acc=0.3558, AnsAcc=0.3352
  Batch 400: Loss=4.4011, Acc=0.3071, AnsAcc=0.2863
  Batch 450: Loss=4.1486, Acc=0.3239, AnsAcc=0.3611
  Batch 500: Loss=4.1679, Acc=0.3114, AnsAcc=0.3000
  Batch 550: Loss=4.4881, Acc=0.2932, AnsAcc=0.2951
  Batch 600: Loss=4.4967, Acc=0.3129, AnsAcc=0.2557
  Batch 650: Loss=4.1614, Acc=0.3284, AnsAcc=0.3883
  Batch 700: Loss=4.2097, Acc=0.3261, AnsAcc=0.3014

  Train Loss: 4.1048, Train Acc: 0.3316, Train AnsAcc: 0.3364
  Val Loss: 4.2210, Val Acc: 0.3341, Val AnsAcc: 0.3367
  ✓ Saved best model (val_loss=4.2210)

Epoch 6/15
----------------------------------------
  Batch 50: Loss=4.2492, Acc=0.3198, AnsAcc=0.3516
  Batch 100: Loss=4.3896, Acc=0.3135, AnsAcc=0.2747
  Batch 150: Loss=4.2093, Acc=0.2940, AnsAcc=0.3279
  Batch 200: Loss=3.8708, Acc=0.3283, AnsAcc=0.3317
  Batch 250: Loss=4.2217, Acc=0.3087, AnsAcc=0.3127
  Batch 300: Loss=3.8608, Acc=0.3403, AnsAcc=0.3160
  Batch 350: Loss=4.1395, Acc=0.3101, AnsAcc=0.4068
  Batch 400: Loss=3.8450, Acc=0.3673, AnsAcc=0.4262
  Batch 450: Loss=4.0479, Acc=0.3374, AnsAcc=0.3130
  Batch 500: Loss=3.8206, Acc=0.3456, AnsAcc=0.3423
  Batch 550: Loss=4.4129, Acc=0.2984, AnsAcc=0.2772
  Batch 600: Loss=3.9684, Acc=0.3363, AnsAcc=0.3130
  Batch 650: Loss=3.7060, Acc=0.3816, AnsAcc=0.3522
  Batch 700: Loss=4.0827, Acc=0.3345, AnsAcc=0.3045

  Train Loss: 3.9768, Train Acc: 0.3429, Train AnsAcc: 0.3464
  Val Loss: 4.1281, Val Acc: 0.3435, Val AnsAcc: 0.3432
  ✓ Saved best model (val_loss=4.1281)

Epoch 7/15
----------------------------------------
  Batch 50: Loss=4.0315, Acc=0.3333, AnsAcc=0.3084
  Batch 100: Loss=3.7960, Acc=0.3913, AnsAcc=0.4121
  Batch 150: Loss=3.8622, Acc=0.3505, AnsAcc=0.3125
  Batch 200: Loss=4.1574, Acc=0.3287, AnsAcc=0.3163
  Batch 250: Loss=4.0230, Acc=0.3313, AnsAcc=0.3139
  Batch 300: Loss=4.2518, Acc=0.2987, AnsAcc=0.2878
  Batch 350: Loss=3.6921, Acc=0.3431, AnsAcc=0.3720
  Batch 400: Loss=3.9341, Acc=0.3755, AnsAcc=0.3527
  Batch 450: Loss=3.7843, Acc=0.3479, AnsAcc=0.3233
  Batch 500: Loss=3.6835, Acc=0.3458, AnsAcc=0.3665
  Batch 550: Loss=3.7304, Acc=0.3599, AnsAcc=0.3146
  Batch 600: Loss=4.1898, Acc=0.3190, AnsAcc=0.2876
  Batch 650: Loss=3.5442, Acc=0.4222, AnsAcc=0.4163
  Batch 700: Loss=3.7709, Acc=0.3783, AnsAcc=0.3425

  Train Loss: 3.8540, Train Acc: 0.3570, Train AnsAcc: 0.3575
  Val Loss: 4.0281, Val Acc: 0.3600, Val AnsAcc: 0.3568
  ✓ Saved best model (val_loss=4.0281)

Epoch 8/15
----------------------------------------
  Batch 50: Loss=3.5823, Acc=0.3682, AnsAcc=0.3568
  Batch 100: Loss=3.9365, Acc=0.3361, AnsAcc=0.3636
  Batch 150: Loss=3.4252, Acc=0.4198, AnsAcc=0.5035
  Batch 200: Loss=3.9712, Acc=0.3546, AnsAcc=0.3138
  Batch 250: Loss=3.6527, Acc=0.3576, AnsAcc=0.3393
  Batch 300: Loss=4.0478, Acc=0.3679, AnsAcc=0.3447
  Batch 350: Loss=3.5461, Acc=0.4106, AnsAcc=0.3918
  Batch 400: Loss=3.6106, Acc=0.4094, AnsAcc=0.3872
  Batch 450: Loss=3.6510, Acc=0.3955, AnsAcc=0.3707
  Batch 500: Loss=3.7919, Acc=0.3567, AnsAcc=0.3240
  Batch 550: Loss=4.0985, Acc=0.3738, AnsAcc=0.3803
  Batch 600: Loss=3.6462, Acc=0.3701, AnsAcc=0.3412
  Batch 650: Loss=3.9435, Acc=0.3769, AnsAcc=0.3838
  Batch 700: Loss=3.4938, Acc=0.3975, AnsAcc=0.3889

  Train Loss: 3.7271, Train Acc: 0.3742, Train AnsAcc: 0.3715
  Val Loss: 3.9197, Val Acc: 0.3779, Val AnsAcc: 0.3708
  ✓ Saved best model (val_loss=3.9197)

Epoch 9/15
----------------------------------------
  Batch 50: Loss=3.4463, Acc=0.3905, AnsAcc=0.3509
  Batch 100: Loss=3.8389, Acc=0.3821, AnsAcc=0.3581
  Batch 150: Loss=3.8188, Acc=0.3509, AnsAcc=0.3278
  Batch 200: Loss=3.6412, Acc=0.3790, AnsAcc=0.3744
  Batch 250: Loss=3.6868, Acc=0.3468, AnsAcc=0.3393
  Batch 300: Loss=3.2979, Acc=0.4194, AnsAcc=0.4043
  Batch 350: Loss=3.6379, Acc=0.4007, AnsAcc=0.3785
  Batch 400: Loss=3.6867, Acc=0.4060, AnsAcc=0.3496
  Batch 450: Loss=3.6336, Acc=0.3782, AnsAcc=0.3354
  Batch 500: Loss=3.8136, Acc=0.3638, AnsAcc=0.3411
  Batch 550: Loss=3.7670, Acc=0.3799, AnsAcc=0.3953
  Batch 600: Loss=4.1376, Acc=0.3678, AnsAcc=0.3579
  Batch 650: Loss=3.4839, Acc=0.4137, AnsAcc=0.4174
  Batch 700: Loss=3.8490, Acc=0.3519, AnsAcc=0.3557

  Train Loss: 3.5939, Train Acc: 0.3938, Train AnsAcc: 0.3897
  Val Loss: 3.7955, Val Acc: 0.4014, Val AnsAcc: 0.3903
  ✓ Saved best model (val_loss=3.7955)

Epoch 10/15
----------------------------------------
  Batch 50: Loss=3.7423, Acc=0.3864, AnsAcc=0.3974
  Batch 100: Loss=3.6260, Acc=0.4019, AnsAcc=0.3838
  Batch 150: Loss=3.3604, Acc=0.4236, AnsAcc=0.4141
  Batch 200: Loss=3.4578, Acc=0.4282, AnsAcc=0.4266
  Batch 250: Loss=3.5727, Acc=0.3658, AnsAcc=0.3344
  Batch 300: Loss=3.1790, Acc=0.4460, AnsAcc=0.4745
  Batch 350: Loss=3.4488, Acc=0.4207, AnsAcc=0.4008
  Batch 400: Loss=3.2346, Acc=0.4487, AnsAcc=0.4303
  Batch 450: Loss=3.3738, Acc=0.4025, AnsAcc=0.3707
  Batch 500: Loss=3.3455, Acc=0.3838, AnsAcc=0.3658
  Batch 550: Loss=3.8624, Acc=0.3679, AnsAcc=0.3595
  Batch 600: Loss=3.3859, Acc=0.4378, AnsAcc=0.4333
  Batch 650: Loss=3.3167, Acc=0.4148, AnsAcc=0.4433
  Batch 700: Loss=3.5522, Acc=0.4004, AnsAcc=0.3764

  Train Loss: 3.4530, Train Acc: 0.4169, Train AnsAcc: 0.4068
  Val Loss: 3.6500, Val Acc: 0.4271, Val AnsAcc: 0.4124
  ✓ Saved best model (val_loss=3.6500)

Epoch 11/15
----------------------------------------
  Batch 50: Loss=3.5541, Acc=0.4165, AnsAcc=0.3754
  Batch 100: Loss=3.4668, Acc=0.4366, AnsAcc=0.4108
  Batch 150: Loss=3.4724, Acc=0.4088, AnsAcc=0.3664
  Batch 200: Loss=3.4227, Acc=0.4403, AnsAcc=0.4452
  Batch 250: Loss=2.9501, Acc=0.4831, AnsAcc=0.5728
  Batch 300: Loss=3.1329, Acc=0.4889, AnsAcc=0.4649
  Batch 350: Loss=3.3244, Acc=0.4412, AnsAcc=0.4155
  Batch 400: Loss=3.4281, Acc=0.4308, AnsAcc=0.3884
  Batch 450: Loss=3.5632, Acc=0.4272, AnsAcc=0.3596
  Batch 500: Loss=2.8950, Acc=0.4800, AnsAcc=0.4685
  Batch 550: Loss=3.3237, Acc=0.4352, AnsAcc=0.3910
  Batch 600: Loss=3.5714, Acc=0.4262, AnsAcc=0.3980
  Batch 650: Loss=3.1645, Acc=0.4769, AnsAcc=0.4122
  Batch 700: Loss=3.2003, Acc=0.4414, AnsAcc=0.3994

  Train Loss: 3.2952, Train Acc: 0.4428, Train AnsAcc: 0.4281
  Val Loss: 3.4669, Val Acc: 0.4607, Val AnsAcc: 0.4425
  ✓ Saved best model (val_loss=3.4669)

Epoch 12/15
----------------------------------------
  Batch 50: Loss=3.3041, Acc=0.4592, AnsAcc=0.4256
  Batch 100: Loss=3.3287, Acc=0.4349, AnsAcc=0.4495
  Batch 150: Loss=3.1052, Acc=0.4520, AnsAcc=0.4527
  Batch 200: Loss=3.1965, Acc=0.4324, AnsAcc=0.4145
  Batch 250: Loss=3.0326, Acc=0.5121, AnsAcc=0.4773
  Batch 300: Loss=3.2403, Acc=0.4453, AnsAcc=0.4085
  Batch 350: Loss=3.4636, Acc=0.4382, AnsAcc=0.5377
  Batch 400: Loss=3.4531, Acc=0.4388, AnsAcc=0.4044
  Batch 450: Loss=3.0099, Acc=0.4958, AnsAcc=0.5132
  Batch 500: Loss=2.8226, Acc=0.5439, AnsAcc=0.5423
  Batch 550: Loss=3.2670, Acc=0.4610, AnsAcc=0.4203
  Batch 600: Loss=3.2711, Acc=0.4657, AnsAcc=0.4157
  Batch 650: Loss=3.0607, Acc=0.4777, AnsAcc=0.4426
  Batch 700: Loss=2.7286, Acc=0.5329, AnsAcc=0.4862

  Train Loss: 3.0873, Train Acc: 0.4808, Train AnsAcc: 0.4643
  Val Loss: 3.1518, Val Acc: 0.5208, Val AnsAcc: 0.5011
  ✓ Saved best model (val_loss=3.1518)

Epoch 13/15
----------------------------------------
  Batch 50: Loss=2.8422, Acc=0.5159, AnsAcc=0.4932
  Batch 100: Loss=3.0036, Acc=0.5039, AnsAcc=0.4951
  Batch 150: Loss=2.8791, Acc=0.5276, AnsAcc=0.4945
  Batch 200: Loss=2.8643, Acc=0.5068, AnsAcc=0.4603
  Batch 250: Loss=2.9940, Acc=0.4885, AnsAcc=0.4495
  Batch 300: Loss=2.8050, Acc=0.5598, AnsAcc=0.5316
  Batch 350: Loss=2.3165, Acc=0.5978, AnsAcc=0.6463
  Batch 400: Loss=3.0162, Acc=0.5248, AnsAcc=0.4423
  Batch 450: Loss=2.2450, Acc=0.6270, AnsAcc=0.6303
  Batch 500: Loss=2.9852, Acc=0.5171, AnsAcc=0.4729
  Batch 550: Loss=2.1063, Acc=0.6517, AnsAcc=0.6352
  Batch 600: Loss=2.6588, Acc=0.5714, AnsAcc=0.5293
  Batch 650: Loss=2.0692, Acc=0.6837, AnsAcc=0.6915
  Batch 700: Loss=2.6602, Acc=0.5730, AnsAcc=0.5426

  Train Loss: 2.6683, Train Acc: 0.5617, Train AnsAcc: 0.5432
  Val Loss: 2.3982, Val Acc: 0.6493, Val AnsAcc: 0.6327
  ✓ Saved best model (val_loss=2.3982)

Epoch 14/15
----------------------------------------
  Batch 50: Loss=2.3620, Acc=0.6286, AnsAcc=0.6090
  Batch 100: Loss=2.2156, Acc=0.6406, AnsAcc=0.6183
  Batch 150: Loss=2.2194, Acc=0.6426, AnsAcc=0.6274
  Batch 200: Loss=2.2485, Acc=0.6371, AnsAcc=0.5560
  Batch 250: Loss=2.2733, Acc=0.6435, AnsAcc=0.6029
  Batch 300: Loss=1.7984, Acc=0.6888, AnsAcc=0.6654
  Batch 350: Loss=1.8695, Acc=0.6996, AnsAcc=0.6626
  Batch 400: Loss=2.0850, Acc=0.6645, AnsAcc=0.6329
  Batch 450: Loss=2.1730, Acc=0.6226, AnsAcc=0.6667
  Batch 500: Loss=2.1144, Acc=0.6710, AnsAcc=0.6652
  Batch 550: Loss=1.9220, Acc=0.6974, AnsAcc=0.6933
  Batch 600: Loss=1.9224, Acc=0.6916, AnsAcc=0.6944
  Batch 650: Loss=2.0118, Acc=0.6847, AnsAcc=0.6370
  Batch 700: Loss=1.9918, Acc=0.6833, AnsAcc=0.6656

  Train Loss: 2.0590, Train Acc: 0.6708, Train AnsAcc: 0.6572
  Val Loss: 1.8137, Val Acc: 0.7450, Val AnsAcc: 0.7377
  ✓ Saved best model (val_loss=1.8137)

Epoch 15/15
----------------------------------------
  Batch 50: Loss=1.7888, Acc=0.7184, AnsAcc=0.6883
  Batch 100: Loss=1.5572, Acc=0.7603, AnsAcc=0.7422
  Batch 150: Loss=1.3806, Acc=0.7803, AnsAcc=0.7633
  Batch 200: Loss=1.3670, Acc=0.7746, AnsAcc=0.7645
  Batch 250: Loss=1.3843, Acc=0.7720, AnsAcc=0.7452
  Batch 300: Loss=1.8748, Acc=0.7189, AnsAcc=0.7070
  Batch 350: Loss=1.9656, Acc=0.6827, AnsAcc=0.6505
  Batch 400: Loss=1.9152, Acc=0.7083, AnsAcc=0.6425
  Batch 450: Loss=1.5578, Acc=0.7729, AnsAcc=0.7688
  Batch 500: Loss=1.5857, Acc=0.7582, AnsAcc=0.7684
  Batch 550: Loss=1.7738, Acc=0.7322, AnsAcc=0.7232
  Batch 600: Loss=1.5734, Acc=0.7882, AnsAcc=0.7889
  Batch 650: Loss=1.2908, Acc=0.7978, AnsAcc=0.7744
  Batch 700: Loss=1.4802, Acc=0.7915, AnsAcc=0.7222

  Train Loss: 1.5946, Train Acc: 0.7511, Train AnsAcc: 0.7427
  Val Loss: 1.4395, Val Acc: 0.8066, Val AnsAcc: 0.8043
  ✓ Saved best model (val_loss=1.4395)

✓ Training complete. Best validation loss: 1.4395

Evaluating Control Transformer...
  Test Loss: 1.4474
  Test Accuracy: 0.8059

############################################################
# RECURSIVE TRANSFORMER (TRM-Inspired)
############################################################

Recursive Model Parameters: 3,579,036

============================================================
TRAINING RECURSIVE TRANSFORMER (TRM-Inspired)
============================================================
  Deep recursion cycles (T): 4
  Inner iterations (n): 6
  Deep supervision steps: 4
  ACT loss weight: 0.01
  Halt exploration prob: 0.1
  Max halt steps: 16
============================================================


Epoch 1/15
----------------------------------------
  Batch 50: Loss=7.9919 (CE=7.9846, ACT=0.7335, Penalty=0.0000) | Acc=0.0000, AnsAcc=0.0000
  Batch 100: Loss=6.8554 (CE=6.8482, ACT=0.7230, Penalty=-0.0000) | Acc=0.0000, AnsAcc=0.0000
  Batch 150: Loss=6.0648 (CE=6.0574, ACT=0.7366, Penalty=0.0000) | Acc=0.0000, AnsAcc=0.0000
  Batch 200: Loss=5.3690 (CE=5.3620, ACT=0.7107, Penalty=-0.0001) | Acc=0.0000, AnsAcc=0.0000
  Batch 250: Loss=4.6712 (CE=4.6640, ACT=0.7377, Penalty=-0.0001) | Acc=0.0000, AnsAcc=0.0000
  Batch 300: Loss=3.7771 (CE=3.7702, ACT=0.7009, Penalty=-0.0001) | Acc=0.0000, AnsAcc=0.0000
  Batch 350: Loss=3.7712 (CE=3.7639, ACT=0.7380, Penalty=-0.0001) | Acc=0.0000, AnsAcc=0.0000
  Batch 400: Loss=3.5955 (CE=3.5884, ACT=0.7198, Penalty=-0.0000) | Acc=0.0000, AnsAcc=0.0000
  Batch 450: Loss=3.6207 (CE=3.6135, ACT=0.7378, Penalty=-0.0001) | Acc=0.0056, AnsAcc=0.0029
  Batch 500: Loss=3.1206 (CE=3.1137, ACT=0.6979, Penalty=-0.0001) | Acc=0.0083, AnsAcc=0.0060
  Batch 550: Loss=3.4476 (CE=3.4405, ACT=0.7274, Penalty=-0.0002) | Acc=0.0259, AnsAcc=0.0224
  Batch 600: Loss=2.4671 (CE=2.4605, ACT=0.6786, Penalty=-0.0002) | Acc=0.0430, AnsAcc=0.0583
  Batch 650: Loss=3.0579 (CE=3.0512, ACT=0.6974, Penalty=-0.0002) | Acc=0.1016, AnsAcc=0.0825
  Batch 700: Loss=2.8024 (CE=2.7961, ACT=0.6499, Penalty=-0.0002) | Acc=0.1537, AnsAcc=0.1525

  Train Loss: 4.5179, Train Acc: 0.0283, Train AnsAcc: 0.0271
  Val Loss: 5.7108, Val Acc: 0.1983, Val AnsAcc: 0.1854
  ✓ Saved best model (val_loss=5.7108)

Epoch 2/15
----------------------------------------
  Batch 50: Loss=3.2813 (CE=3.2750, ACT=0.6586, Penalty=-0.0003) | Acc=0.1846, AnsAcc=0.1491
  Batch 100: Loss=2.4475 (CE=2.4414, ACT=0.6493, Penalty=-0.0004) | Acc=0.2550, AnsAcc=0.2559
  Batch 150: Loss=2.4547 (CE=2.4485, ACT=0.6689, Penalty=-0.0004) | Acc=0.2538, AnsAcc=0.2355
  Batch 200: Loss=2.2421 (CE=2.2359, ACT=0.6580, Penalty=-0.0004) | Acc=0.2386, AnsAcc=0.2051
