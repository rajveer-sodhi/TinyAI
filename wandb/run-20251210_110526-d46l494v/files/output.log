
Epoch 1/15
----------------------------------------
  Batch 50: Loss=8.9268, Acc=0.0450
  Batch 100: Loss=8.3971, Acc=0.0502
  Batch 150: Loss=7.8890, Acc=0.0522
  Batch 200: Loss=7.5622, Acc=0.0377
  Batch 250: Loss=7.1064, Acc=0.0441
  Batch 300: Loss=6.7850, Acc=0.0449
  Batch 350: Loss=6.6763, Acc=0.0281
  Batch 400: Loss=6.5711, Acc=0.0345
  Batch 450: Loss=6.3991, Acc=0.0504
  Batch 500: Loss=6.2333, Acc=0.0283
  Batch 550: Loss=6.1707, Acc=0.0510
  Batch 600: Loss=6.1485, Acc=0.0354
  Batch 650: Loss=5.9077, Acc=0.0531
  Batch 700: Loss=6.0058, Acc=0.0501
  Batch 750: Loss=5.9644, Acc=0.0558
  Batch 800: Loss=6.3074, Acc=0.0747
  Batch 850: Loss=6.0395, Acc=0.0931
  Batch 900: Loss=5.6238, Acc=0.1277
  Batch 950: Loss=5.9704, Acc=0.1008
  Batch 1000: Loss=5.6658, Acc=0.1235
  Batch 1050: Loss=5.8444, Acc=0.1362
  Batch 1100: Loss=5.8428, Acc=0.1256
  Batch 1150: Loss=5.4226, Acc=0.1755
  Batch 1200: Loss=5.6085, Acc=0.1860
  Batch 1250: Loss=5.5813, Acc=0.1747
  Batch 1300: Loss=5.6716, Acc=0.1743
  Batch 1350: Loss=5.6223, Acc=0.1698
  Batch 1400: Loss=5.5144, Acc=0.1870
  Batch 1450: Loss=5.5603, Acc=0.2034
  Batch 1500: Loss=5.2405, Acc=0.2293
  Batch 1550: Loss=5.0418, Acc=0.2156
  Batch 1600: Loss=5.2168, Acc=0.2205
  Batch 1650: Loss=5.0235, Acc=0.2440
  Batch 1700: Loss=5.0793, Acc=0.2435
  Batch 1750: Loss=4.8300, Acc=0.2526
  Batch 1800: Loss=4.8819, Acc=0.2711
  Batch 1850: Loss=4.8287, Acc=0.2838
  Batch 1900: Loss=4.7789, Acc=0.2521
  Batch 1950: Loss=5.4395, Acc=0.2256
  Batch 2000: Loss=5.1192, Acc=0.2446
  Batch 2050: Loss=5.0209, Acc=0.2192
  Batch 2100: Loss=4.7657, Acc=0.2395
  Batch 2150: Loss=4.9546, Acc=0.2251

  Train Loss: 5.8985, Train Acc: 0.1388
  Val Loss: 4.7033, Val Acc: 0.2479
  ✓ Saved best model (val_loss=4.7033)

Epoch 2/15
----------------------------------------
  Batch 50: Loss=4.7875, Acc=0.2652
  Batch 100: Loss=4.4919, Acc=0.2830
  Batch 150: Loss=4.8262, Acc=0.2559
  Batch 200: Loss=4.6291, Acc=0.2560
  Batch 250: Loss=4.6187, Acc=0.2428
  Batch 300: Loss=4.4869, Acc=0.2505
  Batch 350: Loss=4.5757, Acc=0.2346
  Batch 400: Loss=4.3353, Acc=0.3006
  Batch 450: Loss=4.6526, Acc=0.2390
  Batch 500: Loss=4.4030, Acc=0.2635
  Batch 550: Loss=4.5242, Acc=0.2586
  Batch 600: Loss=4.6532, Acc=0.2454
  Batch 650: Loss=4.6180, Acc=0.2694
  Batch 700: Loss=4.1482, Acc=0.2946
  Batch 750: Loss=4.3462, Acc=0.2746
  Batch 800: Loss=4.4002, Acc=0.2381
  Batch 850: Loss=4.3787, Acc=0.2437
  Batch 900: Loss=4.3167, Acc=0.2656
  Batch 950: Loss=4.5705, Acc=0.2411
  Batch 1000: Loss=4.1795, Acc=0.2765
  Batch 1050: Loss=4.4048, Acc=0.2561
  Batch 1100: Loss=4.5155, Acc=0.2343
  Batch 1150: Loss=4.3610, Acc=0.2698
  Batch 1200: Loss=4.3138, Acc=0.2891
  Batch 1250: Loss=4.2487, Acc=0.2559
  Batch 1300: Loss=4.3225, Acc=0.2523
  Batch 1350: Loss=4.2625, Acc=0.2734
  Batch 1400: Loss=4.0886, Acc=0.2683
  Batch 1450: Loss=3.8819, Acc=0.2694
  Batch 1500: Loss=4.3377, Acc=0.2816
  Batch 1550: Loss=4.2557, Acc=0.2763
  Batch 1600: Loss=3.9392, Acc=0.2958
  Batch 1650: Loss=4.1501, Acc=0.2701
  Batch 1700: Loss=4.2166, Acc=0.2898
  Batch 1750: Loss=4.2502, Acc=0.2658
  Batch 1800: Loss=4.2856, Acc=0.2509
  Batch 1850: Loss=4.1662, Acc=0.2817
  Batch 1900: Loss=4.0528, Acc=0.2831
  Batch 1950: Loss=4.2328, Acc=0.2775
  Batch 2000: Loss=4.0548, Acc=0.3220
  Batch 2050: Loss=4.2647, Acc=0.2955
  Batch 2100: Loss=3.8659, Acc=0.2917
  Batch 2150: Loss=4.0698, Acc=0.3112

  Train Loss: 4.3620, Train Acc: 0.2670
  Val Loss: 4.0740, Val Acc: 0.2888
  ✓ Saved best model (val_loss=4.0740)

Epoch 3/15
----------------------------------------
  Batch 50: Loss=4.0213, Acc=0.2755
  Batch 100: Loss=4.3161, Acc=0.2776
  Batch 150: Loss=4.3172, Acc=0.2654
  Batch 200: Loss=3.8965, Acc=0.3220
  Batch 250: Loss=3.7925, Acc=0.3192
  Batch 300: Loss=3.9631, Acc=0.2877
  Batch 350: Loss=4.1425, Acc=0.2820
  Batch 400: Loss=3.9421, Acc=0.2849
  Batch 450: Loss=4.0940, Acc=0.2816
  Batch 500: Loss=3.9924, Acc=0.3068
  Batch 550: Loss=4.1358, Acc=0.3080
  Batch 600: Loss=4.0113, Acc=0.3039
  Batch 650: Loss=3.9269, Acc=0.3047
  Batch 700: Loss=4.1067, Acc=0.2653
  Batch 750: Loss=4.1358, Acc=0.2569
  Batch 800: Loss=3.9520, Acc=0.2898
  Batch 850: Loss=3.5898, Acc=0.3329
  Batch 900: Loss=3.9606, Acc=0.3102
  Batch 950: Loss=3.8310, Acc=0.3144
  Batch 1000: Loss=3.8693, Acc=0.3036
  Batch 1050: Loss=4.2441, Acc=0.2579
  Batch 1100: Loss=4.0726, Acc=0.3107
  Batch 1150: Loss=3.7333, Acc=0.3329
  Batch 1200: Loss=4.1431, Acc=0.2963
  Batch 1250: Loss=3.9758, Acc=0.2928
  Batch 1300: Loss=4.2486, Acc=0.2759
  Batch 1350: Loss=3.9458, Acc=0.3076
  Batch 1400: Loss=4.0707, Acc=0.3136
  Batch 1450: Loss=4.2332, Acc=0.2870
  Batch 1500: Loss=3.7430, Acc=0.3123
  Batch 1550: Loss=3.5410, Acc=0.3228
  Batch 1600: Loss=4.2416, Acc=0.2947
  Batch 1650: Loss=3.9918, Acc=0.2745
  Batch 1700: Loss=3.8222, Acc=0.3094
  Batch 1750: Loss=3.7668, Acc=0.3266
  Batch 1800: Loss=3.9347, Acc=0.3109
  Batch 1850: Loss=3.7644, Acc=0.3405
  Batch 1900: Loss=3.7965, Acc=0.3262
  Batch 1950: Loss=3.6647, Acc=0.2891
  Batch 2000: Loss=4.2906, Acc=0.2846
  Batch 2050: Loss=3.7650, Acc=0.3015
  Batch 2100: Loss=4.1580, Acc=0.2759
  Batch 2150: Loss=3.5369, Acc=0.3489

  Train Loss: 3.9432, Train Acc: 0.2986
  Val Loss: 3.7545, Val Acc: 0.3241
  ✓ Saved best model (val_loss=3.7545)

Epoch 4/15
----------------------------------------
  Batch 50: Loss=3.8329, Acc=0.3143
  Batch 100: Loss=3.8154, Acc=0.3066
  Batch 150: Loss=3.5273, Acc=0.3214
  Batch 200: Loss=3.9069, Acc=0.3135
  Batch 250: Loss=3.9749, Acc=0.2811
  Batch 300: Loss=3.9866, Acc=0.2856
  Batch 350: Loss=3.9309, Acc=0.2784
  Batch 400: Loss=3.4177, Acc=0.3454
  Batch 450: Loss=3.7968, Acc=0.3259
  Batch 500: Loss=3.8149, Acc=0.3039
  Batch 550: Loss=3.6742, Acc=0.3391
  Batch 600: Loss=4.0806, Acc=0.3057
  Batch 650: Loss=3.9581, Acc=0.3111
  Batch 700: Loss=3.4178, Acc=0.3125
  Batch 750: Loss=3.5828, Acc=0.3499
  Batch 800: Loss=3.9470, Acc=0.3273
  Batch 850: Loss=3.3819, Acc=0.3690
  Batch 900: Loss=3.6509, Acc=0.3149
  Batch 950: Loss=3.7667, Acc=0.3159
  Batch 1000: Loss=3.6060, Acc=0.3424
  Batch 1050: Loss=3.7301, Acc=0.3483
  Batch 1100: Loss=3.5191, Acc=0.3098
  Batch 1150: Loss=3.6099, Acc=0.3055
  Batch 1200: Loss=3.4327, Acc=0.3241
  Batch 1250: Loss=3.7466, Acc=0.3250
  Batch 1300: Loss=3.7686, Acc=0.3259
  Batch 1350: Loss=3.5539, Acc=0.3364
  Batch 1400: Loss=3.6487, Acc=0.3345
  Batch 1450: Loss=3.6534, Acc=0.3698
  Batch 1500: Loss=3.7073, Acc=0.3123
  Batch 1550: Loss=3.6693, Acc=0.3341
  Batch 1600: Loss=3.4451, Acc=0.3482
  Batch 1650: Loss=3.6829, Acc=0.3204
  Batch 1700: Loss=3.6198, Acc=0.3083
  Batch 1750: Loss=3.9424, Acc=0.2974
  Batch 1800: Loss=3.8374, Acc=0.3227
  Batch 1850: Loss=3.7795, Acc=0.2924
  Batch 1900: Loss=3.1935, Acc=0.3811
  Batch 1950: Loss=3.9145, Acc=0.3073
  Batch 2000: Loss=3.7436, Acc=0.3020
  Batch 2050: Loss=3.6195, Acc=0.3228
  Batch 2100: Loss=3.4501, Acc=0.3188
  Batch 2150: Loss=3.6606, Acc=0.3144

  Train Loss: 3.6598, Train Acc: 0.3297
  Val Loss: 3.4832, Val Acc: 0.3587
  ✓ Saved best model (val_loss=3.4832)

Epoch 5/15
----------------------------------------
  Batch 50: Loss=3.7155, Acc=0.2941
  Batch 100: Loss=3.6974, Acc=0.3587
  Batch 150: Loss=3.7642, Acc=0.3384
  Batch 200: Loss=3.2725, Acc=0.3716
  Batch 250: Loss=3.1272, Acc=0.3785
  Batch 300: Loss=3.4678, Acc=0.3159
  Batch 350: Loss=3.4885, Acc=0.3749
  Batch 400: Loss=3.4695, Acc=0.3535
  Batch 450: Loss=3.2975, Acc=0.3969
  Batch 500: Loss=3.5812, Acc=0.3198
  Batch 550: Loss=3.7409, Acc=0.3432
  Batch 600: Loss=3.6248, Acc=0.3561
  Batch 650: Loss=3.5830, Acc=0.3323
  Batch 700: Loss=3.2576, Acc=0.3770
  Batch 750: Loss=3.7006, Acc=0.3102
  Batch 800: Loss=3.3947, Acc=0.3748
  Batch 850: Loss=3.9156, Acc=0.3287
  Batch 900: Loss=3.2752, Acc=0.3791
  Batch 950: Loss=3.4717, Acc=0.3350
  Batch 1000: Loss=3.0796, Acc=0.3806
  Batch 1050: Loss=3.4600, Acc=0.3731
  Batch 1100: Loss=3.3436, Acc=0.3781
  Batch 1150: Loss=3.7153, Acc=0.3613
  Batch 1200: Loss=3.3097, Acc=0.3833
  Batch 1250: Loss=3.4701, Acc=0.3764
  Batch 1300: Loss=3.3557, Acc=0.3640
  Batch 1350: Loss=3.1374, Acc=0.4170
  Batch 1400: Loss=3.4919, Acc=0.3586
  Batch 1450: Loss=3.2224, Acc=0.3568
  Batch 1500: Loss=3.1814, Acc=0.4141
  Batch 1550: Loss=3.1465, Acc=0.3902
  Batch 1600: Loss=3.2409, Acc=0.3925
  Batch 1650: Loss=3.0334, Acc=0.4046
  Batch 1700: Loss=3.1958, Acc=0.3873
  Batch 1750: Loss=3.5936, Acc=0.3350
  Batch 1800: Loss=3.3652, Acc=0.3602
  Batch 1850: Loss=3.5724, Acc=0.3370
  Batch 1900: Loss=2.8172, Acc=0.4573
  Batch 1950: Loss=3.1720, Acc=0.3924
  Batch 2000: Loss=3.3805, Acc=0.3807
  Batch 2050: Loss=3.3112, Acc=0.3534
  Batch 2100: Loss=3.3255, Acc=0.3434
  Batch 2150: Loss=3.5749, Acc=0.3471

  Train Loss: 3.4041, Train Acc: 0.3638
  Val Loss: 3.1876, Val Acc: 0.4088
  ✓ Saved best model (val_loss=3.1876)

Epoch 6/15
----------------------------------------
  Batch 50: Loss=3.3083, Acc=0.3947
  Batch 100: Loss=3.2271, Acc=0.3938
  Batch 150: Loss=3.1438, Acc=0.4057
  Batch 200: Loss=3.2500, Acc=0.3858
  Batch 250: Loss=3.0524, Acc=0.4240
  Batch 300: Loss=2.9667, Acc=0.3998
  Batch 350: Loss=2.9547, Acc=0.4207
  Batch 400: Loss=3.4562, Acc=0.3640
  Batch 450: Loss=2.7390, Acc=0.4419
  Batch 500: Loss=2.6335, Acc=0.4580
  Batch 550: Loss=3.6308, Acc=0.3696
  Batch 600: Loss=3.3005, Acc=0.3883
  Batch 650: Loss=3.3921, Acc=0.3915
  Batch 700: Loss=3.1358, Acc=0.4300
  Batch 750: Loss=3.1722, Acc=0.4096
  Batch 800: Loss=3.7891, Acc=0.3650
  Batch 850: Loss=2.8408, Acc=0.4502
  Batch 900: Loss=3.2881, Acc=0.3945
  Batch 950: Loss=3.3274, Acc=0.3710
  Batch 1000: Loss=3.1885, Acc=0.4166
  Batch 1050: Loss=3.0434, Acc=0.4472
  Batch 1100: Loss=2.7542, Acc=0.4586
  Batch 1150: Loss=3.0519, Acc=0.4468
  Batch 1200: Loss=3.1551, Acc=0.3953
  Batch 1250: Loss=2.9180, Acc=0.4472
  Batch 1300: Loss=2.7738, Acc=0.4624
  Batch 1350: Loss=3.3017, Acc=0.4436
  Batch 1400: Loss=2.8913, Acc=0.4430
  Batch 1450: Loss=3.5412, Acc=0.3825
  Batch 1500: Loss=2.5972, Acc=0.5073
  Batch 1550: Loss=2.8129, Acc=0.4864
  Batch 1600: Loss=2.6968, Acc=0.5157
  Batch 1650: Loss=2.7784, Acc=0.4552
  Batch 1700: Loss=2.7619, Acc=0.4760
  Batch 1750: Loss=3.1944, Acc=0.4344
  Batch 1800: Loss=3.0027, Acc=0.4526
  Batch 1850: Loss=2.6463, Acc=0.4938
  Batch 1900: Loss=2.8464, Acc=0.4522
  Batch 1950: Loss=2.8499, Acc=0.4695
  Batch 2000: Loss=2.1673, Acc=0.5378
  Batch 2050: Loss=2.8632, Acc=0.4825
  Batch 2100: Loss=2.7016, Acc=0.4885
  Batch 2150: Loss=2.3854, Acc=0.5329

  Train Loss: 3.0276, Train Acc: 0.4368
  Val Loss: 2.4144, Val Acc: 0.5771
  ✓ Saved best model (val_loss=2.4144)

Epoch 7/15
----------------------------------------
  Batch 50: Loss=2.6141, Acc=0.5343
  Batch 100: Loss=2.4165, Acc=0.5354
  Batch 150: Loss=2.4791, Acc=0.5285
  Batch 200: Loss=2.4818, Acc=0.5597
  Batch 250: Loss=2.5566, Acc=0.5397
  Batch 300: Loss=2.5255, Acc=0.5390
  Batch 350: Loss=2.5337, Acc=0.5265
  Batch 400: Loss=2.4451, Acc=0.5514
  Batch 450: Loss=2.4520, Acc=0.5488
  Batch 500: Loss=2.2039, Acc=0.5658
  Batch 550: Loss=2.6466, Acc=0.5201
  Batch 600: Loss=2.5958, Acc=0.5372
  Batch 650: Loss=2.5307, Acc=0.5342
  Batch 700: Loss=2.3472, Acc=0.5433
  Batch 750: Loss=2.2843, Acc=0.6104
  Batch 800: Loss=2.8467, Acc=0.5006
  Batch 850: Loss=2.2167, Acc=0.5939
  Batch 900: Loss=2.2279, Acc=0.5954
  Batch 950: Loss=2.5140, Acc=0.5660
  Batch 1000: Loss=2.2236, Acc=0.5637
  Batch 1050: Loss=2.5116, Acc=0.5714
  Batch 1100: Loss=2.7767, Acc=0.5380
  Batch 1150: Loss=2.4755, Acc=0.5733
  Batch 1200: Loss=2.5600, Acc=0.5340
  Batch 1250: Loss=2.3060, Acc=0.5978
  Batch 1300: Loss=2.5052, Acc=0.5549
  Batch 1350: Loss=2.3343, Acc=0.5878
  Batch 1400: Loss=2.1010, Acc=0.6231
  Batch 1450: Loss=2.3192, Acc=0.5798
  Batch 1500: Loss=2.5214, Acc=0.5633
  Batch 1550: Loss=2.0276, Acc=0.6139
  Batch 1600: Loss=2.0828, Acc=0.6368
  Batch 1650: Loss=2.3696, Acc=0.6116
  Batch 1700: Loss=2.1710, Acc=0.5982
  Batch 1750: Loss=2.1179, Acc=0.6439
  Batch 1800: Loss=1.9514, Acc=0.6459
  Batch 1850: Loss=2.0470, Acc=0.6419
  Batch 1900: Loss=2.1315, Acc=0.6209
  Batch 1950: Loss=2.1768, Acc=0.6383
  Batch 2000: Loss=2.1014, Acc=0.6468
  Batch 2050: Loss=1.9711, Acc=0.6461
  Batch 2100: Loss=1.8560, Acc=0.6645
  Batch 2150: Loss=1.7113, Acc=0.6804

  Train Loss: 2.3508, Train Acc: 0.5841
  Val Loss: 1.5379, Val Acc: 0.7584
  ✓ Saved best model (val_loss=1.5379)

Epoch 8/15
----------------------------------------
  Batch 50: Loss=1.5953, Acc=0.7121
  Batch 100: Loss=2.1433, Acc=0.6138
  Batch 150: Loss=1.8282, Acc=0.6654
  Batch 200: Loss=1.8842, Acc=0.6658
  Batch 250: Loss=1.8380, Acc=0.7022
  Batch 300: Loss=2.0169, Acc=0.6667
  Batch 350: Loss=1.7952, Acc=0.6733
  Batch 400: Loss=2.1157, Acc=0.6638
  Batch 450: Loss=1.8755, Acc=0.6675
  Batch 500: Loss=2.1189, Acc=0.6505
  Batch 550: Loss=1.7238, Acc=0.6941
  Batch 600: Loss=1.8832, Acc=0.6855
  Batch 650: Loss=1.6335, Acc=0.7193
  Batch 700: Loss=1.7131, Acc=0.7038
  Batch 750: Loss=1.9033, Acc=0.7018
  Batch 800: Loss=1.8677, Acc=0.6918
  Batch 850: Loss=1.7115, Acc=0.7045
  Batch 900: Loss=1.5203, Acc=0.7526
  Batch 950: Loss=1.2889, Acc=0.7812
  Batch 1000: Loss=1.5042, Acc=0.7464
  Batch 1050: Loss=1.6322, Acc=0.7380
  Batch 1100: Loss=1.5682, Acc=0.7443
  Batch 1150: Loss=1.6377, Acc=0.7691
  Batch 1200: Loss=1.3079, Acc=0.7875
  Batch 1250: Loss=1.1952, Acc=0.8056
  Batch 1300: Loss=1.7494, Acc=0.7263
  Batch 1350: Loss=1.2649, Acc=0.7907
  Batch 1400: Loss=1.2210, Acc=0.8047
  Batch 1450: Loss=1.0808, Acc=0.8217
  Batch 1500: Loss=1.2747, Acc=0.7801
  Batch 1550: Loss=1.4533, Acc=0.7796
  Batch 1600: Loss=1.2467, Acc=0.8116
  Batch 1650: Loss=1.1877, Acc=0.8245
  Batch 1700: Loss=1.1067, Acc=0.8287
  Batch 1750: Loss=1.3987, Acc=0.7823
  Batch 1800: Loss=1.1864, Acc=0.8274
  Batch 1850: Loss=1.0800, Acc=0.8183
  Batch 1900: Loss=0.7536, Acc=0.8813
  Batch 1950: Loss=0.7740, Acc=0.8922
  Batch 2000: Loss=1.3322, Acc=0.7940
  Batch 2050: Loss=0.8275, Acc=0.8912
  Batch 2100: Loss=1.0587, Acc=0.8155
  Batch 2150: Loss=1.0457, Acc=0.8366

  Train Loss: 1.5323, Train Acc: 0.7485
  Val Loss: 0.7391, Val Acc: 0.8986
  ✓ Saved best model (val_loss=0.7391)

Epoch 9/15
----------------------------------------
  Batch 50: Loss=0.7754, Acc=0.8779
  Batch 100: Loss=0.9795, Acc=0.8502
  Batch 150: Loss=0.9278, Acc=0.8570
  Batch 200: Loss=0.9235, Acc=0.8638
  Batch 250: Loss=0.7046, Acc=0.8852
  Batch 300: Loss=0.7785, Acc=0.8688
  Batch 350: Loss=1.0389, Acc=0.8354
  Batch 400: Loss=0.9827, Acc=0.8536
  Batch 450: Loss=0.8658, Acc=0.8732
  Batch 500: Loss=0.8000, Acc=0.8856
  Batch 550: Loss=1.1298, Acc=0.8283
  Batch 600: Loss=0.6345, Acc=0.9024
  Batch 650: Loss=0.8802, Acc=0.8721
  Batch 700: Loss=0.6970, Acc=0.9080
  Batch 750: Loss=0.7038, Acc=0.8937
  Batch 800: Loss=0.7834, Acc=0.8902
  Batch 850: Loss=0.7057, Acc=0.8966
  Batch 900: Loss=0.8109, Acc=0.8878
  Batch 950: Loss=0.9043, Acc=0.8754
  Batch 1000: Loss=0.8075, Acc=0.8810
  Batch 1050: Loss=0.7533, Acc=0.8877
  Batch 1100: Loss=0.5910, Acc=0.9135
  Batch 1150: Loss=0.5791, Acc=0.9044
  Batch 1200: Loss=0.5886, Acc=0.9135
  Batch 1250: Loss=0.7407, Acc=0.8986
  Batch 1300: Loss=1.0365, Acc=0.8378
  Batch 1350: Loss=0.6220, Acc=0.9042
  Batch 1400: Loss=0.5685, Acc=0.9073
  Batch 1450: Loss=0.6114, Acc=0.9174
  Batch 1500: Loss=0.5507, Acc=0.9342
  Batch 1550: Loss=0.6566, Acc=0.9021
  Batch 1600: Loss=0.5180, Acc=0.9411
  Batch 1650: Loss=0.5192, Acc=0.9279
  Batch 1700: Loss=0.5977, Acc=0.9271
  Batch 1750: Loss=0.7523, Acc=0.8987
  Batch 1800: Loss=0.7231, Acc=0.8970
  Batch 1850: Loss=0.4101, Acc=0.9418
  Batch 1900: Loss=0.4904, Acc=0.9307
  Batch 1950: Loss=0.5035, Acc=0.9313
  Batch 2000: Loss=0.9329, Acc=0.8717
  Batch 2050: Loss=0.8338, Acc=0.8886
  Batch 2100: Loss=0.4989, Acc=0.9305
  Batch 2150: Loss=0.4883, Acc=0.9331

  Train Loss: 0.7392, Train Acc: 0.8928
  Val Loss: 0.4327, Val Acc: 0.9454
  ✓ Saved best model (val_loss=0.4327)

Epoch 10/15
----------------------------------------
  Batch 50: Loss=0.4243, Acc=0.9395
  Batch 100: Loss=0.4755, Acc=0.9348
  Batch 150: Loss=0.3227, Acc=0.9559
  Batch 200: Loss=0.4458, Acc=0.9440
  Batch 250: Loss=0.5990, Acc=0.9124
  Batch 300: Loss=0.6373, Acc=0.9168
  Batch 350: Loss=0.4977, Acc=0.9315
  Batch 400: Loss=0.8370, Acc=0.8849
  Batch 450: Loss=0.5809, Acc=0.9120
  Batch 500: Loss=0.4907, Acc=0.9339
  Batch 550: Loss=0.6072, Acc=0.9052
  Batch 600: Loss=0.7697, Acc=0.8890
  Batch 650: Loss=0.6950, Acc=0.9091
  Batch 700: Loss=0.4368, Acc=0.9380
  Batch 750: Loss=0.3519, Acc=0.9515
  Batch 800: Loss=0.3109, Acc=0.9576
  Batch 850: Loss=0.4101, Acc=0.9283
  Batch 900: Loss=0.5127, Acc=0.9274
  Batch 950: Loss=0.4845, Acc=0.9304
  Batch 1000: Loss=0.4995, Acc=0.9244
  Batch 1050: Loss=0.2774, Acc=0.9644
  Batch 1100: Loss=0.3022, Acc=0.9547
  Batch 1150: Loss=0.4330, Acc=0.9368
  Batch 1200: Loss=0.4424, Acc=0.9450
  Batch 1250: Loss=0.6273, Acc=0.9088
  Batch 1300: Loss=0.4434, Acc=0.9370
  Batch 1350: Loss=0.3461, Acc=0.9559
  Batch 1400: Loss=0.2559, Acc=0.9665
  Batch 1450: Loss=0.4345, Acc=0.9394
  Batch 1500: Loss=0.2971, Acc=0.9669
  Batch 1550: Loss=0.4095, Acc=0.9383
  Batch 1600: Loss=0.2819, Acc=0.9575
  Batch 1650: Loss=0.3012, Acc=0.9600
  Batch 1700: Loss=0.5230, Acc=0.9255
  Batch 1750: Loss=0.4658, Acc=0.9250
  Batch 1800: Loss=0.4477, Acc=0.9350
  Batch 1850: Loss=0.2418, Acc=0.9646
  Batch 1900: Loss=0.2811, Acc=0.9589
  Batch 1950: Loss=0.5023, Acc=0.9333
  Batch 2000: Loss=0.5024, Acc=0.9218
  Batch 2050: Loss=0.3193, Acc=0.9603
  Batch 2100: Loss=0.2456, Acc=0.9688
  Batch 2150: Loss=0.5438, Acc=0.9245

  Train Loss: 0.4539, Train Acc: 0.9374
  Val Loss: 0.3065, Val Acc: 0.9639
  ✓ Saved best model (val_loss=0.3065)

Epoch 11/15
----------------------------------------
  Batch 50: Loss=0.4118, Acc=0.9404
  Batch 100: Loss=0.3787, Acc=0.9425
  Batch 150: Loss=0.2921, Acc=0.9655
  Batch 200: Loss=0.2537, Acc=0.9712
  Batch 250: Loss=0.1838, Acc=0.9736
  Batch 300: Loss=0.2547, Acc=0.9655
  Batch 350: Loss=0.3277, Acc=0.9529
  Batch 400: Loss=0.2876, Acc=0.9584
  Batch 450: Loss=0.3846, Acc=0.9498
  Batch 500: Loss=0.2756, Acc=0.9609
  Batch 550: Loss=0.4362, Acc=0.9329
  Batch 600: Loss=0.3009, Acc=0.9604
  Batch 650: Loss=0.2771, Acc=0.9624
  Batch 700: Loss=0.4419, Acc=0.9487
  Batch 750: Loss=0.3845, Acc=0.9474
  Batch 800: Loss=0.4141, Acc=0.9337
  Batch 850: Loss=0.3236, Acc=0.9537
  Batch 900: Loss=0.3412, Acc=0.9566
  Batch 950: Loss=0.2586, Acc=0.9699
  Batch 1000: Loss=0.2124, Acc=0.9747
  Batch 1050: Loss=0.2649, Acc=0.9577
  Batch 1100: Loss=0.2501, Acc=0.9665
  Batch 1150: Loss=0.3995, Acc=0.9478
  Batch 1200: Loss=0.3782, Acc=0.9487
  Batch 1250: Loss=0.1152, Acc=0.9871
  Batch 1300: Loss=0.2644, Acc=0.9617
  Batch 1350: Loss=0.2679, Acc=0.9506
  Batch 1400: Loss=0.3868, Acc=0.9424
  Batch 1450: Loss=0.2066, Acc=0.9642
  Batch 1500: Loss=0.3322, Acc=0.9567
  Batch 1550: Loss=0.2437, Acc=0.9688
  Batch 1600: Loss=0.1847, Acc=0.9761
  Batch 1650: Loss=0.2510, Acc=0.9631
  Batch 1700: Loss=0.3140, Acc=0.9532
  Batch 1750: Loss=0.0763, Acc=0.9907
  Batch 1800: Loss=0.3271, Acc=0.9583
  Batch 1850: Loss=0.4163, Acc=0.9441
  Batch 1900: Loss=0.2460, Acc=0.9650
  Batch 1950: Loss=0.2865, Acc=0.9672
  Batch 2000: Loss=0.2223, Acc=0.9745
  Batch 2050: Loss=0.2604, Acc=0.9697
  Batch 2100: Loss=0.2653, Acc=0.9653
  Batch 2150: Loss=0.3819, Acc=0.9492

  Train Loss: 0.3193, Train Acc: 0.9569
  Val Loss: 0.2341, Val Acc: 0.9736
  ✓ Saved best model (val_loss=0.2341)

Epoch 12/15
----------------------------------------
  Batch 50: Loss=0.4028, Acc=0.9455
  Batch 100: Loss=0.2319, Acc=0.9710
  Batch 150: Loss=0.1973, Acc=0.9698
  Batch 200: Loss=0.3954, Acc=0.9508
  Batch 250: Loss=0.1520, Acc=0.9797
  Batch 300: Loss=0.1371, Acc=0.9869
  Batch 350: Loss=0.2073, Acc=0.9744
  Batch 400: Loss=0.1786, Acc=0.9764
  Batch 450: Loss=0.2100, Acc=0.9674
  Batch 500: Loss=0.2338, Acc=0.9631
  Batch 550: Loss=0.3300, Acc=0.9495
  Batch 600: Loss=0.2864, Acc=0.9588
  Batch 650: Loss=0.1655, Acc=0.9842
  Batch 700: Loss=0.4311, Acc=0.9340
  Batch 750: Loss=0.1792, Acc=0.9804
  Batch 800: Loss=0.3878, Acc=0.9371
  Batch 850: Loss=0.4193, Acc=0.9389
  Batch 900: Loss=0.2479, Acc=0.9683
  Batch 950: Loss=0.3252, Acc=0.9535
  Batch 1000: Loss=0.3289, Acc=0.9568
  Batch 1050: Loss=0.3256, Acc=0.9553
  Batch 1100: Loss=0.2137, Acc=0.9679
  Batch 1150: Loss=0.2813, Acc=0.9648
  Batch 1200: Loss=0.2094, Acc=0.9722
  Batch 1250: Loss=0.1694, Acc=0.9774
  Batch 1300: Loss=0.1490, Acc=0.9805
  Batch 1350: Loss=0.2109, Acc=0.9735
  Batch 1400: Loss=0.2429, Acc=0.9709
  Batch 1450: Loss=0.2083, Acc=0.9722
  Batch 1500: Loss=0.2837, Acc=0.9543
  Batch 1550: Loss=0.3978, Acc=0.9346
  Batch 1600: Loss=0.1869, Acc=0.9786
  Batch 1650: Loss=0.3528, Acc=0.9426
  Batch 1700: Loss=0.2265, Acc=0.9748
  Batch 1750: Loss=0.1584, Acc=0.9779
  Batch 1800: Loss=0.2845, Acc=0.9528
  Batch 1850: Loss=0.5314, Acc=0.9250
  Batch 1900: Loss=0.2934, Acc=0.9574
  Batch 1950: Loss=0.4257, Acc=0.9374
  Batch 2000: Loss=0.2594, Acc=0.9662
  Batch 2050: Loss=0.1466, Acc=0.9805
  Batch 2100: Loss=0.2255, Acc=0.9674
  Batch 2150: Loss=0.1963, Acc=0.9748

  Train Loss: 0.2374, Train Acc: 0.9683
  Val Loss: 0.1890, Val Acc: 0.9811
  ✓ Saved best model (val_loss=0.1890)

Epoch 13/15
----------------------------------------
  Batch 50: Loss=0.1160, Acc=0.9853
  Batch 100: Loss=0.1685, Acc=0.9802
  Batch 150: Loss=0.2133, Acc=0.9761
  Batch 200: Loss=0.1103, Acc=0.9839
  Batch 250: Loss=0.2372, Acc=0.9654
  Batch 300: Loss=0.0961, Acc=0.9870
  Batch 350: Loss=0.1439, Acc=0.9810
  Batch 400: Loss=0.0823, Acc=0.9929
  Batch 450: Loss=0.1706, Acc=0.9751
  Batch 500: Loss=0.1430, Acc=0.9826
  Batch 550: Loss=0.0375, Acc=0.9967
  Batch 600: Loss=0.1141, Acc=0.9809
  Batch 650: Loss=0.0953, Acc=0.9893
  Batch 700: Loss=0.3230, Acc=0.9558
  Batch 750: Loss=0.2184, Acc=0.9658
  Batch 800: Loss=0.0995, Acc=0.9870
  Batch 850: Loss=0.0898, Acc=0.9923
  Batch 900: Loss=0.3095, Acc=0.9542
  Batch 950: Loss=0.1865, Acc=0.9735
  Batch 1000: Loss=0.1818, Acc=0.9757
  Batch 1050: Loss=0.2518, Acc=0.9615
  Batch 1100: Loss=0.1224, Acc=0.9841
  Batch 1150: Loss=0.1571, Acc=0.9806
  Batch 1200: Loss=0.1879, Acc=0.9768
  Batch 1250: Loss=0.1246, Acc=0.9837
  Batch 1300: Loss=0.2270, Acc=0.9709
  Batch 1350: Loss=0.0944, Acc=0.9921
  Batch 1400: Loss=0.1821, Acc=0.9791
  Batch 1450: Loss=0.2873, Acc=0.9561
  Batch 1500: Loss=0.1499, Acc=0.9841
  Batch 1550: Loss=0.0705, Acc=0.9923
  Batch 1600: Loss=0.1206, Acc=0.9860
  Batch 1650: Loss=0.1174, Acc=0.9865
  Batch 1700: Loss=0.0803, Acc=0.9920
  Batch 1750: Loss=0.1188, Acc=0.9868
  Batch 1800: Loss=0.1117, Acc=0.9881
  Batch 1850: Loss=0.1098, Acc=0.9856
  Batch 1900: Loss=0.3043, Acc=0.9555
  Batch 1950: Loss=0.2392, Acc=0.9652
  Batch 2000: Loss=0.1365, Acc=0.9884
  Batch 2050: Loss=0.1463, Acc=0.9803
  Batch 2100: Loss=0.1836, Acc=0.9688
  Batch 2150: Loss=0.0692, Acc=0.9964

  Train Loss: 0.1831, Train Acc: 0.9757
  Val Loss: 0.1576, Val Acc: 0.9858
  ✓ Saved best model (val_loss=0.1576)

Epoch 14/15
----------------------------------------
  Batch 50: Loss=0.2159, Acc=0.9666
  Batch 100: Loss=0.2001, Acc=0.9748
  Batch 150: Loss=0.1077, Acc=0.9865
  Batch 200: Loss=0.1156, Acc=0.9839
  Batch 250: Loss=0.0979, Acc=0.9942
  Batch 300: Loss=0.1280, Acc=0.9848
  Batch 350: Loss=0.1855, Acc=0.9728
  Batch 400: Loss=0.1302, Acc=0.9825
  Batch 450: Loss=0.1276, Acc=0.9830
  Batch 500: Loss=0.1838, Acc=0.9759
  Batch 550: Loss=0.0676, Acc=0.9924
  Batch 600: Loss=0.0836, Acc=0.9900
  Batch 650: Loss=0.2128, Acc=0.9764
  Batch 700: Loss=0.1220, Acc=0.9863
  Batch 750: Loss=0.2363, Acc=0.9631
  Batch 800: Loss=0.1853, Acc=0.9760
  Batch 850: Loss=0.1397, Acc=0.9827
  Batch 900: Loss=0.1877, Acc=0.9725
  Batch 950: Loss=0.0918, Acc=0.9882
  Batch 1000: Loss=0.2429, Acc=0.9661
  Batch 1050: Loss=0.1908, Acc=0.9783
  Batch 1100: Loss=0.1833, Acc=0.9768
  Batch 1150: Loss=0.0678, Acc=0.9930
  Batch 1200: Loss=0.0772, Acc=0.9880
  Batch 1250: Loss=0.1858, Acc=0.9769
  Batch 1300: Loss=0.1488, Acc=0.9767
  Batch 1350: Loss=0.0688, Acc=0.9937
  Batch 1400: Loss=0.0974, Acc=0.9883
  Batch 1450: Loss=0.0524, Acc=0.9883
  Batch 1500: Loss=0.0917, Acc=0.9913
  Batch 1550: Loss=0.1345, Acc=0.9791
  Batch 1600: Loss=0.1538, Acc=0.9809
  Batch 1650: Loss=0.1598, Acc=0.9814
  Batch 1700: Loss=0.0906, Acc=0.9915
  Batch 1750: Loss=0.0833, Acc=0.9906
  Batch 1800: Loss=0.1110, Acc=0.9834
  Batch 1850: Loss=0.1104, Acc=0.9853
  Batch 1900: Loss=0.3045, Acc=0.9536
  Batch 1950: Loss=0.1535, Acc=0.9713
  Batch 2000: Loss=0.1324, Acc=0.9848
  Batch 2050: Loss=0.1706, Acc=0.9740
  Batch 2100: Loss=0.1188, Acc=0.9863
  Batch 2150: Loss=0.1387, Acc=0.9769

  Train Loss: 0.1440, Train Acc: 0.9809
  Val Loss: 0.1365, Val Acc: 0.9884
  ✓ Saved best model (val_loss=0.1365)

Epoch 15/15
----------------------------------------
  Batch 50: Loss=0.0821, Acc=0.9917
  Batch 100: Loss=0.1011, Acc=0.9871
  Batch 150: Loss=0.0837, Acc=0.9883
  Batch 200: Loss=0.2109, Acc=0.9691
  Batch 250: Loss=0.1211, Acc=0.9855
  Batch 300: Loss=0.1229, Acc=0.9775
  Batch 350: Loss=0.0898, Acc=0.9978
  Batch 400: Loss=0.1109, Acc=0.9828
  Batch 450: Loss=0.0867, Acc=0.9913
  Batch 500: Loss=0.0752, Acc=0.9924
  Batch 550: Loss=0.0343, Acc=0.9960
  Batch 600: Loss=0.0936, Acc=0.9891
  Batch 650: Loss=0.1168, Acc=0.9842
  Batch 700: Loss=0.1979, Acc=0.9691
  Batch 750: Loss=0.1548, Acc=0.9758
  Batch 800: Loss=0.1267, Acc=0.9869
  Batch 850: Loss=0.1032, Acc=0.9912
  Batch 900: Loss=0.2354, Acc=0.9715
  Batch 950: Loss=0.1790, Acc=0.9718
  Batch 1000: Loss=0.0923, Acc=0.9870
  Batch 1050: Loss=0.0608, Acc=0.9956
  Batch 1100: Loss=0.1259, Acc=0.9838
  Batch 1150: Loss=0.1801, Acc=0.9729
  Batch 1200: Loss=0.0668, Acc=0.9914
  Batch 1250: Loss=0.0514, Acc=0.9942
  Batch 1300: Loss=0.1796, Acc=0.9739
  Batch 1350: Loss=0.1304, Acc=0.9824
  Batch 1400: Loss=0.0606, Acc=0.9919
  Batch 1450: Loss=0.2233, Acc=0.9706
  Batch 1500: Loss=0.1480, Acc=0.9780
  Batch 1550: Loss=0.1328, Acc=0.9822
  Batch 1600: Loss=0.0737, Acc=0.9929
  Batch 1650: Loss=0.2369, Acc=0.9691
  Batch 1700: Loss=0.0815, Acc=0.9951
  Batch 1750: Loss=0.0612, Acc=0.9956
  Batch 1800: Loss=0.0979, Acc=0.9855
  Batch 1850: Loss=0.2153, Acc=0.9690
  Batch 1900: Loss=0.0934, Acc=0.9891
  Batch 1950: Loss=0.1064, Acc=0.9852
  Batch 2000: Loss=0.1663, Acc=0.9760
  Batch 2050: Loss=0.1117, Acc=0.9836
  Batch 2100: Loss=0.0794, Acc=0.9893
  Batch 2150: Loss=0.0752, Acc=0.9924

  Train Loss: 0.1155, Train Acc: 0.9849
  Val Loss: 0.1199, Val Acc: 0.9907
  ✓ Saved best model (val_loss=0.1199)

✓ Training complete. Best validation loss: 0.1199
