
Epoch 1/15
----------------------------------------
  Batch 50: Loss=8.6774, Acc=0.0443
  Batch 100: Loss=8.2119, Acc=0.0824
  Batch 150: Loss=7.7119, Acc=0.0643
  Batch 200: Loss=7.3530, Acc=0.0392
  Batch 250: Loss=7.0466, Acc=0.0375
  Batch 300: Loss=6.6871, Acc=0.0531
  Batch 350: Loss=6.4226, Acc=0.0432
  Batch 400: Loss=6.3685, Acc=0.0485
  Batch 450: Loss=5.9855, Acc=0.0349
  Batch 500: Loss=6.1092, Acc=0.0611
  Batch 550: Loss=5.9112, Acc=0.0587
  Batch 600: Loss=5.9703, Acc=0.0789
  Batch 650: Loss=5.8269, Acc=0.0756
  Batch 700: Loss=5.9803, Acc=0.1056
  Batch 750: Loss=5.9761, Acc=0.1450
  Batch 800: Loss=5.7452, Acc=0.1106
  Batch 850: Loss=5.9149, Acc=0.1140
  Batch 900: Loss=5.8861, Acc=0.1109
  Batch 950: Loss=5.8343, Acc=0.1301
  Batch 1000: Loss=5.8327, Acc=0.1332
  Batch 1050: Loss=5.7242, Acc=0.1622
  Batch 1100: Loss=5.4936, Acc=0.1675
  Batch 1150: Loss=5.7240, Acc=0.1581
  Batch 1200: Loss=5.5719, Acc=0.1652
  Batch 1250: Loss=5.2379, Acc=0.2141
  Batch 1300: Loss=5.3735, Acc=0.2003
  Batch 1350: Loss=5.4337, Acc=0.1702
  Batch 1400: Loss=5.2414, Acc=0.1803

  Train Loss: 6.1994, Train Acc: 0.1096
  Val Loss: 5.1067, Val Acc: 0.2176
  ✓ Saved best model (val_loss=5.1067)

Epoch 2/15
----------------------------------------
  Batch 50: Loss=5.0881, Acc=0.2064
  Batch 100: Loss=5.0126, Acc=0.2138
  Batch 150: Loss=4.8995, Acc=0.2201
  Batch 200: Loss=5.0397, Acc=0.2299
  Batch 250: Loss=4.9003, Acc=0.2190
  Batch 300: Loss=4.9089, Acc=0.2445
  Batch 350: Loss=4.7168, Acc=0.2535
  Batch 400: Loss=4.5732, Acc=0.2488
  Batch 450: Loss=4.8201, Acc=0.2661
  Batch 500: Loss=4.9942, Acc=0.1998
  Batch 550: Loss=5.0001, Acc=0.2286
  Batch 600: Loss=4.8153, Acc=0.2400
  Batch 650: Loss=4.4628, Acc=0.2353
  Batch 700: Loss=4.8564, Acc=0.2417
  Batch 750: Loss=4.7378, Acc=0.2548
  Batch 800: Loss=4.4290, Acc=0.3017
  Batch 850: Loss=4.4095, Acc=0.2381
  Batch 900: Loss=4.9039, Acc=0.2334
  Batch 950: Loss=4.3674, Acc=0.2880
  Batch 1000: Loss=4.3813, Acc=0.2553
  Batch 1050: Loss=4.4584, Acc=0.2622
  Batch 1100: Loss=4.5571, Acc=0.2761
  Batch 1150: Loss=4.3244, Acc=0.3057
  Batch 1200: Loss=4.4851, Acc=0.2480
  Batch 1250: Loss=4.5758, Acc=0.2724
  Batch 1300: Loss=4.3035, Acc=0.2649
  Batch 1350: Loss=4.1115, Acc=0.2894
  Batch 1400: Loss=4.5480, Acc=0.2830

  Train Loss: 4.6561, Train Acc: 0.2529
  Val Loss: 4.2505, Val Acc: 0.2802
  ✓ Saved best model (val_loss=4.2505)

Epoch 3/15
----------------------------------------
  Batch 50: Loss=4.2214, Acc=0.3224
  Batch 100: Loss=4.1279, Acc=0.2891
  Batch 150: Loss=4.5462, Acc=0.2512
  Batch 200: Loss=4.1152, Acc=0.2710
  Batch 250: Loss=4.1867, Acc=0.2897
  Batch 300: Loss=4.0829, Acc=0.2938
  Batch 350: Loss=4.3188, Acc=0.2844
  Batch 400: Loss=4.0760, Acc=0.2812
  Batch 450: Loss=4.2158, Acc=0.2560
  Batch 500: Loss=4.3163, Acc=0.2614
  Batch 550: Loss=4.2531, Acc=0.2507
  Batch 600: Loss=4.2557, Acc=0.2671
  Batch 650: Loss=3.9165, Acc=0.3164
  Batch 700: Loss=4.1445, Acc=0.2794
  Batch 750: Loss=4.0694, Acc=0.3171
  Batch 800: Loss=3.9265, Acc=0.2797
  Batch 850: Loss=4.1876, Acc=0.2922
  Batch 900: Loss=4.0256, Acc=0.2891
  Batch 950: Loss=3.7986, Acc=0.3261
  Batch 1000: Loss=4.3490, Acc=0.2763
  Batch 1050: Loss=3.9586, Acc=0.3047
  Batch 1100: Loss=4.2925, Acc=0.2563
  Batch 1150: Loss=3.8108, Acc=0.3097
  Batch 1200: Loss=3.9473, Acc=0.3045
  Batch 1250: Loss=4.2139, Acc=0.2574
  Batch 1300: Loss=3.6699, Acc=0.3221
  Batch 1350: Loss=3.8168, Acc=0.2848
  Batch 1400: Loss=3.9772, Acc=0.3161

  Train Loss: 4.1146, Train Acc: 0.2889
  Val Loss: 3.9145, Val Acc: 0.3074
  ✓ Saved best model (val_loss=3.9145)

Epoch 4/15
----------------------------------------
  Batch 50: Loss=3.7632, Acc=0.3252
