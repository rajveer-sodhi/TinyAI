
Epoch 1/30
----------------------------------------
  Batch 50: Loss=8.8069, Acc=0.0000
  Batch 100: Loss=8.6428, Acc=0.0413
  Batch 150: Loss=8.4110, Acc=0.0610
  Batch 200: Loss=8.0762, Acc=0.0455
  Batch 250: Loss=7.8730, Acc=0.0556
  Batch 300: Loss=7.6017, Acc=0.0488
  Batch 350: Loss=7.3512, Acc=0.0431
  Batch 400: Loss=7.1412, Acc=0.0518
  Batch 450: Loss=7.0282, Acc=0.0444
  Batch 500: Loss=6.9055, Acc=0.0436
  Batch 550: Loss=6.6941, Acc=0.0420
  Batch 600: Loss=6.5778, Acc=0.0370
  Batch 650: Loss=6.4090, Acc=0.0493
  Batch 700: Loss=6.3212, Acc=0.0466
  Batch 750: Loss=6.3988, Acc=0.0502
  Batch 800: Loss=6.1019, Acc=0.0383
  Batch 850: Loss=6.2743, Acc=0.0473
  Batch 900: Loss=6.0255, Acc=0.0535
  Batch 950: Loss=6.0273, Acc=0.0385
  Batch 1000: Loss=6.2034, Acc=0.0452
  Batch 1050: Loss=5.8695, Acc=0.0520
  Batch 1100: Loss=5.9930, Acc=0.0528
  Batch 1150: Loss=6.0476, Acc=0.0643
  Batch 1200: Loss=6.1252, Acc=0.0559
  Batch 1250: Loss=5.9934, Acc=0.0682
  Batch 1300: Loss=5.9126, Acc=0.0539
  Batch 1350: Loss=6.0015, Acc=0.0766
  Batch 1400: Loss=5.9142, Acc=0.0766

  Train Loss: 6.7709, Train Acc: 0.0486
  Val Loss: 5.8176, Val Acc: 0.0876
  ✓ Saved best model (val_loss=5.8176)

Epoch 2/30
----------------------------------------
  Batch 50: Loss=6.0327, Acc=0.1092
  Batch 100: Loss=5.7688, Acc=0.1235
  Batch 150: Loss=6.0447, Acc=0.1166
  Batch 200: Loss=5.7337, Acc=0.1529
  Batch 250: Loss=5.7693, Acc=0.1324
  Batch 300: Loss=5.6321, Acc=0.1690
  Batch 350: Loss=5.6261, Acc=0.1717
  Batch 400: Loss=5.7684, Acc=0.1467
  Batch 450: Loss=5.7479, Acc=0.1514
  Batch 500: Loss=5.4430, Acc=0.2149
  Batch 550: Loss=5.5399, Acc=0.1656
  Batch 600: Loss=5.4084, Acc=0.1756
  Batch 650: Loss=5.5169, Acc=0.1951
  Batch 700: Loss=5.2464, Acc=0.2175
  Batch 750: Loss=5.3760, Acc=0.1902
  Batch 800: Loss=5.1477, Acc=0.1923
  Batch 850: Loss=5.2046, Acc=0.2247
  Batch 900: Loss=5.2262, Acc=0.2300
  Batch 950: Loss=5.0918, Acc=0.2218
  Batch 1000: Loss=5.1794, Acc=0.2266
  Batch 1050: Loss=5.0617, Acc=0.2184
  Batch 1100: Loss=4.9981, Acc=0.2294
  Batch 1150: Loss=5.1371, Acc=0.2020
  Batch 1200: Loss=4.8308, Acc=0.2244
  Batch 1250: Loss=5.1072, Acc=0.2252
  Batch 1300: Loss=4.8572, Acc=0.2288
  Batch 1350: Loss=5.2863, Acc=0.2062
  Batch 1400: Loss=4.8333, Acc=0.2426

  Train Loss: 5.4028, Train Acc: 0.1862
  Val Loss: 4.8639, Val Acc: 0.2387
  ✓ Saved best model (val_loss=4.8639)

Epoch 3/30
----------------------------------------
  Batch 50: Loss=4.8436, Acc=0.2480
  Batch 100: Loss=4.8172, Acc=0.2353
  Batch 150: Loss=4.9432, Acc=0.2428
  Batch 200: Loss=5.0348, Acc=0.2299
  Batch 250: Loss=4.9621, Acc=0.2305
  Batch 300: Loss=4.7229, Acc=0.2500
  Batch 350: Loss=4.6738, Acc=0.2571
  Batch 400: Loss=4.7139, Acc=0.2675
  Batch 450: Loss=4.7656, Acc=0.2788
  Batch 500: Loss=4.6137, Acc=0.2531
  Batch 550: Loss=4.7605, Acc=0.2251
  Batch 600: Loss=4.6891, Acc=0.2301
  Batch 650: Loss=4.8795, Acc=0.2493
  Batch 700: Loss=4.5868, Acc=0.2809
  Batch 750: Loss=4.6697, Acc=0.2612
  Batch 800: Loss=4.5480, Acc=0.2429
  Batch 850: Loss=4.7402, Acc=0.2475
  Batch 900: Loss=4.5725, Acc=0.2794
  Batch 950: Loss=4.2634, Acc=0.2629
  Batch 1000: Loss=4.3235, Acc=0.2547
  Batch 1050: Loss=4.6884, Acc=0.2648
  Batch 1100: Loss=4.4528, Acc=0.2743
  Batch 1150: Loss=4.3738, Acc=0.2629
  Batch 1200: Loss=4.4821, Acc=0.2852
  Batch 1250: Loss=4.7171, Acc=0.2484
  Batch 1300: Loss=4.4548, Acc=0.2570
  Batch 1350: Loss=4.3009, Acc=0.2742
  Batch 1400: Loss=4.1056, Acc=0.2722

  Train Loss: 4.6548, Train Acc: 0.2537
  Val Loss: 4.3560, Val Acc: 0.2729
  ✓ Saved best model (val_loss=4.3560)

Epoch 4/30
----------------------------------------
  Batch 50: Loss=4.4475, Acc=0.2695
  Batch 100: Loss=4.3999, Acc=0.2568
  Batch 150: Loss=4.4684, Acc=0.2437
  Batch 200: Loss=4.4762, Acc=0.2744
  Batch 250: Loss=4.5718, Acc=0.2526
  Batch 300: Loss=4.4326, Acc=0.2325
  Batch 350: Loss=4.4533, Acc=0.2429
  Batch 400: Loss=4.2297, Acc=0.2792
  Batch 450: Loss=4.5104, Acc=0.2857
  Batch 500: Loss=4.2878, Acc=0.2970
  Batch 550: Loss=4.2491, Acc=0.2937
  Batch 600: Loss=4.2170, Acc=0.3037
  Batch 650: Loss=4.2733, Acc=0.2736
  Batch 700: Loss=4.2863, Acc=0.2765
  Batch 750: Loss=4.3763, Acc=0.2779
  Batch 800: Loss=4.3020, Acc=0.2740
  Batch 850: Loss=4.3399, Acc=0.2751
  Batch 900: Loss=4.1213, Acc=0.2692
  Batch 950: Loss=4.3733, Acc=0.2545
  Batch 1000: Loss=4.3615, Acc=0.2852
  Batch 1050: Loss=4.1937, Acc=0.2787
  Batch 1100: Loss=4.4402, Acc=0.2379
  Batch 1150: Loss=4.2852, Acc=0.2715
  Batch 1200: Loss=4.1750, Acc=0.2906
  Batch 1250: Loss=4.2742, Acc=0.3007
  Batch 1300: Loss=3.8956, Acc=0.2877
  Batch 1350: Loss=4.3026, Acc=0.2831
  Batch 1400: Loss=4.0128, Acc=0.2877

  Train Loss: 4.2960, Train Acc: 0.2739
  Val Loss: 4.1143, Val Acc: 0.2865
  ✓ Saved best model (val_loss=4.1143)

Epoch 5/30
----------------------------------------
  Batch 50: Loss=4.2785, Acc=0.2899
  Batch 100: Loss=4.1787, Acc=0.2920
  Batch 150: Loss=4.1032, Acc=0.2726
  Batch 200: Loss=4.1737, Acc=0.3124
  Batch 250: Loss=4.2283, Acc=0.2874
  Batch 300: Loss=4.3090, Acc=0.2706
  Batch 350: Loss=4.1582, Acc=0.2887
  Batch 400: Loss=3.9906, Acc=0.2975
  Batch 450: Loss=4.0287, Acc=0.2977
  Batch 500: Loss=4.0854, Acc=0.2708
  Batch 550: Loss=4.1137, Acc=0.3093
  Batch 600: Loss=4.0205, Acc=0.3024
  Batch 650: Loss=3.8693, Acc=0.2911
  Batch 700: Loss=4.2912, Acc=0.2652
  Batch 750: Loss=3.7232, Acc=0.3003
  Batch 800: Loss=3.9111, Acc=0.2931
  Batch 850: Loss=4.0716, Acc=0.2692
  Batch 900: Loss=4.0908, Acc=0.2840
  Batch 950: Loss=4.1242, Acc=0.2890
  Batch 1000: Loss=4.2494, Acc=0.2884
  Batch 1050: Loss=4.1131, Acc=0.2921
  Batch 1100: Loss=4.0651, Acc=0.2655
  Batch 1150: Loss=4.1098, Acc=0.2621
  Batch 1200: Loss=4.0144, Acc=0.2846
  Batch 1250: Loss=4.0848, Acc=0.2948
  Batch 1300: Loss=3.8165, Acc=0.3068
  Batch 1350: Loss=3.9600, Acc=0.2937
  Batch 1400: Loss=3.8984, Acc=0.3013

  Train Loss: 4.0968, Train Acc: 0.2855
  Val Loss: 3.9610, Val Acc: 0.2997
  ✓ Saved best model (val_loss=3.9610)

Epoch 6/30
----------------------------------------
  Batch 50: Loss=3.7020, Acc=0.2905
  Batch 100: Loss=4.0839, Acc=0.2842
  Batch 150: Loss=3.8198, Acc=0.3202
  Batch 200: Loss=3.8857, Acc=0.3316
  Batch 250: Loss=3.9755, Acc=0.2832
  Batch 300: Loss=3.6080, Acc=0.3468
  Batch 350: Loss=4.3157, Acc=0.3001
  Batch 400: Loss=3.9921, Acc=0.2720
  Batch 450: Loss=4.0638, Acc=0.2870
  Batch 500: Loss=3.8117, Acc=0.3065
  Batch 550: Loss=4.2197, Acc=0.2918
  Batch 600: Loss=4.0395, Acc=0.2642
  Batch 650: Loss=3.8348, Acc=0.3062
  Batch 700: Loss=3.6452, Acc=0.3320
  Batch 750: Loss=4.2156, Acc=0.3011
  Batch 800: Loss=4.2906, Acc=0.2707
  Batch 850: Loss=4.0446, Acc=0.2616
  Batch 900: Loss=3.7360, Acc=0.3177
  Batch 950: Loss=4.0722, Acc=0.2711
  Batch 1000: Loss=3.5075, Acc=0.3306
  Batch 1050: Loss=3.9529, Acc=0.2960
  Batch 1100: Loss=4.1057, Acc=0.2934
  Batch 1150: Loss=4.1817, Acc=0.2937
  Batch 1200: Loss=3.7089, Acc=0.3112
  Batch 1250: Loss=3.7801, Acc=0.2818
  Batch 1300: Loss=4.2427, Acc=0.2643
  Batch 1350: Loss=3.6718, Acc=0.3216
  Batch 1400: Loss=3.8320, Acc=0.3038

  Train Loss: 3.9605, Train Acc: 0.2948
  Val Loss: 3.8437, Val Acc: 0.3084
  ✓ Saved best model (val_loss=3.8437)

Epoch 7/30
----------------------------------------
  Batch 50: Loss=3.5977, Acc=0.3254
  Batch 100: Loss=3.7907, Acc=0.3051
  Batch 150: Loss=3.5540, Acc=0.3155
  Batch 200: Loss=3.9026, Acc=0.3282
  Batch 250: Loss=4.0180, Acc=0.2784
  Batch 300: Loss=3.7628, Acc=0.3102
  Batch 350: Loss=3.6928, Acc=0.3028
  Batch 400: Loss=3.8927, Acc=0.3305
  Batch 450: Loss=4.3570, Acc=0.2710
  Batch 500: Loss=3.6809, Acc=0.3545
  Batch 550: Loss=4.0812, Acc=0.2994
  Batch 600: Loss=3.9883, Acc=0.2839
  Batch 650: Loss=3.9542, Acc=0.2943
  Batch 700: Loss=3.8413, Acc=0.3137
  Batch 750: Loss=3.9305, Acc=0.2615
  Batch 800: Loss=4.0067, Acc=0.2815
  Batch 850: Loss=3.5318, Acc=0.3058
  Batch 900: Loss=3.7220, Acc=0.3308
  Batch 950: Loss=3.7269, Acc=0.2995
  Batch 1000: Loss=3.8442, Acc=0.3271
  Batch 1050: Loss=3.9893, Acc=0.2914
  Batch 1100: Loss=3.6983, Acc=0.3096
  Batch 1150: Loss=3.7995, Acc=0.3092
  Batch 1200: Loss=4.2299, Acc=0.2774
  Batch 1250: Loss=3.9432, Acc=0.3284
  Batch 1300: Loss=4.2188, Acc=0.2491
  Batch 1350: Loss=4.1719, Acc=0.2892
  Batch 1400: Loss=3.6473, Acc=0.3233

  Train Loss: 3.8469, Train Acc: 0.3020
  Val Loss: 3.7369, Val Acc: 0.3190
  ✓ Saved best model (val_loss=3.7369)

Epoch 8/30
----------------------------------------
  Batch 50: Loss=3.7857, Acc=0.3467
  Batch 100: Loss=3.7577, Acc=0.2756
  Batch 150: Loss=3.9450, Acc=0.2837
  Batch 200: Loss=3.7911, Acc=0.2908
  Batch 250: Loss=4.2529, Acc=0.2597
  Batch 300: Loss=3.7680, Acc=0.3144
  Batch 350: Loss=4.0038, Acc=0.3143
  Batch 400: Loss=3.7472, Acc=0.3216
  Batch 450: Loss=3.7415, Acc=0.3276
  Batch 500: Loss=3.4901, Acc=0.2870
  Batch 550: Loss=3.9782, Acc=0.2645
  Batch 600: Loss=3.6431, Acc=0.2972
  Batch 650: Loss=3.5588, Acc=0.3278
  Batch 700: Loss=3.7556, Acc=0.2991
  Batch 750: Loss=3.5388, Acc=0.3014
  Batch 800: Loss=3.3378, Acc=0.3502
  Batch 850: Loss=3.7003, Acc=0.3299
  Batch 900: Loss=4.1532, Acc=0.2650
  Batch 950: Loss=3.7199, Acc=0.2984
  Batch 1000: Loss=3.7541, Acc=0.3014
  Batch 1050: Loss=3.6383, Acc=0.3311
  Batch 1100: Loss=3.9778, Acc=0.3104
  Batch 1150: Loss=3.3346, Acc=0.3491
  Batch 1200: Loss=3.5289, Acc=0.3181
  Batch 1250: Loss=3.6477, Acc=0.3148
  Batch 1300: Loss=3.8986, Acc=0.2728
  Batch 1350: Loss=3.7495, Acc=0.2893
  Batch 1400: Loss=4.0033, Acc=0.2820

  Train Loss: 3.7443, Train Acc: 0.3106
  Val Loss: 3.6332, Val Acc: 0.3279
  ✓ Saved best model (val_loss=3.6332)

Epoch 9/30
----------------------------------------
  Batch 50: Loss=3.4733, Acc=0.3489
  Batch 100: Loss=3.4315, Acc=0.3652
  Batch 150: Loss=3.8219, Acc=0.3149
  Batch 200: Loss=3.9045, Acc=0.3004
  Batch 250: Loss=3.4230, Acc=0.3459
  Batch 300: Loss=3.7123, Acc=0.2914
  Batch 350: Loss=3.2257, Acc=0.3352
  Batch 400: Loss=3.6707, Acc=0.3070
  Batch 450: Loss=4.2028, Acc=0.2936
  Batch 500: Loss=3.3116, Acc=0.3574
  Batch 550: Loss=3.6440, Acc=0.3482
  Batch 600: Loss=3.6141, Acc=0.3295
  Batch 650: Loss=3.4109, Acc=0.3389
  Batch 700: Loss=3.5459, Acc=0.3189
  Batch 750: Loss=3.3621, Acc=0.3483
  Batch 800: Loss=3.7620, Acc=0.2887
  Batch 850: Loss=3.2718, Acc=0.3628
  Batch 900: Loss=3.9936, Acc=0.2940
  Batch 950: Loss=3.7698, Acc=0.2938
  Batch 1000: Loss=3.5108, Acc=0.3588
  Batch 1050: Loss=4.0420, Acc=0.2909
  Batch 1100: Loss=3.4035, Acc=0.3299
  Batch 1150: Loss=3.4180, Acc=0.3589
  Batch 1200: Loss=3.9867, Acc=0.3025
  Batch 1250: Loss=3.4709, Acc=0.3475
  Batch 1300: Loss=3.6297, Acc=0.3481
  Batch 1350: Loss=3.8441, Acc=0.3026
  Batch 1400: Loss=3.9622, Acc=0.2825

  Train Loss: 3.6459, Train Acc: 0.3196
  Val Loss: 3.5356, Val Acc: 0.3436
  ✓ Saved best model (val_loss=3.5356)

Epoch 10/30
----------------------------------------
  Batch 50: Loss=3.8283, Acc=0.3087
  Batch 100: Loss=3.3535, Acc=0.3826
  Batch 150: Loss=3.5597, Acc=0.3312
  Batch 200: Loss=3.3886, Acc=0.3495
  Batch 250: Loss=3.3825, Acc=0.3629
  Batch 300: Loss=3.5918, Acc=0.3036
  Batch 350: Loss=3.7081, Acc=0.3141
  Batch 400: Loss=3.7912, Acc=0.3579
  Batch 450: Loss=3.8050, Acc=0.3007
  Batch 500: Loss=3.6076, Acc=0.3277
  Batch 550: Loss=3.3398, Acc=0.3568
  Batch 600: Loss=3.5065, Acc=0.3357
  Batch 650: Loss=3.3733, Acc=0.3333
  Batch 700: Loss=3.8721, Acc=0.3071
  Batch 750: Loss=3.4491, Acc=0.3494
  Batch 800: Loss=3.3218, Acc=0.3598
  Batch 850: Loss=3.4663, Acc=0.3287
  Batch 900: Loss=3.8594, Acc=0.3361
  Batch 950: Loss=3.4884, Acc=0.3465
  Batch 1000: Loss=3.3360, Acc=0.3541
  Batch 1050: Loss=3.4579, Acc=0.3523
  Batch 1100: Loss=3.5598, Acc=0.3258
  Batch 1150: Loss=3.5073, Acc=0.3304
  Batch 1200: Loss=3.4643, Acc=0.3227
  Batch 1250: Loss=3.4712, Acc=0.3447
  Batch 1300: Loss=3.6023, Acc=0.3632
  Batch 1350: Loss=3.8214, Acc=0.2978
  Batch 1400: Loss=3.2819, Acc=0.3695

  Train Loss: 3.5544, Train Acc: 0.3307
  Val Loss: 3.4392, Val Acc: 0.3584
  ✓ Saved best model (val_loss=3.4392)

Epoch 11/30
----------------------------------------
  Batch 50: Loss=3.8741, Acc=0.3149
  Batch 100: Loss=3.3679, Acc=0.3480
  Batch 150: Loss=3.3711, Acc=0.3426
  Batch 200: Loss=3.8771, Acc=0.2979
  Batch 250: Loss=3.1132, Acc=0.3866
  Batch 300: Loss=3.6188, Acc=0.3253
  Batch 350: Loss=3.5756, Acc=0.3210
  Batch 400: Loss=3.3731, Acc=0.3467
  Batch 450: Loss=3.1733, Acc=0.3450
  Batch 500: Loss=3.8669, Acc=0.3084
  Batch 550: Loss=3.5764, Acc=0.3800
  Batch 600: Loss=3.5570, Acc=0.3383
  Batch 650: Loss=3.5649, Acc=0.3514
  Batch 700: Loss=3.3552, Acc=0.3668
  Batch 750: Loss=3.5637, Acc=0.3667
  Batch 800: Loss=3.3725, Acc=0.3263
  Batch 850: Loss=3.3397, Acc=0.3573
  Batch 900: Loss=3.4376, Acc=0.3301
  Batch 950: Loss=3.5979, Acc=0.3414
  Batch 1000: Loss=3.4933, Acc=0.3494
  Batch 1050: Loss=3.3920, Acc=0.3604
  Batch 1100: Loss=3.5726, Acc=0.3616
  Batch 1150: Loss=3.2455, Acc=0.3625
  Batch 1200: Loss=3.5641, Acc=0.3520
  Batch 1250: Loss=3.7649, Acc=0.3053
  Batch 1300: Loss=3.3885, Acc=0.3628
  Batch 1350: Loss=3.4014, Acc=0.3387
  Batch 1400: Loss=3.7593, Acc=0.3443

  Train Loss: 3.4658, Train Acc: 0.3426
  Val Loss: 3.3504, Val Acc: 0.3719
  ✓ Saved best model (val_loss=3.3504)

Epoch 12/30
----------------------------------------
  Batch 50: Loss=3.1019, Acc=0.3794
  Batch 100: Loss=3.5524, Acc=0.3478
  Batch 150: Loss=3.2916, Acc=0.3543
  Batch 200: Loss=3.2476, Acc=0.3923
  Batch 250: Loss=3.3378, Acc=0.3666
  Batch 300: Loss=3.4287, Acc=0.3390
  Batch 350: Loss=3.5522, Acc=0.3248
  Batch 400: Loss=3.3829, Acc=0.3246
  Batch 450: Loss=3.2735, Acc=0.3338
  Batch 500: Loss=3.0191, Acc=0.3817
  Batch 550: Loss=3.4219, Acc=0.3452
  Batch 600: Loss=3.1774, Acc=0.3636
  Batch 650: Loss=3.6338, Acc=0.3268
  Batch 700: Loss=3.4535, Acc=0.3467
  Batch 750: Loss=3.2964, Acc=0.3357
  Batch 800: Loss=3.6602, Acc=0.3245
  Batch 850: Loss=3.4610, Acc=0.3571
  Batch 900: Loss=3.3459, Acc=0.3533
  Batch 950: Loss=3.4878, Acc=0.3482
  Batch 1000: Loss=3.2196, Acc=0.3769
  Batch 1050: Loss=3.6593, Acc=0.3330
  Batch 1100: Loss=3.4260, Acc=0.3595
  Batch 1150: Loss=3.4740, Acc=0.3498
  Batch 1200: Loss=3.1712, Acc=0.3746
  Batch 1250: Loss=3.6665, Acc=0.3354
  Batch 1300: Loss=3.2770, Acc=0.3503
  Batch 1350: Loss=3.3877, Acc=0.3607
  Batch 1400: Loss=3.3368, Acc=0.3761

  Train Loss: 3.3849, Train Acc: 0.3532
  Val Loss: 3.2637, Val Acc: 0.3847
  ✓ Saved best model (val_loss=3.2637)

Epoch 13/30
----------------------------------------
  Batch 50: Loss=3.1414, Acc=0.3609
  Batch 100: Loss=3.3666, Acc=0.3554
  Batch 150: Loss=3.4124, Acc=0.3373
  Batch 200: Loss=3.6387, Acc=0.3244
  Batch 250: Loss=3.2932, Acc=0.3750
  Batch 300: Loss=3.6876, Acc=0.3403
  Batch 350: Loss=3.0875, Acc=0.3787
  Batch 400: Loss=3.5693, Acc=0.3473
  Batch 450: Loss=3.3563, Acc=0.3632
  Batch 500: Loss=3.2838, Acc=0.3880
  Batch 550: Loss=3.5124, Acc=0.3333
  Batch 600: Loss=2.9552, Acc=0.3997
  Batch 650: Loss=3.4968, Acc=0.3603
  Batch 700: Loss=3.0360, Acc=0.3921
  Batch 750: Loss=3.1267, Acc=0.3794
  Batch 800: Loss=3.5671, Acc=0.3467
  Batch 850: Loss=3.1245, Acc=0.3698
  Batch 900: Loss=2.9179, Acc=0.4118
  Batch 950: Loss=3.2401, Acc=0.3929
  Batch 1000: Loss=3.3536, Acc=0.3370
  Batch 1050: Loss=3.2396, Acc=0.3899
  Batch 1100: Loss=3.2422, Acc=0.3480
  Batch 1150: Loss=3.4505, Acc=0.3398
  Batch 1200: Loss=3.1030, Acc=0.3839
  Batch 1250: Loss=3.6786, Acc=0.3151
  Batch 1300: Loss=3.7932, Acc=0.3221
  Batch 1350: Loss=3.2654, Acc=0.4138
  Batch 1400: Loss=3.4350, Acc=0.3800

  Train Loss: 3.3077, Train Acc: 0.3638
  Val Loss: 3.1816, Val Acc: 0.3972
  ✓ Saved best model (val_loss=3.1816)

Epoch 14/30
----------------------------------------
  Batch 50: Loss=3.5578, Acc=0.3629
  Batch 100: Loss=3.2652, Acc=0.3717
  Batch 150: Loss=3.5940, Acc=0.3649
  Batch 200: Loss=3.0772, Acc=0.3738
  Batch 250: Loss=3.4315, Acc=0.3333
  Batch 300: Loss=3.3625, Acc=0.3295
  Batch 350: Loss=2.9378, Acc=0.4068
  Batch 400: Loss=3.2815, Acc=0.3624
  Batch 450: Loss=3.1129, Acc=0.4039
  Batch 500: Loss=3.3220, Acc=0.3522
  Batch 550: Loss=3.4091, Acc=0.3752
  Batch 600: Loss=3.0186, Acc=0.3974
  Batch 650: Loss=3.1519, Acc=0.3644
  Batch 700: Loss=3.1264, Acc=0.3510
  Batch 750: Loss=3.1136, Acc=0.3943
  Batch 800: Loss=2.9753, Acc=0.3848
  Batch 850: Loss=3.4956, Acc=0.3134
  Batch 900: Loss=3.0699, Acc=0.3736
  Batch 950: Loss=3.3824, Acc=0.3257
  Batch 1000: Loss=3.2900, Acc=0.3724
  Batch 1050: Loss=3.5199, Acc=0.3243
  Batch 1100: Loss=3.4833, Acc=0.3437
  Batch 1150: Loss=3.4357, Acc=0.3463
  Batch 1200: Loss=3.1821, Acc=0.3729
  Batch 1250: Loss=2.9573, Acc=0.4220
  Batch 1300: Loss=3.3767, Acc=0.3577
  Batch 1350: Loss=3.2709, Acc=0.4038
  Batch 1400: Loss=3.2334, Acc=0.3680

  Train Loss: 3.2349, Train Acc: 0.3743
  Val Loss: 3.0997, Val Acc: 0.4097
  ✓ Saved best model (val_loss=3.0997)

Epoch 15/30
----------------------------------------
  Batch 50: Loss=3.0850, Acc=0.3848
  Batch 100: Loss=3.3239, Acc=0.3577
  Batch 150: Loss=3.1250, Acc=0.4003
  Batch 200: Loss=3.1287, Acc=0.3946
  Batch 250: Loss=3.1795, Acc=0.3491
  Batch 300: Loss=3.0972, Acc=0.3963
  Batch 350: Loss=3.5940, Acc=0.3622
  Batch 400: Loss=2.8713, Acc=0.4383
  Batch 450: Loss=3.1783, Acc=0.3584
  Batch 500: Loss=3.3461, Acc=0.3712
  Batch 550: Loss=3.2159, Acc=0.3898
  Batch 600: Loss=3.2613, Acc=0.3932
  Batch 650: Loss=3.3328, Acc=0.3620
  Batch 700: Loss=3.3311, Acc=0.3380
  Batch 750: Loss=3.4001, Acc=0.3664
  Batch 800: Loss=2.7308, Acc=0.4030
  Batch 850: Loss=3.2722, Acc=0.3652
  Batch 900: Loss=3.0646, Acc=0.3594
  Batch 950: Loss=3.3332, Acc=0.3753
  Batch 1000: Loss=3.2731, Acc=0.3970
  Batch 1050: Loss=2.7529, Acc=0.4363
  Batch 1100: Loss=2.8889, Acc=0.4116
  Batch 1150: Loss=3.1806, Acc=0.4014
  Batch 1200: Loss=3.3842, Acc=0.3682
  Batch 1250: Loss=3.0197, Acc=0.3981
  Batch 1300: Loss=3.1668, Acc=0.3759
  Batch 1350: Loss=3.2000, Acc=0.3517
  Batch 1400: Loss=3.1523, Acc=0.3973

  Train Loss: 3.1660, Train Acc: 0.3849
  Val Loss: 3.0130, Val Acc: 0.4243
  ✓ Saved best model (val_loss=3.0130)

Epoch 16/30
----------------------------------------
  Batch 50: Loss=3.0426, Acc=0.3938
  Batch 100: Loss=3.0835, Acc=0.3898
  Batch 150: Loss=3.0469, Acc=0.3861
  Batch 200: Loss=2.7373, Acc=0.4499
  Batch 250: Loss=3.2194, Acc=0.3765
  Batch 300: Loss=3.1739, Acc=0.4207
  Batch 350: Loss=3.1955, Acc=0.3925
  Batch 400: Loss=3.0460, Acc=0.4167
  Batch 450: Loss=2.5206, Acc=0.4582
  Batch 500: Loss=3.0884, Acc=0.4151
  Batch 550: Loss=2.9615, Acc=0.4190
  Batch 600: Loss=3.2793, Acc=0.3934
  Batch 650: Loss=3.0351, Acc=0.4174
  Batch 700: Loss=2.8567, Acc=0.3802
  Batch 750: Loss=3.1217, Acc=0.3745
  Batch 800: Loss=3.0142, Acc=0.4235
  Batch 850: Loss=2.9305, Acc=0.4123
  Batch 900: Loss=3.1319, Acc=0.3861
  Batch 950: Loss=3.2525, Acc=0.3668
  Batch 1000: Loss=3.5351, Acc=0.3279
  Batch 1050: Loss=2.8939, Acc=0.4090
  Batch 1100: Loss=3.0196, Acc=0.4175
  Batch 1150: Loss=2.9008, Acc=0.4120
  Batch 1200: Loss=3.4290, Acc=0.3827
  Batch 1250: Loss=3.1094, Acc=0.3753
  Batch 1300: Loss=3.4160, Acc=0.3789
  Batch 1350: Loss=2.9925, Acc=0.4000
  Batch 1400: Loss=3.0906, Acc=0.3937

  Train Loss: 3.0955, Train Acc: 0.3963
  Val Loss: 2.9164, Val Acc: 0.4425
  ✓ Saved best model (val_loss=2.9164)

Epoch 17/30
----------------------------------------
  Batch 50: Loss=2.6962, Acc=0.4377
  Batch 100: Loss=3.0443, Acc=0.3725
  Batch 150: Loss=3.4489, Acc=0.3721
  Batch 200: Loss=3.0781, Acc=0.4124
  Batch 250: Loss=2.6016, Acc=0.4329
  Batch 300: Loss=3.4009, Acc=0.3919
  Batch 350: Loss=2.9514, Acc=0.4131
  Batch 400: Loss=3.2149, Acc=0.3537
  Batch 450: Loss=3.4146, Acc=0.3430
  Batch 500: Loss=2.9762, Acc=0.4191
  Batch 550: Loss=3.0189, Acc=0.3913
  Batch 600: Loss=3.2962, Acc=0.4155
  Batch 650: Loss=3.2980, Acc=0.3654
  Batch 700: Loss=2.8846, Acc=0.4422
  Batch 750: Loss=3.5239, Acc=0.3670
  Batch 800: Loss=2.7506, Acc=0.3985
  Batch 850: Loss=3.2644, Acc=0.4217
  Batch 900: Loss=3.1951, Acc=0.4249
  Batch 950: Loss=2.7143, Acc=0.4609
  Batch 1000: Loss=2.9886, Acc=0.4470
  Batch 1050: Loss=2.8079, Acc=0.4475
  Batch 1100: Loss=3.2531, Acc=0.3642
  Batch 1150: Loss=2.6551, Acc=0.4665
  Batch 1200: Loss=2.9843, Acc=0.4468
  Batch 1250: Loss=2.8406, Acc=0.4682
  Batch 1300: Loss=2.7840, Acc=0.4157
  Batch 1350: Loss=3.0171, Acc=0.4104
  Batch 1400: Loss=3.2037, Acc=0.3564

  Train Loss: 3.0185, Train Acc: 0.4099
  Val Loss: 2.8080, Val Acc: 0.4633
  ✓ Saved best model (val_loss=2.8080)

Epoch 18/30
----------------------------------------
  Batch 50: Loss=3.1573, Acc=0.3738
  Batch 100: Loss=3.1182, Acc=0.4226
  Batch 150: Loss=2.9076, Acc=0.4169
  Batch 200: Loss=3.1462, Acc=0.4095
  Batch 250: Loss=3.0698, Acc=0.4139
  Batch 300: Loss=2.8626, Acc=0.4194
  Batch 350: Loss=2.8725, Acc=0.4155
  Batch 400: Loss=2.9524, Acc=0.4094
  Batch 450: Loss=3.3239, Acc=0.4014
  Batch 500: Loss=2.6776, Acc=0.4468
  Batch 550: Loss=3.2958, Acc=0.3731
  Batch 600: Loss=2.4096, Acc=0.4569
  Batch 650: Loss=3.2260, Acc=0.3718
  Batch 700: Loss=3.0130, Acc=0.4480
  Batch 750: Loss=3.1627, Acc=0.4029
  Batch 800: Loss=3.0699, Acc=0.3916
  Batch 850: Loss=3.0320, Acc=0.4515
  Batch 900: Loss=2.6352, Acc=0.4379
  Batch 950: Loss=2.7131, Acc=0.4620
  Batch 1000: Loss=2.7173, Acc=0.4927
  Batch 1050: Loss=3.1876, Acc=0.4047
  Batch 1100: Loss=2.9221, Acc=0.4640
  Batch 1150: Loss=2.8671, Acc=0.4064
  Batch 1200: Loss=2.8162, Acc=0.4461
  Batch 1250: Loss=2.8323, Acc=0.4502
  Batch 1300: Loss=2.9395, Acc=0.4340
  Batch 1350: Loss=2.8146, Acc=0.4282
  Batch 1400: Loss=3.1993, Acc=0.3983

  Train Loss: 2.9426, Train Acc: 0.4241
  Val Loss: 2.6900, Val Acc: 0.4902
  ✓ Saved best model (val_loss=2.6900)

Epoch 19/30
----------------------------------------
  Batch 50: Loss=2.9143, Acc=0.4411
  Batch 100: Loss=2.7160, Acc=0.4993
  Batch 150: Loss=2.8331, Acc=0.4579
  Batch 200: Loss=2.9504, Acc=0.4222
  Batch 250: Loss=3.1624, Acc=0.4217
  Batch 300: Loss=2.9373, Acc=0.4046
  Batch 350: Loss=2.7955, Acc=0.4577
  Batch 400: Loss=3.3302, Acc=0.4114
  Batch 450: Loss=2.6535, Acc=0.4967
  Batch 500: Loss=3.0866, Acc=0.4161
  Batch 550: Loss=2.6804, Acc=0.4565
  Batch 600: Loss=2.6617, Acc=0.4686
  Batch 650: Loss=2.6863, Acc=0.4545
  Batch 700: Loss=2.8170, Acc=0.4432
  Batch 750: Loss=2.9305, Acc=0.4278
  Batch 800: Loss=3.3586, Acc=0.3865
  Batch 850: Loss=3.0635, Acc=0.4253
  Batch 900: Loss=2.8765, Acc=0.4355
  Batch 950: Loss=2.7141, Acc=0.4528
  Batch 1000: Loss=2.8003, Acc=0.4481
  Batch 1050: Loss=2.7435, Acc=0.4435
  Batch 1100: Loss=2.7636, Acc=0.4461
  Batch 1150: Loss=2.9337, Acc=0.4411
  Batch 1200: Loss=3.1528, Acc=0.4236
  Batch 1250: Loss=2.7136, Acc=0.4644
  Batch 1300: Loss=3.2241, Acc=0.4059
  Batch 1350: Loss=3.0594, Acc=0.4344
  Batch 1400: Loss=2.7059, Acc=0.4739

  Train Loss: 2.8514, Train Acc: 0.4439
  Val Loss: 2.5107, Val Acc: 0.5333
  ✓ Saved best model (val_loss=2.5107)

Epoch 20/30
----------------------------------------
  Batch 50: Loss=2.7972, Acc=0.4370
  Batch 100: Loss=2.4737, Acc=0.5155
  Batch 150: Loss=2.6643, Acc=0.4983
  Batch 200: Loss=2.7268, Acc=0.4739
  Batch 250: Loss=2.5219, Acc=0.5098
  Batch 300: Loss=2.2756, Acc=0.5233
  Batch 350: Loss=3.0747, Acc=0.4314
  Batch 400: Loss=2.3932, Acc=0.5402
  Batch 450: Loss=2.5661, Acc=0.4729
  Batch 500: Loss=2.9819, Acc=0.4738
  Batch 550: Loss=3.0554, Acc=0.4592
  Batch 600: Loss=2.8626, Acc=0.4529
  Batch 650: Loss=2.6002, Acc=0.4911
  Batch 700: Loss=2.5104, Acc=0.4902
  Batch 750: Loss=2.5259, Acc=0.4805
  Batch 800: Loss=2.4007, Acc=0.5195
  Batch 850: Loss=2.7296, Acc=0.4282
  Batch 900: Loss=2.3636, Acc=0.5369
  Batch 950: Loss=2.5755, Acc=0.4797
  Batch 1000: Loss=2.5793, Acc=0.5197
  Batch 1050: Loss=2.2469, Acc=0.5518
  Batch 1100: Loss=2.6497, Acc=0.5019
  Batch 1150: Loss=2.5039, Acc=0.5167
  Batch 1200: Loss=2.4807, Acc=0.5276
  Batch 1250: Loss=2.6965, Acc=0.4719
  Batch 1300: Loss=2.2515, Acc=0.5654
  Batch 1350: Loss=2.5063, Acc=0.5354
  Batch 1400: Loss=2.3219, Acc=0.5593

  Train Loss: 2.7005, Train Acc: 0.4831
  Val Loss: 2.1552, Val Acc: 0.6249
  ✓ Saved best model (val_loss=2.1552)

Epoch 21/30
----------------------------------------
  Batch 50: Loss=2.4146, Acc=0.5393
  Batch 100: Loss=3.0082, Acc=0.4632
  Batch 150: Loss=2.2738, Acc=0.5308
  Batch 200: Loss=2.3401, Acc=0.5436
  Batch 250: Loss=2.7056, Acc=0.5043
  Batch 300: Loss=2.4117, Acc=0.5378
  Batch 350: Loss=2.2690, Acc=0.5515
  Batch 400: Loss=2.2746, Acc=0.5644
  Batch 450: Loss=2.6432, Acc=0.5101
  Batch 500: Loss=2.4514, Acc=0.5201
  Batch 550: Loss=2.5361, Acc=0.5550
  Batch 600: Loss=2.3771, Acc=0.5501
  Batch 650: Loss=2.8200, Acc=0.5014
  Batch 700: Loss=2.6156, Acc=0.5267
  Batch 750: Loss=2.3506, Acc=0.5515
  Batch 800: Loss=2.5407, Acc=0.5265
  Batch 850: Loss=2.5385, Acc=0.5701
  Batch 900: Loss=2.7029, Acc=0.5499
  Batch 950: Loss=2.6924, Acc=0.5449
  Batch 1000: Loss=2.2200, Acc=0.5694
  Batch 1050: Loss=2.3128, Acc=0.5997
  Batch 1100: Loss=2.1047, Acc=0.5978
  Batch 1150: Loss=2.5100, Acc=0.5620
  Batch 1200: Loss=2.3855, Acc=0.5735
  Batch 1250: Loss=2.0416, Acc=0.6149
  Batch 1300: Loss=2.2297, Acc=0.6072
  Batch 1350: Loss=2.1367, Acc=0.6153
  Batch 1400: Loss=1.9012, Acc=0.6362

  Train Loss: 2.4308, Train Acc: 0.5504
  Val Loss: 1.6419, Val Acc: 0.7343
  ✓ Saved best model (val_loss=1.6419)

Epoch 22/30
----------------------------------------
  Batch 50: Loss=2.0773, Acc=0.6196
  Batch 100: Loss=2.4239, Acc=0.5614
  Batch 150: Loss=2.0305, Acc=0.6424
  Batch 200: Loss=2.1151, Acc=0.6063
  Batch 250: Loss=2.1570, Acc=0.6119
  Batch 300: Loss=2.4750, Acc=0.5589
  Batch 350: Loss=1.9101, Acc=0.6640
  Batch 400: Loss=2.2475, Acc=0.5914
  Batch 450: Loss=2.2858, Acc=0.6148
  Batch 500: Loss=2.2478, Acc=0.6061
  Batch 550: Loss=2.1187, Acc=0.6070
  Batch 600: Loss=2.3946, Acc=0.5729
  Batch 650: Loss=1.8892, Acc=0.6579
  Batch 700: Loss=2.0420, Acc=0.6343
  Batch 750: Loss=1.9518, Acc=0.6443
  Batch 800: Loss=1.5813, Acc=0.7073
  Batch 850: Loss=2.0515, Acc=0.6349
  Batch 900: Loss=2.2742, Acc=0.6105
  Batch 950: Loss=1.9239, Acc=0.6652
  Batch 1000: Loss=2.0311, Acc=0.6371
  Batch 1050: Loss=2.2756, Acc=0.6118
  Batch 1100: Loss=1.9557, Acc=0.6157
  Batch 1150: Loss=1.7128, Acc=0.6650
  Batch 1200: Loss=2.1961, Acc=0.5906
  Batch 1250: Loss=2.1704, Acc=0.6258
  Batch 1300: Loss=2.1684, Acc=0.6116
  Batch 1350: Loss=1.9000, Acc=0.6478
  Batch 1400: Loss=1.9958, Acc=0.6584

  Train Loss: 2.1089, Train Acc: 0.6218
  Val Loss: 1.2387, Val Acc: 0.8128
  ✓ Saved best model (val_loss=1.2387)

Epoch 23/30
----------------------------------------
  Batch 50: Loss=1.9836, Acc=0.6502
  Batch 100: Loss=1.7980, Acc=0.7017
  Batch 150: Loss=1.7683, Acc=0.6586
  Batch 200: Loss=1.6629, Acc=0.6901
  Batch 250: Loss=1.8937, Acc=0.6737
  Batch 300: Loss=1.7641, Acc=0.6757
  Batch 350: Loss=1.6620, Acc=0.7191
  Batch 400: Loss=2.0851, Acc=0.6248
  Batch 450: Loss=2.0159, Acc=0.6434
  Batch 500: Loss=1.8737, Acc=0.6597
  Batch 550: Loss=1.8452, Acc=0.6460
  Batch 600: Loss=1.6254, Acc=0.7027
  Batch 650: Loss=1.7318, Acc=0.6803
  Batch 700: Loss=1.4335, Acc=0.7313
  Batch 750: Loss=1.8506, Acc=0.6691
  Batch 800: Loss=2.2622, Acc=0.6371
  Batch 850: Loss=1.3058, Acc=0.7647
  Batch 900: Loss=1.7324, Acc=0.6980
  Batch 950: Loss=2.1006, Acc=0.6478
  Batch 1000: Loss=2.0760, Acc=0.6615
  Batch 1050: Loss=1.4646, Acc=0.7170
  Batch 1100: Loss=1.7623, Acc=0.6893
  Batch 1150: Loss=1.6987, Acc=0.7054
  Batch 1200: Loss=1.9037, Acc=0.6659
  Batch 1250: Loss=1.9061, Acc=0.6633
  Batch 1300: Loss=1.5806, Acc=0.7380
  Batch 1350: Loss=1.8211, Acc=0.7124
  Batch 1400: Loss=1.8529, Acc=0.6730

  Train Loss: 1.8296, Train Acc: 0.6790
  Val Loss: 0.9463, Val Acc: 0.8641
  ✓ Saved best model (val_loss=0.9463)

Epoch 24/30
----------------------------------------
  Batch 50: Loss=1.3636, Acc=0.7518
  Batch 100: Loss=1.5523, Acc=0.7027
  Batch 150: Loss=1.6957, Acc=0.7139
  Batch 200: Loss=1.6480, Acc=0.7264
  Batch 250: Loss=1.7764, Acc=0.6844
  Batch 300: Loss=1.6759, Acc=0.7133
  Batch 350: Loss=1.5686, Acc=0.7064
  Batch 400: Loss=1.5567, Acc=0.7399
  Batch 450: Loss=1.9288, Acc=0.7053
  Batch 500: Loss=1.3597, Acc=0.7486
  Batch 550: Loss=1.4834, Acc=0.7249
  Batch 600: Loss=1.5466, Acc=0.7464
  Batch 650: Loss=1.6524, Acc=0.7254
  Batch 700: Loss=1.3395, Acc=0.7719
  Batch 750: Loss=1.4697, Acc=0.7500
  Batch 800: Loss=1.4998, Acc=0.7467
  Batch 850: Loss=1.6002, Acc=0.7068
  Batch 900: Loss=1.5067, Acc=0.7453
  Batch 950: Loss=1.2453, Acc=0.7894
  Batch 1000: Loss=1.3322, Acc=0.7715
  Batch 1050: Loss=1.2672, Acc=0.7740
  Batch 1100: Loss=1.0428, Acc=0.8299
  Batch 1150: Loss=1.4170, Acc=0.7575
  Batch 1200: Loss=1.1386, Acc=0.7986
  Batch 1250: Loss=1.2348, Acc=0.7884
  Batch 1300: Loss=1.5471, Acc=0.7463
  Batch 1350: Loss=1.5019, Acc=0.7521
  Batch 1400: Loss=1.6603, Acc=0.7303

  Train Loss: 1.5182, Train Acc: 0.7398
  Val Loss: 0.6698, Val Acc: 0.9104
  ✓ Saved best model (val_loss=0.6698)

Epoch 25/30
----------------------------------------
  Batch 50: Loss=1.8207, Acc=0.7258
  Batch 100: Loss=1.2615, Acc=0.7968
  Batch 150: Loss=1.3766, Acc=0.7860
  Batch 200: Loss=1.4843, Acc=0.7431
  Batch 250: Loss=1.3711, Acc=0.7663
  Batch 300: Loss=1.3812, Acc=0.7613
  Batch 350: Loss=1.1406, Acc=0.8135
  Batch 400: Loss=1.3349, Acc=0.7728
  Batch 450: Loss=1.2606, Acc=0.7874
  Batch 500: Loss=1.0216, Acc=0.8170
  Batch 550: Loss=0.9725, Acc=0.8406
  Batch 600: Loss=1.0943, Acc=0.8303
  Batch 650: Loss=1.2581, Acc=0.7759
  Batch 700: Loss=1.2584, Acc=0.7922
  Batch 750: Loss=1.5326, Acc=0.7513
  Batch 800: Loss=1.1473, Acc=0.8100
  Batch 850: Loss=1.0446, Acc=0.8197
  Batch 900: Loss=1.4547, Acc=0.7763
  Batch 950: Loss=1.2643, Acc=0.7834
  Batch 1000: Loss=1.1889, Acc=0.7987
  Batch 1050: Loss=1.1649, Acc=0.7971
  Batch 1100: Loss=0.9207, Acc=0.8552
  Batch 1150: Loss=1.0913, Acc=0.8206
  Batch 1200: Loss=0.9637, Acc=0.8419
  Batch 1250: Loss=1.0616, Acc=0.8274
  Batch 1300: Loss=0.9883, Acc=0.8168
  Batch 1350: Loss=0.8452, Acc=0.8638
  Batch 1400: Loss=1.1283, Acc=0.8152

  Train Loss: 1.1961, Train Acc: 0.7994
  Val Loss: 0.5153, Val Acc: 0.9333
  ✓ Saved best model (val_loss=0.5153)

Epoch 26/30
----------------------------------------
  Batch 50: Loss=1.2310, Acc=0.7889
  Batch 100: Loss=1.1182, Acc=0.8158
  Batch 150: Loss=1.0822, Acc=0.8160
  Batch 200: Loss=1.1217, Acc=0.8279
  Batch 250: Loss=0.8747, Acc=0.8492
  Batch 300: Loss=1.0768, Acc=0.8113
  Batch 350: Loss=0.9092, Acc=0.8575
  Batch 400: Loss=0.9507, Acc=0.8329
  Batch 450: Loss=1.0479, Acc=0.8300
  Batch 500: Loss=1.0385, Acc=0.8196
  Batch 550: Loss=1.2455, Acc=0.7975
  Batch 600: Loss=0.7686, Acc=0.8666
  Batch 650: Loss=0.8900, Acc=0.8458
  Batch 700: Loss=0.7979, Acc=0.8748
  Batch 750: Loss=0.9901, Acc=0.8463
  Batch 800: Loss=0.8495, Acc=0.8595
  Batch 850: Loss=0.7844, Acc=0.8724
  Batch 900: Loss=1.0389, Acc=0.8212
  Batch 950: Loss=0.9032, Acc=0.8479
  Batch 1000: Loss=0.9360, Acc=0.8391
  Batch 1050: Loss=1.0135, Acc=0.8367
  Batch 1100: Loss=0.5072, Acc=0.9171
  Batch 1150: Loss=0.9582, Acc=0.8318
  Batch 1200: Loss=0.7226, Acc=0.8737
  Batch 1250: Loss=0.8763, Acc=0.8408
  Batch 1300: Loss=1.1328, Acc=0.8314
  Batch 1350: Loss=0.8523, Acc=0.8532
  Batch 1400: Loss=0.8480, Acc=0.8419

  Train Loss: 0.9917, Train Acc: 0.8357
  Val Loss: 0.4196, Val Acc: 0.9476
  ✓ Saved best model (val_loss=0.4196)

Epoch 27/30
----------------------------------------
  Batch 50: Loss=0.9162, Acc=0.8438
  Batch 100: Loss=0.8978, Acc=0.8547
  Batch 150: Loss=1.1772, Acc=0.8048
  Batch 200: Loss=0.6997, Acc=0.8709
  Batch 250: Loss=0.9336, Acc=0.8389
  Batch 300: Loss=1.1248, Acc=0.8296
  Batch 350: Loss=1.1441, Acc=0.8103
  Batch 400: Loss=0.8135, Acc=0.8534
  Batch 450: Loss=0.7240, Acc=0.8831
  Batch 500: Loss=0.8727, Acc=0.8480
  Batch 550: Loss=0.7473, Acc=0.8826
  Batch 600: Loss=0.7633, Acc=0.8674
  Batch 650: Loss=0.7556, Acc=0.8804
  Batch 700: Loss=0.8204, Acc=0.8777
  Batch 750: Loss=0.7206, Acc=0.8825
  Batch 800: Loss=0.8428, Acc=0.8705
  Batch 850: Loss=0.7328, Acc=0.8638
  Batch 900: Loss=0.7892, Acc=0.8551
  Batch 950: Loss=0.9896, Acc=0.8290
  Batch 1000: Loss=0.9187, Acc=0.8632
  Batch 1050: Loss=1.0957, Acc=0.8390
  Batch 1100: Loss=0.8678, Acc=0.8606
  Batch 1150: Loss=0.9154, Acc=0.8517
  Batch 1200: Loss=0.9703, Acc=0.8372
  Batch 1250: Loss=1.0460, Acc=0.8177
  Batch 1300: Loss=0.6351, Acc=0.8967
  Batch 1350: Loss=0.8545, Acc=0.8652
  Batch 1400: Loss=0.8098, Acc=0.8645

  Train Loss: 0.8632, Train Acc: 0.8583
  Val Loss: 0.3598, Val Acc: 0.9567
  ✓ Saved best model (val_loss=0.3598)

Epoch 28/30
----------------------------------------
  Batch 50: Loss=0.7364, Acc=0.8871
  Batch 100: Loss=0.7897, Acc=0.8660
  Batch 150: Loss=0.6731, Acc=0.8929
  Batch 200: Loss=0.9078, Acc=0.8577
  Batch 250: Loss=1.1139, Acc=0.8212
  Batch 300: Loss=0.7029, Acc=0.8870
  Batch 350: Loss=0.6522, Acc=0.8977
  Batch 400: Loss=0.8194, Acc=0.8575
  Batch 450: Loss=0.7650, Acc=0.8857
  Batch 500: Loss=0.7897, Acc=0.8662
  Batch 550: Loss=0.7443, Acc=0.8722
  Batch 600: Loss=0.8281, Acc=0.8510
  Batch 650: Loss=0.6971, Acc=0.8821
  Batch 700: Loss=0.5959, Acc=0.9032
  Batch 750: Loss=0.9280, Acc=0.8494
  Batch 800: Loss=0.8486, Acc=0.8579
  Batch 850: Loss=0.8650, Acc=0.8632
  Batch 900: Loss=0.7805, Acc=0.8703
  Batch 950: Loss=0.6273, Acc=0.9002
  Batch 1000: Loss=0.5411, Acc=0.8980
  Batch 1050: Loss=0.7973, Acc=0.8573
  Batch 1100: Loss=0.7022, Acc=0.8864
  Batch 1150: Loss=1.1797, Acc=0.8010
  Batch 1200: Loss=0.7169, Acc=0.8907
  Batch 1250: Loss=0.9737, Acc=0.8441
  Batch 1300: Loss=0.8224, Acc=0.8515
  Batch 1350: Loss=0.6035, Acc=0.8969
  Batch 1400: Loss=0.7178, Acc=0.8872

  Train Loss: 0.7729, Train Acc: 0.8736
  Val Loss: 0.3134, Val Acc: 0.9636
  ✓ Saved best model (val_loss=0.3134)

Epoch 29/30
----------------------------------------
  Batch 50: Loss=0.9677, Acc=0.8581
  Batch 100: Loss=0.6379, Acc=0.8938
  Batch 150: Loss=0.8417, Acc=0.8736
  Batch 200: Loss=0.7184, Acc=0.8918
  Batch 250: Loss=0.5622, Acc=0.9021
  Batch 300: Loss=0.9306, Acc=0.8564
  Batch 350: Loss=0.7282, Acc=0.8923
  Batch 400: Loss=0.6456, Acc=0.8982
  Batch 450: Loss=0.6242, Acc=0.8897
  Batch 500: Loss=0.5862, Acc=0.9073
  Batch 550: Loss=0.9366, Acc=0.8645
  Batch 600: Loss=0.7597, Acc=0.8901
  Batch 650: Loss=0.8255, Acc=0.8603
  Batch 700: Loss=0.6673, Acc=0.8894
  Batch 750: Loss=0.6380, Acc=0.8870
  Batch 800: Loss=0.7874, Acc=0.8768
  Batch 850: Loss=0.5770, Acc=0.9089
  Batch 900: Loss=0.7701, Acc=0.8741
  Batch 950: Loss=0.4550, Acc=0.9297
  Batch 1000: Loss=0.8668, Acc=0.8620
  Batch 1050: Loss=0.6740, Acc=0.8861
  Batch 1100: Loss=0.7962, Acc=0.8720
  Batch 1150: Loss=0.6358, Acc=0.8987
  Batch 1200: Loss=0.4427, Acc=0.9280
  Batch 1250: Loss=0.6982, Acc=0.8888
  Batch 1300: Loss=0.8817, Acc=0.8462
  Batch 1350: Loss=0.6723, Acc=0.8872
  Batch 1400: Loss=0.5777, Acc=0.9014

  Train Loss: 0.7028, Train Acc: 0.8860
  Val Loss: 0.2785, Val Acc: 0.9690
  ✓ Saved best model (val_loss=0.2785)

Epoch 30/30
----------------------------------------
  Batch 50: Loss=0.7408, Acc=0.8890
  Batch 100: Loss=0.6454, Acc=0.8930
  Batch 150: Loss=0.5995, Acc=0.8942
  Batch 200: Loss=0.6274, Acc=0.8996
  Batch 250: Loss=0.5759, Acc=0.9111
  Batch 300: Loss=0.7517, Acc=0.8725
  Batch 350: Loss=0.4825, Acc=0.9237
  Batch 400: Loss=0.8231, Acc=0.8613
  Batch 450: Loss=0.6675, Acc=0.8918
  Batch 500: Loss=0.7291, Acc=0.8865
  Batch 550: Loss=0.5467, Acc=0.9254
  Batch 600: Loss=0.6326, Acc=0.8939
  Batch 650: Loss=0.5620, Acc=0.9116
  Batch 700: Loss=0.5523, Acc=0.9110
  Batch 750: Loss=0.5501, Acc=0.9120
  Batch 800: Loss=0.5129, Acc=0.9061
  Batch 850: Loss=0.7867, Acc=0.8722
  Batch 900: Loss=0.4332, Acc=0.9284
  Batch 950: Loss=0.3968, Acc=0.9382
  Batch 1000: Loss=0.7878, Acc=0.8700
  Batch 1050: Loss=0.6540, Acc=0.8922
  Batch 1100: Loss=0.5034, Acc=0.9177
  Batch 1150: Loss=0.7519, Acc=0.8850
  Batch 1200: Loss=0.3842, Acc=0.9264
  Batch 1250: Loss=0.4590, Acc=0.9311
  Batch 1300: Loss=1.0927, Acc=0.8484
  Batch 1350: Loss=0.4214, Acc=0.9218
  Batch 1400: Loss=0.5489, Acc=0.9089

  Train Loss: 0.6415, Train Acc: 0.8963
  Val Loss: 0.2505, Val Acc: 0.9731
  ✓ Saved best model (val_loss=0.2505)

✓ Training complete. Best validation loss: 0.2505
