
Epoch 1/20
----------------------------------------
  Batch 50: Loss=8.7524, Acc=0.0000
  Batch 100: Loss=8.5274, Acc=0.0000
  Batch 150: Loss=8.2253, Acc=0.0000
  Batch 200: Loss=7.9813, Acc=0.0000
  Batch 250: Loss=7.6931, Acc=0.0000
  Batch 300: Loss=7.4829, Acc=0.0000
  Batch 350: Loss=7.3513, Acc=0.0000
  Batch 400: Loss=6.9840, Acc=0.0013
  Batch 450: Loss=6.7923, Acc=0.0078
  Batch 500: Loss=6.5577, Acc=0.0099
  Batch 550: Loss=6.2807, Acc=0.0015
  Batch 600: Loss=6.3743, Acc=0.0000
  Batch 650: Loss=6.0597, Acc=0.0000
  Batch 700: Loss=6.0571, Acc=0.0000
  Batch 750: Loss=5.9751, Acc=0.0000
  Batch 800: Loss=5.8056, Acc=0.0000
  Batch 850: Loss=5.2945, Acc=0.0000
  Batch 900: Loss=5.6109, Acc=0.0000
  Batch 950: Loss=5.5595, Acc=0.0000
  Batch 1000: Loss=5.3577, Acc=0.0000
  Batch 1050: Loss=5.2816, Acc=0.0000
  Batch 1100: Loss=5.3680, Acc=0.0000
  Batch 1150: Loss=5.5759, Acc=0.0000
  Batch 1200: Loss=5.5113, Acc=0.0000
  Batch 1250: Loss=5.5377, Acc=0.0000
  Batch 1300: Loss=5.6670, Acc=0.0000
  Batch 1350: Loss=5.0247, Acc=0.0000
  Batch 1400: Loss=5.2405, Acc=0.0000

  Train Loss: 6.4082, Train Acc: 0.0008
  Val Loss: 6.0206, Val Acc: 0.0000
  ✓ Saved best model (val_loss=6.0206)

Epoch 2/20
----------------------------------------
  Batch 50: Loss=5.0561, Acc=0.0000
  Batch 100: Loss=5.0886, Acc=0.0000
  Batch 150: Loss=4.9180, Acc=0.0000
  Batch 200: Loss=5.0366, Acc=0.0000
  Batch 250: Loss=4.9243, Acc=0.0000
  Batch 300: Loss=5.2653, Acc=0.0000
  Batch 350: Loss=4.7708, Acc=0.0000
  Batch 400: Loss=5.4699, Acc=0.0000
  Batch 450: Loss=4.6259, Acc=0.0000
  Batch 500: Loss=4.3905, Acc=0.0126
  Batch 550: Loss=4.3710, Acc=0.0014
  Batch 600: Loss=4.7433, Acc=0.0026
  Batch 650: Loss=4.3484, Acc=0.0135
  Batch 700: Loss=4.7572, Acc=0.0202
  Batch 750: Loss=4.3825, Acc=0.0329
  Batch 800: Loss=4.1490, Acc=0.0686
  Batch 850: Loss=3.9982, Acc=0.0891
  Batch 900: Loss=4.1445, Acc=0.0951
  Batch 950: Loss=5.1391, Acc=0.1041
  Batch 1000: Loss=4.1498, Acc=0.1110
  Batch 1050: Loss=4.1636, Acc=0.1474
  Batch 1100: Loss=4.1619, Acc=0.1529
  Batch 1150: Loss=4.9176, Acc=0.1457
  Batch 1200: Loss=4.3817, Acc=0.1608
  Batch 1250: Loss=3.6535, Acc=0.1864
  Batch 1300: Loss=4.4654, Acc=0.1796
  Batch 1350: Loss=4.0174, Acc=0.1847
  Batch 1400: Loss=4.1687, Acc=0.2031

  Train Loss: 4.5807, Train Acc: 0.0682
  Val Loss: 5.1382, Val Acc: 0.2024
  ✓ Saved best model (val_loss=5.1382)

Epoch 3/20
----------------------------------------
  Batch 50: Loss=4.3416, Acc=0.2093
  Batch 100: Loss=3.7201, Acc=0.2288
  Batch 150: Loss=4.4608, Acc=0.2314
  Batch 200: Loss=3.8847, Acc=0.2408
  Batch 250: Loss=3.9719, Acc=0.2270
  Batch 300: Loss=3.4156, Acc=0.1967
  Batch 350: Loss=3.9122, Acc=0.2447
  Batch 400: Loss=4.2153, Acc=0.2250
  Batch 450: Loss=3.9156, Acc=0.2235
  Batch 500: Loss=3.7026, Acc=0.2183
  Batch 550: Loss=4.3154, Acc=0.2081
  Batch 600: Loss=3.3563, Acc=0.2463
  Batch 650: Loss=3.6711, Acc=0.2500
  Batch 700: Loss=3.8420, Acc=0.2471
  Batch 750: Loss=3.3384, Acc=0.2197
  Batch 800: Loss=3.8503, Acc=0.2590
  Batch 850: Loss=3.2324, Acc=0.2676
  Batch 900: Loss=3.8835, Acc=0.2331
  Batch 950: Loss=3.9147, Acc=0.2568
  Batch 1000: Loss=3.4691, Acc=0.2901
  Batch 1050: Loss=3.7479, Acc=0.2541
  Batch 1100: Loss=3.7987, Acc=0.2741
  Batch 1150: Loss=2.8299, Acc=0.2638
  Batch 1200: Loss=3.4492, Acc=0.2553
  Batch 1250: Loss=3.7551, Acc=0.2557
  Batch 1300: Loss=3.9418, Acc=0.2690
  Batch 1350: Loss=3.2275, Acc=0.2696
  Batch 1400: Loss=4.0718, Acc=0.2656

  Train Loss: 3.8541, Train Acc: 0.2399
  Val Loss: 4.5255, Val Acc: 0.2610
  ✓ Saved best model (val_loss=4.5255)

Epoch 4/20
----------------------------------------
  Batch 50: Loss=3.7460, Acc=0.2509
  Batch 100: Loss=3.2029, Acc=0.2685
  Batch 150: Loss=4.1841, Acc=0.2583
  Batch 200: Loss=3.5095, Acc=0.2613
  Batch 250: Loss=3.7498, Acc=0.2533
  Batch 300: Loss=3.6058, Acc=0.2572
  Batch 350: Loss=3.0074, Acc=0.2810
  Batch 400: Loss=4.0272, Acc=0.2545
  Batch 450: Loss=3.4165, Acc=0.2775
  Batch 500: Loss=3.6321, Acc=0.2648
  Batch 550: Loss=3.3103, Acc=0.2775
  Batch 600: Loss=4.3330, Acc=0.2810
  Batch 650: Loss=3.8644, Acc=0.2870
  Batch 700: Loss=3.4187, Acc=0.2767
  Batch 750: Loss=2.7834, Acc=0.3224
  Batch 800: Loss=3.7535, Acc=0.2676
  Batch 850: Loss=3.2536, Acc=0.2808
  Batch 900: Loss=3.7416, Acc=0.2752
  Batch 950: Loss=2.9366, Acc=0.3291
  Batch 1000: Loss=3.6068, Acc=0.2696
  Batch 1050: Loss=3.8422, Acc=0.2944
  Batch 1100: Loss=3.7530, Acc=0.2920
  Batch 1150: Loss=3.5271, Acc=0.2616
  Batch 1200: Loss=3.1790, Acc=0.2884
  Batch 1250: Loss=3.7896, Acc=0.2986
  Batch 1300: Loss=3.1586, Acc=0.2771
  Batch 1350: Loss=2.8359, Acc=0.2911
  Batch 1400: Loss=3.6534, Acc=0.2561

  Train Loss: 3.5102, Train Acc: 0.2702
  Val Loss: 4.2403, Val Acc: 0.2778
  ✓ Saved best model (val_loss=4.2403)

Epoch 5/20
----------------------------------------
  Batch 50: Loss=3.8207, Acc=0.2629
  Batch 100: Loss=3.3708, Acc=0.3238
  Batch 150: Loss=3.5898, Acc=0.2673
  Batch 200: Loss=3.5685, Acc=0.2990
  Batch 250: Loss=3.4867, Acc=0.2634
  Batch 300: Loss=3.2988, Acc=0.2847
  Batch 350: Loss=3.4587, Acc=0.2742
  Batch 400: Loss=2.9635, Acc=0.2870
  Batch 450: Loss=3.9045, Acc=0.2674
  Batch 500: Loss=3.3873, Acc=0.2785
  Batch 550: Loss=3.5946, Acc=0.2478
  Batch 600: Loss=3.1598, Acc=0.2818
  Batch 650: Loss=3.4806, Acc=0.2940
  Batch 700: Loss=2.8901, Acc=0.3229
  Batch 750: Loss=3.3683, Acc=0.2778
  Batch 800: Loss=3.1877, Acc=0.3088
  Batch 850: Loss=2.4623, Acc=0.3000
  Batch 900: Loss=3.3501, Acc=0.2708
  Batch 950: Loss=3.2434, Acc=0.3016
  Batch 1000: Loss=3.2205, Acc=0.2968
  Batch 1050: Loss=3.4096, Acc=0.2988
  Batch 1100: Loss=3.7358, Acc=0.2643
  Batch 1150: Loss=2.6379, Acc=0.3394
  Batch 1200: Loss=3.2421, Acc=0.2990
  Batch 1250: Loss=3.3814, Acc=0.2603
  Batch 1300: Loss=3.2517, Acc=0.2545
  Batch 1350: Loss=3.7696, Acc=0.2928
  Batch 1400: Loss=3.3180, Acc=0.2786

  Train Loss: 3.3264, Train Acc: 0.2861
  Val Loss: 4.0535, Val Acc: 0.2910
  ✓ Saved best model (val_loss=4.0535)

Epoch 6/20
----------------------------------------
  Batch 50: Loss=3.2194, Acc=0.2967
  Batch 100: Loss=2.8878, Acc=0.3356
  Batch 150: Loss=2.9472, Acc=0.2914
  Batch 200: Loss=3.5303, Acc=0.2931
  Batch 250: Loss=3.6683, Acc=0.2991
  Batch 300: Loss=3.7037, Acc=0.2761
  Batch 350: Loss=3.4203, Acc=0.3130
  Batch 400: Loss=3.1410, Acc=0.2995
  Batch 450: Loss=2.8916, Acc=0.3210
  Batch 500: Loss=3.3593, Acc=0.3080
  Batch 550: Loss=3.4657, Acc=0.2637
  Batch 600: Loss=2.8847, Acc=0.3285
  Batch 650: Loss=2.9454, Acc=0.3445
  Batch 700: Loss=3.2084, Acc=0.3127
  Batch 750: Loss=2.8773, Acc=0.2997
  Batch 800: Loss=3.6598, Acc=0.2989
  Batch 850: Loss=3.3722, Acc=0.2896
  Batch 900: Loss=3.7523, Acc=0.2697
  Batch 950: Loss=2.8246, Acc=0.3495
  Batch 1000: Loss=2.9591, Acc=0.2956
  Batch 1050: Loss=2.8050, Acc=0.3111
  Batch 1100: Loss=4.1676, Acc=0.2654
  Batch 1150: Loss=3.4035, Acc=0.2896
  Batch 1200: Loss=2.4954, Acc=0.3318
  Batch 1250: Loss=3.1363, Acc=0.2995
  Batch 1300: Loss=3.3006, Acc=0.2951
  Batch 1350: Loss=3.4660, Acc=0.2684
  Batch 1400: Loss=3.2275, Acc=0.2954

  Train Loss: 3.2031, Train Acc: 0.2980
  Val Loss: 3.9190, Val Acc: 0.3011
  ✓ Saved best model (val_loss=3.9190)

Epoch 7/20
----------------------------------------
  Batch 50: Loss=3.1969, Acc=0.3230
  Batch 100: Loss=3.3007, Acc=0.2957
  Batch 150: Loss=3.3725, Acc=0.2575
  Batch 200: Loss=3.7813, Acc=0.3045
  Batch 250: Loss=3.2115, Acc=0.3479
  Batch 300: Loss=2.6839, Acc=0.3180
  Batch 350: Loss=3.5664, Acc=0.2929
  Batch 400: Loss=2.7492, Acc=0.3188
  Batch 450: Loss=3.2768, Acc=0.2780
  Batch 500: Loss=2.8247, Acc=0.2831
  Batch 550: Loss=2.9956, Acc=0.3442
  Batch 600: Loss=3.4454, Acc=0.2896
  Batch 650: Loss=3.2999, Acc=0.3309
  Batch 700: Loss=2.9401, Acc=0.2810
  Batch 750: Loss=3.2075, Acc=0.3333
  Batch 800: Loss=3.0967, Acc=0.3128
  Batch 850: Loss=2.9800, Acc=0.3217
  Batch 900: Loss=3.4477, Acc=0.2764
  Batch 950: Loss=3.4631, Acc=0.3264
  Batch 1000: Loss=3.8875, Acc=0.2514
  Batch 1050: Loss=2.5393, Acc=0.3402
  Batch 1100: Loss=3.3477, Acc=0.2844
  Batch 1150: Loss=3.4323, Acc=0.3264
  Batch 1200: Loss=2.7784, Acc=0.3281
  Batch 1250: Loss=2.8870, Acc=0.3540
  Batch 1300: Loss=3.3663, Acc=0.3078
  Batch 1350: Loss=2.8698, Acc=0.3150
  Batch 1400: Loss=3.2529, Acc=0.2689

  Train Loss: 3.1123, Train Acc: 0.3079
  Val Loss: 3.8131, Val Acc: 0.3117
  ✓ Saved best model (val_loss=3.8131)

Epoch 8/20
----------------------------------------
  Batch 50: Loss=3.4189, Acc=0.3130
  Batch 100: Loss=2.9836, Acc=0.3359
  Batch 150: Loss=3.2791, Acc=0.3200
  Batch 200: Loss=2.9231, Acc=0.3141
  Batch 250: Loss=3.1441, Acc=0.2830
  Batch 300: Loss=3.1200, Acc=0.2838
  Batch 350: Loss=3.1698, Acc=0.3279
  Batch 400: Loss=2.8376, Acc=0.3245
  Batch 450: Loss=2.9643, Acc=0.3302
  Batch 500: Loss=2.7255, Acc=0.3207
  Batch 550: Loss=3.5204, Acc=0.2722
  Batch 600: Loss=3.4540, Acc=0.3047
  Batch 650: Loss=3.2444, Acc=0.3502
  Batch 700: Loss=2.9926, Acc=0.2806
  Batch 750: Loss=3.1997, Acc=0.2859
  Batch 800: Loss=3.1513, Acc=0.3242
  Batch 850: Loss=3.0622, Acc=0.3030
  Batch 900: Loss=2.8110, Acc=0.3091
  Batch 950: Loss=3.2683, Acc=0.3514
  Batch 1000: Loss=3.4306, Acc=0.2900
  Batch 1050: Loss=3.0139, Acc=0.3307
  Batch 1100: Loss=2.8389, Acc=0.3212
  Batch 1150: Loss=3.0061, Acc=0.3358
  Batch 1200: Loss=2.7988, Acc=0.3290
  Batch 1250: Loss=2.8506, Acc=0.3316
  Batch 1300: Loss=2.7383, Acc=0.3060
  Batch 1350: Loss=3.3213, Acc=0.2946
  Batch 1400: Loss=2.8589, Acc=0.3039

  Train Loss: 3.0368, Train Acc: 0.3177
  Val Loss: 3.7214, Val Acc: 0.3192
  ✓ Saved best model (val_loss=3.7214)

Epoch 9/20
----------------------------------------
  Batch 50: Loss=3.4638, Acc=0.2936
  Batch 100: Loss=3.2942, Acc=0.3357
  Batch 150: Loss=2.7669, Acc=0.3609
  Batch 200: Loss=2.9572, Acc=0.3321
  Batch 250: Loss=3.5691, Acc=0.3123
  Batch 300: Loss=3.1289, Acc=0.3173
  Batch 350: Loss=2.6211, Acc=0.3842
  Batch 400: Loss=3.0465, Acc=0.3262
  Batch 450: Loss=2.8409, Acc=0.3023
  Batch 500: Loss=2.6480, Acc=0.3170
  Batch 550: Loss=2.7431, Acc=0.3511
  Batch 600: Loss=3.0026, Acc=0.3233
  Batch 650: Loss=2.8266, Acc=0.3541
  Batch 700: Loss=3.1354, Acc=0.3341
  Batch 750: Loss=3.0306, Acc=0.3122
  Batch 800: Loss=3.2832, Acc=0.3072
  Batch 850: Loss=2.7816, Acc=0.3363
  Batch 900: Loss=3.1343, Acc=0.3032
  Batch 950: Loss=2.5672, Acc=0.3518
  Batch 1000: Loss=3.0723, Acc=0.3364
  Batch 1050: Loss=2.8619, Acc=0.2999
  Batch 1100: Loss=2.6037, Acc=0.3625
  Batch 1150: Loss=2.5737, Acc=0.3652
  Batch 1200: Loss=2.6822, Acc=0.3469
  Batch 1250: Loss=3.5050, Acc=0.3383
  Batch 1300: Loss=3.1543, Acc=0.3349
  Batch 1350: Loss=3.1242, Acc=0.2934
  Batch 1400: Loss=2.6489, Acc=0.3403

  Train Loss: 2.9716, Train Acc: 0.3268
  Val Loss: 3.6378, Val Acc: 0.3281
  ✓ Saved best model (val_loss=3.6378)

Epoch 10/20
----------------------------------------
  Batch 50: Loss=2.9437, Acc=0.3111
  Batch 100: Loss=3.4593, Acc=0.3037
  Batch 150: Loss=3.3047, Acc=0.3208
  Batch 200: Loss=2.8500, Acc=0.3437
  Batch 250: Loss=2.7495, Acc=0.3696
  Batch 300: Loss=3.0947, Acc=0.3337
  Batch 350: Loss=3.2603, Acc=0.3170
  Batch 400: Loss=3.0712, Acc=0.3128
  Batch 450: Loss=2.6689, Acc=0.3491
  Batch 500: Loss=3.0286, Acc=0.3545
  Batch 550: Loss=3.1702, Acc=0.3283
  Batch 600: Loss=2.7499, Acc=0.3369
  Batch 650: Loss=3.1573, Acc=0.3081
  Batch 700: Loss=3.1340, Acc=0.3312
  Batch 750: Loss=2.8532, Acc=0.3474
  Batch 800: Loss=3.0304, Acc=0.3642
  Batch 850: Loss=3.0016, Acc=0.3299
  Batch 900: Loss=3.0758, Acc=0.3518
  Batch 950: Loss=2.9797, Acc=0.3198
  Batch 1000: Loss=2.9171, Acc=0.3447
  Batch 1050: Loss=3.0177, Acc=0.3554
  Batch 1100: Loss=3.2263, Acc=0.3209
  Batch 1150: Loss=2.7987, Acc=0.2956
  Batch 1200: Loss=3.0629, Acc=0.3539
  Batch 1250: Loss=3.0741, Acc=0.3349
  Batch 1300: Loss=2.8796, Acc=0.3333
  Batch 1350: Loss=2.8958, Acc=0.3639
  Batch 1400: Loss=2.9633, Acc=0.3052

  Train Loss: 2.9122, Train Acc: 0.3363
  Val Loss: 3.5593, Val Acc: 0.3387
  ✓ Saved best model (val_loss=3.5593)

Epoch 11/20
----------------------------------------
  Batch 50: Loss=2.4849, Acc=0.3506
  Batch 100: Loss=2.5924, Acc=0.4003
  Batch 150: Loss=3.3065, Acc=0.3233
  Batch 200: Loss=2.8785, Acc=0.3360
  Batch 250: Loss=2.9888, Acc=0.3494
  Batch 300: Loss=2.7364, Acc=0.3342
  Batch 350: Loss=2.5501, Acc=0.3812
  Batch 400: Loss=3.1584, Acc=0.3259
  Batch 450: Loss=2.5319, Acc=0.3727
  Batch 500: Loss=2.5806, Acc=0.3122
  Batch 550: Loss=2.8834, Acc=0.3308
  Batch 600: Loss=2.9955, Acc=0.3606
  Batch 650: Loss=3.2490, Acc=0.3099
  Batch 700: Loss=3.0710, Acc=0.3205
  Batch 750: Loss=2.7102, Acc=0.3092
  Batch 800: Loss=2.8954, Acc=0.3934
  Batch 850: Loss=2.9812, Acc=0.3369
  Batch 900: Loss=3.2796, Acc=0.3484
  Batch 950: Loss=2.3108, Acc=0.3492
  Batch 1000: Loss=2.6587, Acc=0.3651
  Batch 1050: Loss=3.0683, Acc=0.3629
  Batch 1100: Loss=2.7762, Acc=0.3286
  Batch 1150: Loss=2.5724, Acc=0.3284
  Batch 1200: Loss=3.2004, Acc=0.3534
  Batch 1250: Loss=2.9573, Acc=0.3392
  Batch 1300: Loss=2.5364, Acc=0.3612
  Batch 1350: Loss=2.7440, Acc=0.3681
  Batch 1400: Loss=2.7564, Acc=0.3387

  Train Loss: 2.8556, Train Acc: 0.3476
  Val Loss: 3.4864, Val Acc: 0.3477
  ✓ Saved best model (val_loss=3.4864)

Epoch 12/20
----------------------------------------
  Batch 50: Loss=2.8844, Acc=0.3277
  Batch 100: Loss=2.3941, Acc=0.3696
  Batch 150: Loss=2.6884, Acc=0.3782
  Batch 200: Loss=3.0689, Acc=0.3469
  Batch 250: Loss=3.4701, Acc=0.2676
  Batch 300: Loss=2.6848, Acc=0.3439
  Batch 350: Loss=3.2705, Acc=0.3664
  Batch 400: Loss=3.2568, Acc=0.3070
  Batch 450: Loss=3.1665, Acc=0.3263
  Batch 500: Loss=3.0662, Acc=0.3589
  Batch 550: Loss=2.4776, Acc=0.3912
  Batch 600: Loss=2.5887, Acc=0.3504
  Batch 650: Loss=2.4171, Acc=0.3692
  Batch 700: Loss=2.9061, Acc=0.3649
  Batch 750: Loss=3.3652, Acc=0.2894
  Batch 800: Loss=2.7179, Acc=0.3266
  Batch 850: Loss=3.2806, Acc=0.3241
  Batch 900: Loss=2.7104, Acc=0.3427
  Batch 950: Loss=2.5716, Acc=0.3893
  Batch 1000: Loss=2.7438, Acc=0.3990
  Batch 1050: Loss=3.3954, Acc=0.3098
  Batch 1100: Loss=2.8853, Acc=0.3643
  Batch 1150: Loss=3.0264, Acc=0.3443
  Batch 1200: Loss=2.3099, Acc=0.3912
  Batch 1250: Loss=2.7397, Acc=0.3727
  Batch 1300: Loss=2.7164, Acc=0.3292
  Batch 1350: Loss=2.9298, Acc=0.3753
  Batch 1400: Loss=3.1491, Acc=0.3441

  Train Loss: 2.8024, Train Acc: 0.3584
  Val Loss: 3.4142, Val Acc: 0.3581
  ✓ Saved best model (val_loss=3.4142)

Epoch 13/20
----------------------------------------
  Batch 50: Loss=2.4353, Acc=0.3661
  Batch 100: Loss=2.6096, Acc=0.3900
  Batch 150: Loss=2.7157, Acc=0.3510
  Batch 200: Loss=2.1886, Acc=0.3791
  Batch 250: Loss=2.7676, Acc=0.3381
  Batch 300: Loss=2.6657, Acc=0.4002
  Batch 350: Loss=2.7004, Acc=0.3351
  Batch 400: Loss=2.7866, Acc=0.3460
  Batch 450: Loss=2.5776, Acc=0.3808
  Batch 500: Loss=3.3075, Acc=0.3418
  Batch 550: Loss=3.1919, Acc=0.3497
  Batch 600: Loss=3.0136, Acc=0.3388
  Batch 650: Loss=2.8942, Acc=0.3938
  Batch 700: Loss=2.5255, Acc=0.4101
  Batch 750: Loss=2.7757, Acc=0.3429
  Batch 800: Loss=2.7044, Acc=0.4039
  Batch 850: Loss=2.8441, Acc=0.3320
  Batch 900: Loss=3.0639, Acc=0.3627
  Batch 950: Loss=2.7400, Acc=0.3923
  Batch 1000: Loss=2.2117, Acc=0.3914
  Batch 1050: Loss=2.7743, Acc=0.3797
  Batch 1100: Loss=2.3997, Acc=0.3853
  Batch 1150: Loss=2.5864, Acc=0.3832
  Batch 1200: Loss=3.4405, Acc=0.3360
  Batch 1250: Loss=2.3278, Acc=0.4170
  Batch 1300: Loss=2.7930, Acc=0.3735
  Batch 1350: Loss=2.9902, Acc=0.3793
  Batch 1400: Loss=2.8323, Acc=0.3739

  Train Loss: 2.7532, Train Acc: 0.3690
  Val Loss: 3.3481, Val Acc: 0.3674
  ✓ Saved best model (val_loss=3.3481)

Epoch 14/20
----------------------------------------
  Batch 50: Loss=2.5859, Acc=0.4081
  Batch 100: Loss=2.6356, Acc=0.3669
  Batch 150: Loss=2.9330, Acc=0.3471
  Batch 200: Loss=3.0271, Acc=0.3447
  Batch 250: Loss=2.1629, Acc=0.3790
  Batch 300: Loss=2.5539, Acc=0.3981
  Batch 350: Loss=2.9962, Acc=0.3813
  Batch 400: Loss=2.4792, Acc=0.3640
  Batch 450: Loss=2.9039, Acc=0.3409
  Batch 500: Loss=3.1883, Acc=0.3150
  Batch 550: Loss=2.9306, Acc=0.3763
  Batch 600: Loss=2.0211, Acc=0.4110
  Batch 650: Loss=3.1945, Acc=0.3593
  Batch 700: Loss=2.8655, Acc=0.3542
  Batch 750: Loss=3.2982, Acc=0.3556
  Batch 800: Loss=2.3891, Acc=0.3748
  Batch 850: Loss=2.8500, Acc=0.3736
  Batch 900: Loss=2.9532, Acc=0.3499
  Batch 950: Loss=2.6341, Acc=0.3560
  Batch 1000: Loss=3.0640, Acc=0.3615
  Batch 1050: Loss=2.5707, Acc=0.3533
  Batch 1100: Loss=3.1183, Acc=0.3191
  Batch 1150: Loss=2.6554, Acc=0.3816
  Batch 1200: Loss=3.0325, Acc=0.3728
  Batch 1250: Loss=2.7162, Acc=0.3919
  Batch 1300: Loss=2.6005, Acc=0.4390
  Batch 1350: Loss=2.7781, Acc=0.3635
  Batch 1400: Loss=3.1340, Acc=0.3705

  Train Loss: 2.7070, Train Acc: 0.3795
  Val Loss: 3.2841, Val Acc: 0.3749
  ✓ Saved best model (val_loss=3.2841)

Epoch 15/20
----------------------------------------
  Batch 50: Loss=2.4358, Acc=0.4083
  Batch 100: Loss=3.0599, Acc=0.3786
  Batch 150: Loss=2.9927, Acc=0.4204
  Batch 200: Loss=2.2172, Acc=0.3919
  Batch 250: Loss=2.2641, Acc=0.4451
  Batch 300: Loss=2.5438, Acc=0.3750
  Batch 350: Loss=2.5141, Acc=0.4056
  Batch 400: Loss=2.5867, Acc=0.3799
  Batch 450: Loss=2.7700, Acc=0.3858
  Batch 500: Loss=2.2809, Acc=0.3883
  Batch 550: Loss=2.5797, Acc=0.3539
  Batch 600: Loss=3.1151, Acc=0.4039
  Batch 650: Loss=2.8536, Acc=0.3841
  Batch 700: Loss=2.5676, Acc=0.4134
  Batch 750: Loss=2.3364, Acc=0.3711
  Batch 800: Loss=2.7643, Acc=0.3605
  Batch 850: Loss=2.3933, Acc=0.4430
  Batch 900: Loss=3.0333, Acc=0.4048
  Batch 950: Loss=2.8567, Acc=0.3944
  Batch 1000: Loss=2.3748, Acc=0.4046
  Batch 1050: Loss=2.1385, Acc=0.3842
  Batch 1100: Loss=2.8194, Acc=0.3693
  Batch 1150: Loss=3.5719, Acc=0.3501
  Batch 1200: Loss=2.7837, Acc=0.3695
  Batch 1250: Loss=2.6338, Acc=0.4510
  Batch 1300: Loss=2.7365, Acc=0.3580
  Batch 1350: Loss=2.6744, Acc=0.4189
  Batch 1400: Loss=2.3669, Acc=0.4354

  Train Loss: 2.6638, Train Acc: 0.3886
  Val Loss: 3.2261, Val Acc: 0.3841
  ✓ Saved best model (val_loss=3.2261)

Epoch 16/20
----------------------------------------
  Batch 50: Loss=2.6820, Acc=0.4020
  Batch 100: Loss=2.5484, Acc=0.3710
  Batch 150: Loss=2.4685, Acc=0.3946
  Batch 200: Loss=3.0122, Acc=0.3416
  Batch 250: Loss=2.4741, Acc=0.4163
  Batch 300: Loss=2.0261, Acc=0.3890
  Batch 350: Loss=2.6942, Acc=0.4250
  Batch 400: Loss=2.4656, Acc=0.3943
  Batch 450: Loss=2.4683, Acc=0.3696
  Batch 500: Loss=2.6103, Acc=0.4120
  Batch 550: Loss=2.5785, Acc=0.3470
  Batch 600: Loss=2.4692, Acc=0.4011
  Batch 650: Loss=2.2794, Acc=0.4366
  Batch 700: Loss=2.5697, Acc=0.4176
  Batch 750: Loss=2.0021, Acc=0.4175
  Batch 800: Loss=2.6279, Acc=0.4433
  Batch 850: Loss=2.2849, Acc=0.4453
  Batch 900: Loss=2.6212, Acc=0.3886
  Batch 950: Loss=2.6674, Acc=0.4047
  Batch 1000: Loss=2.5799, Acc=0.4097
  Batch 1050: Loss=2.9995, Acc=0.3863
  Batch 1100: Loss=2.5309, Acc=0.4433
  Batch 1150: Loss=2.8333, Acc=0.3577
  Batch 1200: Loss=2.8223, Acc=0.4083
  Batch 1250: Loss=2.5148, Acc=0.3974
  Batch 1300: Loss=2.6794, Acc=0.3604
  Batch 1350: Loss=2.8906, Acc=0.4140
  Batch 1400: Loss=3.0877, Acc=0.3950

  Train Loss: 2.6228, Train Acc: 0.3982
  Val Loss: 3.1705, Val Acc: 0.3926
  ✓ Saved best model (val_loss=3.1705)

Epoch 17/20
----------------------------------------
  Batch 50: Loss=2.3883, Acc=0.4214
  Batch 100: Loss=2.8514, Acc=0.4242
  Batch 150: Loss=2.6065, Acc=0.3943
  Batch 200: Loss=2.5494, Acc=0.4111
  Batch 250: Loss=2.6370, Acc=0.3672
  Batch 300: Loss=2.4263, Acc=0.3779
  Batch 350: Loss=2.8061, Acc=0.4250
  Batch 400: Loss=2.4184, Acc=0.4433
  Batch 450: Loss=2.5110, Acc=0.4456
  Batch 500: Loss=2.5361, Acc=0.3720
  Batch 550: Loss=2.3001, Acc=0.4150
  Batch 600: Loss=2.5951, Acc=0.4367
  Batch 650: Loss=2.5370, Acc=0.4005
  Batch 700: Loss=2.5245, Acc=0.4247
  Batch 750: Loss=2.2740, Acc=0.4142
  Batch 800: Loss=2.5309, Acc=0.4149
  Batch 850: Loss=2.3267, Acc=0.4270
  Batch 900: Loss=2.2943, Acc=0.4213
  Batch 950: Loss=2.4149, Acc=0.3919
  Batch 1000: Loss=2.2107, Acc=0.4542
  Batch 1050: Loss=2.5472, Acc=0.4263
  Batch 1100: Loss=2.7575, Acc=0.4087
  Batch 1150: Loss=2.7430, Acc=0.3958
  Batch 1200: Loss=2.7039, Acc=0.4199
  Batch 1250: Loss=2.3191, Acc=0.4251
  Batch 1300: Loss=2.4806, Acc=0.4171
  Batch 1350: Loss=2.0664, Acc=0.3985
  Batch 1400: Loss=2.8225, Acc=0.3990

  Train Loss: 2.5846, Train Acc: 0.4072
  Val Loss: 3.1129, Val Acc: 0.4010
  ✓ Saved best model (val_loss=3.1129)

Epoch 18/20
----------------------------------------
  Batch 50: Loss=3.0749, Acc=0.3778
  Batch 100: Loss=2.6079, Acc=0.4275
  Batch 150: Loss=2.7926, Acc=0.4357
  Batch 200: Loss=2.8182, Acc=0.4105
  Batch 250: Loss=2.6799, Acc=0.3983
  Batch 300: Loss=3.0707, Acc=0.3346
  Batch 350: Loss=2.2166, Acc=0.4204
  Batch 400: Loss=2.2359, Acc=0.4461
  Batch 450: Loss=1.9262, Acc=0.4815
  Batch 500: Loss=2.9436, Acc=0.3919
  Batch 550: Loss=2.2105, Acc=0.4191
  Batch 600: Loss=2.2828, Acc=0.4629
  Batch 650: Loss=2.1458, Acc=0.4056
  Batch 700: Loss=2.8151, Acc=0.4106
  Batch 750: Loss=2.3012, Acc=0.4914
  Batch 800: Loss=2.5087, Acc=0.3800
  Batch 850: Loss=2.5722, Acc=0.4469
  Batch 900: Loss=3.1079, Acc=0.3354
  Batch 950: Loss=2.6584, Acc=0.4223
  Batch 1000: Loss=2.0983, Acc=0.4356
  Batch 1050: Loss=2.6895, Acc=0.4205
  Batch 1100: Loss=2.9149, Acc=0.3946
  Batch 1150: Loss=2.2595, Acc=0.4355
  Batch 1200: Loss=2.1343, Acc=0.4479
  Batch 1250: Loss=2.6054, Acc=0.3967
  Batch 1300: Loss=2.5616, Acc=0.4477
  Batch 1350: Loss=2.3179, Acc=0.3817
  Batch 1400: Loss=2.4236, Acc=0.4390

  Train Loss: 2.5485, Train Acc: 0.4157
  Val Loss: 3.0605, Val Acc: 0.4086
  ✓ Saved best model (val_loss=3.0605)

Epoch 19/20
----------------------------------------
  Batch 50: Loss=2.7787, Acc=0.3890
  Batch 100: Loss=2.5675, Acc=0.4075
  Batch 150: Loss=1.9715, Acc=0.4670
  Batch 200: Loss=2.1750, Acc=0.4673
  Batch 250: Loss=2.9864, Acc=0.3895
  Batch 300: Loss=2.7235, Acc=0.3865
  Batch 350: Loss=2.7904, Acc=0.3906
  Batch 400: Loss=1.7948, Acc=0.4618
  Batch 450: Loss=2.4625, Acc=0.4539
  Batch 500: Loss=2.8225, Acc=0.4050
  Batch 550: Loss=2.2544, Acc=0.4577
  Batch 600: Loss=2.4101, Acc=0.4593
  Batch 650: Loss=2.5630, Acc=0.3954
  Batch 700: Loss=2.7606, Acc=0.3971
  Batch 750: Loss=2.6196, Acc=0.4224
  Batch 800: Loss=2.2525, Acc=0.4371
  Batch 850: Loss=3.0709, Acc=0.3678
  Batch 900: Loss=2.9585, Acc=0.3875
  Batch 950: Loss=2.1152, Acc=0.4503
  Batch 1000: Loss=1.9347, Acc=0.4235
  Batch 1050: Loss=1.7544, Acc=0.4976
  Batch 1100: Loss=2.7158, Acc=0.4168
  Batch 1150: Loss=2.5725, Acc=0.4505
  Batch 1200: Loss=2.5796, Acc=0.4153
  Batch 1250: Loss=2.4211, Acc=0.4197
  Batch 1300: Loss=2.6940, Acc=0.3583
  Batch 1350: Loss=3.0357, Acc=0.3656
  Batch 1400: Loss=2.3031, Acc=0.4454

  Train Loss: 2.5139, Train Acc: 0.4240
  Val Loss: 3.0125, Val Acc: 0.4164
  ✓ Saved best model (val_loss=3.0125)

Epoch 20/20
----------------------------------------
  Batch 50: Loss=2.3070, Acc=0.4621
  Batch 100: Loss=2.7317, Acc=0.4047
  Batch 150: Loss=2.0971, Acc=0.5268
  Batch 200: Loss=2.5118, Acc=0.4726
  Batch 250: Loss=1.8430, Acc=0.4354
  Batch 300: Loss=2.7164, Acc=0.4357
  Batch 350: Loss=2.7561, Acc=0.4323
  Batch 400: Loss=2.6350, Acc=0.3753
  Batch 450: Loss=2.3302, Acc=0.4936
  Batch 500: Loss=2.5900, Acc=0.4538
  Batch 550: Loss=2.4587, Acc=0.4474
  Batch 600: Loss=2.3405, Acc=0.4053
  Batch 650: Loss=2.4354, Acc=0.4296
  Batch 700: Loss=2.5075, Acc=0.4563
  Batch 750: Loss=2.2079, Acc=0.4668
  Batch 800: Loss=2.2369, Acc=0.4450
  Batch 850: Loss=2.5828, Acc=0.4418
  Batch 900: Loss=2.4636, Acc=0.4213
  Batch 950: Loss=2.3415, Acc=0.4138
  Batch 1000: Loss=2.1486, Acc=0.4420
  Batch 1050: Loss=3.1350, Acc=0.4150
  Batch 1100: Loss=2.6774, Acc=0.3920
  Batch 1150: Loss=2.2346, Acc=0.5033
  Batch 1200: Loss=2.6166, Acc=0.4438
  Batch 1250: Loss=2.8927, Acc=0.3675
  Batch 1300: Loss=2.2669, Acc=0.4291
  Batch 1350: Loss=2.3421, Acc=0.4514
  Batch 1400: Loss=2.4319, Acc=0.4380

  Train Loss: 2.4826, Train Acc: 0.4321
  Val Loss: 2.9627, Val Acc: 0.4245
  ✓ Saved best model (val_loss=2.9627)

✓ Training complete. Best validation loss: 2.9627
