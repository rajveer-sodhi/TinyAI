
Epoch 1/30
----------------------------------------
  Batch 50: Loss=8.7951, Acc=0.0073
  Batch 100: Loss=8.6108, Acc=0.0770
  Batch 150: Loss=8.3483, Acc=0.0738
  Batch 200: Loss=8.0851, Acc=0.0474
  Batch 250: Loss=7.8133, Acc=0.0769
  Batch 300: Loss=7.5667, Acc=0.0326
  Batch 350: Loss=7.2894, Acc=0.0412
  Batch 400: Loss=7.0748, Acc=0.0465
  Batch 450: Loss=7.0195, Acc=0.0404
  Batch 500: Loss=6.8364, Acc=0.0636
  Batch 550: Loss=6.6104, Acc=0.0403
  Batch 600: Loss=6.5751, Acc=0.0358
  Batch 650: Loss=6.3243, Acc=0.0497
  Batch 700: Loss=6.4542, Acc=0.0314
  Batch 750: Loss=6.2480, Acc=0.0568
  Batch 800: Loss=6.2360, Acc=0.0540
  Batch 850: Loss=6.2647, Acc=0.0442
  Batch 900: Loss=6.0198, Acc=0.0377
  Batch 950: Loss=6.0189, Acc=0.0447
  Batch 1000: Loss=6.1303, Acc=0.0493
  Batch 1050: Loss=5.9851, Acc=0.0569
  Batch 1100: Loss=6.0132, Acc=0.0688
  Batch 1150: Loss=5.6767, Acc=0.0518
  Batch 1200: Loss=5.9043, Acc=0.0670
  Batch 1250: Loss=6.0526, Acc=0.0785
  Batch 1300: Loss=5.6736, Acc=0.0614
  Batch 1350: Loss=5.7928, Acc=0.0934
  Batch 1400: Loss=5.7038, Acc=0.1047

  Train Loss: 6.7543, Train Acc: 0.0531
  Val Loss: 5.8001, Val Acc: 0.0894
  âœ“ Saved best model (val_loss=5.8001)

Epoch 2/30
----------------------------------------
  Batch 50: Loss=5.8265, Acc=0.0946
  Batch 100: Loss=5.7316, Acc=0.1299
  Batch 150: Loss=5.6802, Acc=0.1163
  Batch 200: Loss=5.5863, Acc=0.1471
  Batch 250: Loss=5.4488, Acc=0.1585
  Batch 300: Loss=5.6290, Acc=0.1427
  Batch 350: Loss=5.6118, Acc=0.1432
